    "Incident Insights from NASA, NTSB, and the CDC"
    by Emil Stolarsky
    >> To start, I'd like everybody here to think about somebody that's important in their life right now. It can be a relative, maybe a parent, a sibling, your children, or a close friend.
    Now, picture a Wednesday morning, you walk into the kitchen and there they are, on the floor, unresponsive. What do you do? You call 911. The phone rings. Rings again. Rings a third time, but there's no response. You think to yourself, OK, maybe I didn't put the number in right. 9-1-1. The phone rings. Rings again, but still no answer.
    On April 9th, 2014, this happened over 6,000 times. On this day, the 911 routing system went down in seven states across America. What had happened is over time, we've been moving our 9111 routing systems away from telecommunication equipment to IP-based routing. A third party provider by the name of intrudeo. Their routing system broke. When a 911 call goes out, it hits a routing provider and they'll assign a unique ID and decide which call center to forward that number to. That unique ID is used to identify the call in the case across multiple departments, from the call center to the fire department, EMS, or the Police Department. But there was a dug Intrudo's code. They had hard coded the number of IDs and on April 9th they hit that limit. It took over eight hours to rectify the issue.
    The software we build, the systems we build, the stuff we put out there, has real impact and we all commonly say that software is eating the world, and as it's doing that, our responsibility to the people around us grows. We're moving from a time when our systems not working meant somebody couldn't consume media on the internet to today, where if our systems don't work, people can't pay their bills, transportation grinds to a halt and they can't reach out for help to 911. I come from the province of Ontario up in Canada. Whoo, clapping for Ontario? Any claps for Ontario?
    [applause]
    
    In Ontario over 50% of our energy is generated through nuclear power and if you're lucky you can get a state your through one of these factories, so imagine walking through, you're seeing the guys in the hardhats and the buttons controlling everything and you're walking out and you see something out of the corner of your eye. "move fast and break things."
    
    [laughter]
    
    Gives you pause for a moment, doesn't it? I mean this nuclear power plant is only 100 kilometers from where I live. I'm not sure I want the engineers working there to move fast and break things. Yet, if we were to walk into any tech office in the valley, here, and you saw this sign, you wouldn't think twice. Of course it's there. If we think about nuclear energy, though, we think of this highly reliable, secure, they've got everything sort of figured out, how to build secure systems. But it turns out this isn't the case. Back in '79, this is a picture of the Three Mile Island power plant. In 1979, after a series of mechanical failures and operator mistakes, one of the reactors had a melt down and during this meltdown and as the ensuing emergency sort of evolved, it turned out that there was no plan. They had built this nuclear power plant, assuming they'd built a bunch of safety into it, but they assumed it would always work. So when there was an issue and people had to say, what do we do now, nobody had an answer and only after this event in 1979 did we say all nuclear power plants going forward had to have an emergency plan. If something blows up, if something goes wrong, we know what we're going to do and how we move forward.
    In the airline industry, they have a phrase for this. It's called tombstone mentality. Every now and again, there will be something, maybe a mechanical failure, maybe a cultural problem, that everybody will know, will be aware of, but nobody will do anything to fix it until there's an accident and somebody pays their life.
    And that's because people are reactive. All of us, we wait for something to happen before we figure out, all right, what are we gonna do when something goes wrong? The problem is, is that in the past, all these different industries, all these different accidents, their repercussions were localized. For us, software, its biggest selling point is its leverage. We can have one engineer build software for 100,000 people out there. It can affect the entire continent or potentially even the globe. So what's going on is we don't have that luxury. We don't have the luxury that other industries had to wait for something to go wrong and wait for those lessons. Instead, we have to be proactive somehow, we have to look out, see where we can grab from those lessons and then learn from them. Luckily there's a lot of industries we can do that with, there's the aerospace industry, medicine, natural disasters, to give you an idea of how long these have been doing it, the national transportation safety board has investigated over 148,000 incidents. Today you would have to travel every single day on an airplane for 3,000 years before you experienced an accident.
    There's a lot we can learn from these other industries.
    So my name is Emil, I'm a production engineer at Shopify and I think a lot about the impact our software has on the people around us when it works and when it doesn't work and about a year ago, I realized that, wait, if all these other industries have already went through a lot of these problems, what can we go and learn from these other industries and take back p and apply to software and that's what I did. I went out and read a bunch of manuals, listened to podcasts and read books and tried to learn something that we can take back and try to implement.
    So when you dive into emergency management which would be sort of the academic term for it, all emergencies, crises or disasters can be broken down into psych four cycles, or four steps. Mitigation, preparedness, response and recovery. With mitigation this is trying to identify the risks where the issues can come from and reducing their chance of occurring. Preparedness is how do we respond to the issue when it happens. Response will be the actual response and recovery is getting back to normal. So the way I like to think about this is say you have a database service and your application calls out to the database service and this database has a primary and a standby. Mitigation would be modifying your application so that if it loses connectivity to the database, it can still serve some requests. Preparedness is having it on call. If the database goes down, who's going to go and respond? That's preparedness. Response is actually the tools you'll be running so that you can switch over from the primary database to the stand by and then recovery is setting up a new standby for the currently running database.
    So mitigation, with mitigation, it's actually interesting, because in the outside of technology, mitigation is almost viewed as -- isn't really focused on. There is a phrase that's often -- that's used. Nobody ever got fired for failing to mitigate a disaster.
    Disasters will always happen. So why invest in lowering their impact? But we know in tech that's not true. Sheerly by the number and size of our services we know that good investment in resiliency can have a big impact. But there's still stuff we can learn from other industries.  so as I was reading about these other materials I was getting on to reading about how engineers design airplanes and rockets. Every single part in an airplane is meticulously tracked, how many times it's been flown, when was the last time maintenance was done on that part, and if you hit a certain number of pressurizations, the part will be replaced or it will be rated to do a certain number of other pressurizations. Imagine if every time we wrote a function, we said this function is ready to be called a million times and at the millionth time it's called, we need to go and reevaluate it. Maybe we realize we didn't build it properly the first time and we go in and we do a refactor. This sort of approach could go very interesting.
    Another thing is actually putting risk numbers and chances of failure to our systems. There's a lot of tools that the aerospace industry uses to assign the chances or likelihood of something breaking and one of these is a fault tree. So in a fault tree, you're doing deductive reasoning on a single component in your service failing and then you'll build out boolean operators going down. So if a database service that has quorum, in order for it to lose quorum, it needs to lose up to three nodes if there's five. So you would have an and operator that would say, three nodes go down. Now, on a node, you'll see OK, the node stops working if it loses its network connection, if the disc fails, or if the CPU blows up. So you can have an or on either of those three and you can go and look at the data and say, OK, the chances of the CPUs blowing up is 1 percent. Etc. and you figure out the probability of that system failing and you can look back and say, OK, which components should I focus on? Where should I lower the chance of this system failing? Or you can say, hm, it's not great that these things are decoupled or that they're coupled and that any one of them failing would cause the whole section to fail.
    So this is a fault analysis tree from an airplane, and there's actually software out there where it's just like hundreds of levels of these fault trees, so with a fault tree, what will happen is say for instance it says like fire and explosion, if another server's system will break because of a fire or explosion, there will be a whole other tree and that's just one of the branches off of it.
    Preparedness is -- when I was going through the material on preparedness, oftentimes it could feel a little dry. You sort of go in and you're like, this is just a bunch of process. I don't really see how it can help me. And when I was sort of reflecting on when would you really say, OK, I need this, I was thinking about on-call rotations. Imagine when we were first bringing up the idea of on-calls into tech organizations, you would say, all right, I set up the system, it works, it shouldn't break and then you say, all right, now, whose job is it going to be to fix it when it breaks? It kind of feels weird, it's almost like, well, it's not supposed to break, why should we put anybody on a rotation to fix it. So preparedness is almost like that where it's planning for failure.
    So in 1970, there were a series of forest fires in Southern California. The biggest one being Laguna fire. Over 13 days, 500,000 acres were burned, 700 buildings were lost, and 16 people lost their lives.
    And during this fire, both the Los Angeles county and the Los Angeles city fire department were trying to fight the fires. What happened, though, is instead of them effectively fighting it, they ended up having miscommunication, people didn't go to fires because they assumed somebody else would go, and deal with it in that location, and afterwards, given the disaster that that whole response was, they put together a commission and they said, OK, what happened? We can't let that happen again. The commission realized that in retrospect, having a single fire department would have been more effective than having the two departments fight it and they said, OK, how do we avoid making sure this doesn't happen again? And they came up with the incident command system, ICS. So you can think of the incident command system as putting a formal structure on dealing with emergencies. If you think of an orchestra, the conductor is necessary to be there for the whole orchestra to be able to play that beautiful piece of Mozart. Without them it wouldn't be the same. In an incident it's the same story. You need somebody who's in charge and responsible, who can make executive decisions on what needs to be worked on and what can be ignored at the time. It's this idea that you have a formal tree structure, people constantly report up, and you can delegate out. This system had great success, and it moved on and evolved in NIMS. Which is the National Incident Management System, but it still has the same key ideas. If you go out and you sort of look at the official documents, you'll see these like very large trees, with a finance officer liaison officer, and the reality is, the most important part is having somebody in charge the. Having that structure. Think back to the last incident or outage you dealt with at work. There was somebody implicitly in charge in that moment. This the incident command system formalizes that. And what's nice about the incident command system, we've seen Facebook pick this up at Shopify we have incident commanders and their role will be during an outage in addition to their sort of day jobs, they'll be getting paged when there's a big enough incident and they're not supposed to fix the outage. They're not supposed to actually bring the services back up. Their job is to make sure the right on-calls are there, the on-calls have the tools they need and that the customers get the information they need, that the stakeholders know what's going on.
    Response. So we all care about response, and I think we also sometimes tend to overfocus on it, but even with that, there's a lot we can learn from other industries about them.
    So this is the B-17. In 1930s, the U.S. Army/air Force were running a competition between multiple airline manufacturers to figure out what new bomber they wanted to purchase and the B-17 was sort of the exciting, shiny new toy. It could fly twice as far, it could carry a lot more weight than any other plane and it was a lot more resilient to damage and so they had most of the military officers who were part of the procurement process come out to the airport to watch the B-17 take off and show off its skills and inside piloting it was the head of the test pilots for the Air Force, and a very experienced first officer.
    17 seconds off lift off, the airplane crashed. Nobody knew what happened. How could it be that such a good airplane and such experienced pilots had crashed the airplane? The airplane didn't fail mechanically. What had happened is that certain gauges had to be enabled for the airplane to take off, but shortly after takeoff, they had to be disengaged. The thing is that both the pilots had forgotten to disengage it. So they got a bunch of the test pilots from the Air Force and the army to go and sort of think, OK, how can we learn to fly this airplane? What do we do and interestingly they came back and they didn't say we should get better training, instead, they said we should get a checklist and this was the first checklist used in airplanes, it went over some of the most important things to make sure you're doing before takeoff, during takeoff, just after takeoff, before landing, during landing, after landing and we saw this take the airline industry by storm. If you think about a person who uses a checklist, the first thing you're going to think of is you're going to think of pilots. It's so prevalent at this point in airlines that when two airlines merge, the most contentious part is often which checklist will be the dominant checklist for that airline.
    Now, imagine the last time you were in an incident. We all have playbooks, right? We all have, if this service goes down, these are the steps you should do, but how often do we fully believe and let the checklist guide us? When I was reading about checklists, I was thinking, OK, I can see the value, but airplanes are very mechanical. They're very, this comes first, then comes the second thing, then comes the third thing and I wasn't excited about the idea of taking all my thinking out of my response to an outage. But interestingly, checklists aren't that. Checklists are a way of automating your thought process during an emergency. You don't put down debug issue on checklist. You put down the obvious. So for instance, if somebody deployed bad code, the first thing you're going to do is lock deploys. Why should the person who's on call and responding to the issue always have to remember that? We can put that on the sheet of paper, they take out bad deploy, checklist, go first thing, lock deploys, done. You're freeing up your brain for more complex issues going on at the time.
    So this next story is United Airlines flight 173. The polite was from JFK to Portland with a stopover in Denver. The flight made it to Denver, everything was fine, and then on landing into Portland, as they were lowering the landing gear, they heard a thud. It seemed like the gear was down, but the light wasn't showing that the gear was down securely. So the captain, the first office decided to do a go after around and keep it in holding pattern until they could figure out and debug what the issue was. So it took them an hour, they were flying around, and then they decided, OK, we should start landing. So they said, we won't be able to figure it out and on their approach to landing, all four engines stopped running.
    They'd run out of fuel. And the airplane crashed just before the runway. So investigators went and looked at the story and were trying to figure out what had happened. The captain of the flight was one of the most experienced pilots at United flying this particular airplane and on their first approach into Portland, they had over an hour of fuel left to debug the issue. They found the flight recorders and it turns out on the flight recorders, both the first officer and the flight engineer tried to point out that the flight was running out of fuel, but the captain was so engulfed in trying to figure out what happened to the land gear that he didn't acknowledge T the flight engineer and the first officer didn't know what to do and so they didn't say anything. In the late 60s, and 70s, they went back and they looked at all the recent airline accidents and they saw that in over 70% of the cases, pilots had known what the problem was, but failed to make their other pilots aware of it or point it out or force a solution.
    So what ended up happening was all the airlines knew there was a problem, the NTSB knew there was a problem, and they went to NASA and NASA and a bunch of psychologists came up with an idea.
    Crew resource management. Crew resource management is a formalization between interactions of people running an aircraft. So it seems very obvious when you're going over this. So this is sort of the suggested way to talk about issues or problems on an irplain. Of course you're going to try to get people's attention. Of course you're going to point out the problem. But this wasn't happening, and what happened is once this came out, the airlines, start withing United and then all the other airlines were like, no, we're going to beat this idea into all of our pilots. How do you point it out, how do you talk about what the solution is going to be? And they saw that it worked amazingly. This is to give you an example of what this might look like. Throughout my research, I was lit listening to pilots tell stories of near misses and in every single one of those stories they always talked about crew resource management. They always said that the first officer knew to go work on the issue and I was flying the plane or they said, I asked them to fly the plane, I was working on the issue, we called out what things were going on and so when I think about crew resource management I think about all the outages we've experienced at Shopify. I think about the times where something went wrong, an engineer came in and said hey, this is broken, this is broken, and everybody just ignored them. We didn't immediately hop on it because there were six other people saying, oh, this is broken, no, this is broken and we went back and we said, oh, look, we've seen this 40 minutes ago and I'm sure this is the same everywhere. So imagine if we had started training ourselves to go and use this system, which seems very basic, to respond to incidents and outages
     Never waste a good crisis. Recovery is arguably one of the most important cycles in an incident. After you go and -- every incident has a price to pay. You're either paying for it financially, you're paying for it with time, or worse.
    So we want to come out of all our crises disasters, incidents, outages, smarter. We want to say how do we say that the next time something breaks, we can fix it faster? We have RCAs, and so RACAs, quick show of hands, who runs RCAs after incidents or outages. Almost the majority -- the majority of the room. In RCAs, a lot of the time they feel like process, you formally sit down, you go through your five whys, why did this happen, why did this happen, why did this happen, and you try to identify the root cause, but really it's not what we want from the root cause. The root cause often doesn't give us the answer we're looking for. If you think what's the root cause of people falling, gravity.
    
    [laughter]
    
    Gravity is the root cause of why people fall. But that doesn't give you a lot of information, if a bunch of people are falling outside on the stairs, are you just going to blame gravity? Are you going to file an issue? No, you're going to have to figure out why in that specific area -- what's the context of people falling in that location. In root -- in doing retrospectives or post-mortems, also, they're a way of fighting your own biases. We're all biased people, we all have our own biases we need to fight and having a process and doing it with a team will allow us to sort of point out each other's biases. So one bias is fundamental attribution error, or I'm sure we've all written on a doc, human error. If you look back in time, all mistakes look like a chose. The operator chose to fail over the database. The operator chose to ship broken code. But that's not really the case, right? In hindsight, while all the answers look clear, they're not in the moment.
    And so it's important that when we look at different ways other organizations are running RCAs we look at what tools can we take to sort of adopt a system that will allow us to get rid of more of these biases and NASA has a very interesting one. It's called a causal factor tree. And so what we'll do is when there's an issue, they'll map out all the events, all the things that failed, and the conditions at that time.
    And what's interesting about this is it removes this idea of linearity. You had multiple things going on in parallel, instead of A leading to B leading to C, any of those events could have happened at different times and it gives you a clearer picture of what happened in that moment.
    Here again is another example where it's talking about a rocket part failing. And all the other -- a lot of the industries out there, so like for instance the aerospace industry has tooling built for building these sort of trees and models and analyzing incidents that happened. So it's not like we'll have to go out and do these things ourselves. People are already doing this, we just go and spend the effort to pick them up and learn and try to implement them.
    One final very neat thing that's out there is the aviation safety reporting system. In 1968, the head of the aerospace -- or sorry, aeronautics board in America was giving a speech and talking about how everybody knew or had a lot of internal data on near misses. So if there's not an actual accident you don't have to technically report it, just file it away and every airline had their own horror stories, they even knew about problems with the aircraft but they didn't talk about it and he was a saying in the speech, that's not OK, and NASA said, we're a neutral third party, we don't have any leg in this case. So they set up the aviation safety reporting system. Everyone can file something anonymously to this service. The ASRS can issue monthly newsletters talking about current themes that are going on. In fact, the FAA is forbidden from prosecuting any pilots that may have done something incorrectly if they submitted their report to the ASRS. imagine here if we had this sort of reporting system? Who here has seen a database go out of work? Who here has seen network issues at work. Hm. Seems all of us deal with similar problems, maybe different variations, but I'm sure every single one of those incidents could have taught a valuable lesson to another company in this room. The ASRS does that for airlines, imagine if we started doing that in the tech community.
    The future is coming faster than we think. We're automating away more and more systems, we're modernizing more and more of the technologies we have in our world. Machine learning is making great progress, we're upsetting industries, we don't have the luxury of waiting for something to get our ass in gear and care about this stuff. We have to start caring about it today and we have to start going and looking at the body of knowledge that's out there, and that we can use in our own organizations.
    So I've been talking a lot about being proactive, and I thought it's only fair that I leave you with an example of what proactive companies look like, and interestingly, it's Wal-Mart and Waffle House. Wal-Mart and Waffle House have a lot of locations in the southeastern United States. With Wal-Mart, before Katrina happened, or as Katrina was racing towards the southeastern United States, all the executives gathered in a room and said, all right, we're giving -- we understand this is going to be chaos. We're going to give autonomy to the store managers out there. You're going to make decisions that you don't know if you can make those calls, make them anyway, do the right thing.
    So one store manager in Louisiana took this to heart and after Katrina, the local responders needed extra supplies, but the store was inaccessible. So what did she do? She took a bulldozer, drove through the wall, took all the supplies, put them out in the parking lot, and gave them away for free. Then she broke into the pharmacy and sent all the medical supplies to a local hospital.
    
    [applause]
    
    In New Orleans, Wal-Mart was actually one of the first companies to bring in supplies, because their supply chains were so robust to dealing with natural disasters and crises. Waffle House. Let's give it up for Waffle House
    [applause] 
    Waffle House actually runs two sets of menus. It's got the limited scope down menu, it has less perishables. So during a disaster, Waffle House will actually scope down the amount of items it's serving at its restaurants to deal with the current issue. Then they also are very organized in making sure what staff can and can't reach the store and can work their shifts. They're so good at this that the Director of FEMA nicknamed and index about them.
    
    [laughter]
    
    So there's this thing known as the Waffle House index. I swear all of this is true. It's out there. I was just as surprised as you were. Green Waffle House is serving its full menu. It's not that bad of a storm.
    Yellow, Waffle House is serving only their second menu, their partial scoped-down menu, it's a pretty rough storm. And then red, Waffle House is closed. Shit's real.
    [laughter]
    
    So if I leave you with anything from this talk, try to be so good, and so on-point about dealing with your incidents with your outages that people think of you and use you as an index for the health of the internet. Try to be the Waffle House of the internet. Thank you.
    [laughter]
    
    [applause]
