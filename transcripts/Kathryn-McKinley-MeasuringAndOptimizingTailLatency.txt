    "Measuring and Optimizing Tail Latency"
    Kathryn McKinley
    
    >> So thank you. It's super-exciting to be here. Good morning on this beautiful day. So I'm going to talk to you about understanding what's in the tail of interactive web applications and once you understand it, how the different things that can be responsible for tail latency need to be optimized a little differently so I'm going to go over some of those. A bunch of the research today in my talk is joint work with my collaborators at Australia national university and my collaborators at Microsoft, my former employer. So hopefully you won't be using your phone too much during this talk, but when you are using it, you want it to be super-responsive, and we're impatient people, and if that phone doesn't respond to you quickly, you're really unhappy and you're so unhappy, you stop searching and the providers who are trying to engage you with a good user experience don't get as much of your attention and you abandon it. So we really care about tail latency. And that's our top -- one of our top priorities in optimizing your experience. So what's behind that? Servers and big data centers with lots of networking, they're consuming lots of power and they have lots of processers. And this data comes from the Lawrence Berkeley liver more labs and they collect company data about data centers and this is just a snapshot of what's in data centers in 2014, and then based on their data, they did a forecast. So the bottom is branded one socket, that means one pneumo socket, so everything is shared together and then the red is branded 2 + sockets so that means we've made a real shift so that at least servers have at least two domains which are separate memory and processers, but still a strong interconnect and all using the same power, and then we have a little thin bar of unbranded socket and what unbranded means is you're not buying it from someplace like Intel or AMD or any other provider, and what you're doing is you're custom-building something for your specific workload. And this is a huge trend right now in hardware and the practical implications on Moore's law, what we're seeing in hardware is more and more specialized components, but those, as you can see from this diagram, those specialized components, they aren't taking over the whole chip. The chip manufacturers are still helping us. They're instead and add-on to this chip, and the TPU at Google is an example of that, and Microsoft's FPGA are for their networking is another example of that. So this is kind of what's under there and what we're trying to use to give you that great experience.
    And then once you put something in the data center, you're not done paying for it yet, because it consumes energy, and the 2000 trend was we're going to consume tons and tons of energy but companies like Google and Microsoft and Facebook and Intel were super-motivated to lower that cost and so you see a nice actual engineering trend we have here that the energy that data centers were consuming did not go up at the same rate as the number of servers, that as the hardware industry did a great job doing that and then there's a bunch of lines that are potential predictions, which is we follow the current trend and that's that top dotted -- not the exponential dotted line, but the flat line, and then under that, we have what's called better, which is better software, better hardware design, that can help lower those costs, and that's what I'm going to talk today about trying to build better software that consumes less energy and uses these more efficiently, or bigger. And what does bigger mean? That means more domains on a chip, more sockets and then you can put those together and then there are some best practices that other people have learned and we can share them and do even better. So we could like even decrease the amount of energy consumed in US data centers and that would be a great thing both for the environment and for the bottom lines of the big companies.
    All right. So here are some quick facts about how much money that represents. So a very small data center might cost about $500,000 and there's still a lot of people building their own data center for their workload, so that's still happening, and then there are about 3 million data centers right now in 2016, in the U.S. These are all estimates from that same report. $1.5 trillion US Capitol investments, because it's not just how many we build each year, but how many there are. And that is a lot of money and so even if you save 1% in that in improving performance, reducing the footprint, you can save $30 million by doing less work.
    So lots more if you improve the capacity of the server to handle more workload, and not even -- and so you don't even have to build a data center. That's a hopefully some of the solutions I will show you today, convince you. So that says efficiency is a great important top priority to all these companies in order to make money and also to be a good global citizens. But these things are in conflict, and so how can we get those? And that's what I'm going to focus on today. So just going a little deeper into what's in that server architecture behind your interactive service is usually your request goes to a server and then that gets branched out to a bunch of different workers, and they logically use parallelism and replication at each of these servers in order to get reliability, scalability, and through-put. So the aggregated farms out the request, they wait for workers to respond, and then they return a response to the user.
    Since the overall response time is -- can be no better than the slowest server, that slowest server, that tail latency, really matters to your user experience. And so -- and that's usually the 99th percentile latency or higher or 98th and I'm just going to colloquially call that the tail latency. All the numbers today I'll be giving you on tail latency will be 99 percentiles.
    All right. So what are the underlying characteristics of interactive services? As my example today I'm going to use Lucene, that's an open-source Java Enterprise search and it's widely used. Disney uses it, so to do search on its local sites, so if you go to Disneyland, you want to see Mickey Mouse, you type Mickey Mouse in, Lucene is telling you where Mickey Mouse is, so even though it's not Google search, it's not Bing search, it's doing a good workload and some of the things I'll talk about today incorporate the workload that I was at bing and Microsoft match pretty publicly and. That also shows this kind of workload. So what is this kind of workload? So if you look at the percentage of requests on the Y axis and then you look at the latency on the X axis, you see a lot of requests are really short and a few requests are really long, and you see that red line. That's called a -- that's the cumulative distribution function and you can see that that has this certain shape, and that shape changes very slowly. So even if your underlying workload, like today, or this weekend, football got, like exceedingly more popular, even, because we mixed football and politics, and what happened was this CDF doesn't change that much, even though the underlying queries that are inside it change a lot, all right?
    So the slowest server, that's over here on the side, though, unfortunately is what dictates the tail. And we also have very bursty diurnal workloads, something interesting happens, a hurricane happens, more people search certain things in certain places. And there's an order of magnitude difference between this average latency and the tail latency. So if we're going to bring down that tail, we have to understand it in more detail than this, and that's what we're going to do next.
    And so the overview of where we're going now is we have to have better tools in order to diagnose what's in the tail and then we're going to reveal three sources of things that cause tail latency: Noise, systems aren't perfect, some network queues, some operating system scheduler queues, something bad happens. Queuing, you have too much load, you can't handle the burstiness, so you've got to overprovision somewhat or do additional replication in order to handle too much load. Or work. If there's actually work in those long requests, you have to do something else instead, and we're going to show a couple of things, parallelism, and also using faster processers, and the insight in this talk is because that cumulative distribution function doesn't change, we can solve optimization problems offline, and then we can apply them online by just using functions that we learned offline and then online we can also detect if the shape of this curve changes a lot, and if it does, then we can offline resolve this harder optimization problem, and then use the results online. So I'm going to show you several examples of that so if you don't remember anything else from this talk, remember that cumulative distribution function is your friend and you can use it offline to improve tail latency. All right, oh, and the second insight that you'll see used repeatedly over this talk is that long requests reveal themselves. While your request is becoming a long one, there are certain points in time that you can recognize because there's an order of magnitude difference between a slow request and a long request. All the slow requests should be done by now. This long request is still executing and there's sufficient time to react to that and improve that long request.
    All right. So here's the simplified life of a request. So this is that you have some client application, goes through the network, it goes to the aggregator, then the aggregator sends it over the network, then some worker OS and VM processes it and it goes through the application and processes it and then it has to come back. How do we understand that? The prior state of the art is from this Dick Site at Google. So if you are a Turing award winner, you can read that too. And now we have more and more offline services which are individual components all contributing to a response, then that's more and more systems has to be an expert in. You get a 1% budget sample so what you have to do is turn it on and off and you have to profile it ahead of time so you know how much time all this instrumentation you've introduced on the critical path of your interactive services, which is also not a good idea, is slowing you downment and then you get offschematics and you improve insight and you improve the system. So here's an example of one of those offline schematics and some of the difficulty you see in trying to figure out what the critical paths are through here.
    All right. So what we're going to do is we're going to show you how to do an automated instrumentation that also has 1%, and gives you continuous online profiling of the cumulative distribution function and also can break down what's in the different components of all those requests.
    Still have to have offline schematics, maybe you don't have to be a Turing award winner, but maybe you do, because these systems are very complicated to improve but I'm going to show you some ways we can do it. All right, so this work is how we do the cycle-level online profiling with a minimum disturbing of the system. So our insight here was that hardware is just generating signals. And those signals, you can read out a hardware performance counter. That's one way to see a signal. But also memory locations can communicate signals. They can communicate values or counters.
    And so did there we go. All those things I've just said. And then -- and all of these are things you can read from another thread.
    All right? You don't have to have an interrupt. The common way of doing this is instrument the code or have an interrupt in order to read these signals. If you're running in the same thread in a shared process, you can just read those out of shared locations. And so that's what we did.
    So the other core has a processer on it. For example, it reads last-level misses per cycle. It has just a really simple loop, while true, for counter in the last level cycles. I'm going to put it in a buffer and read those counters. So I'm not going to do much processing on that. I'm going to do the processing on it later offline. I'm just going to stick it in a buffer and you can also control through cache partitioning mechanisms, you can also control how much memory this uses and how much it interferes with the shared caching.
    So -- or you can actually do it on the same core in hyperthread or simultaneous multi-threading cores that are sharing resources, but it interferes more and so then you have to be a little more careful and you have to correct for the IPC that shim, our profiler, is causing you, and so you still just stick it in there but you do the math later. All right, and this lets you read memory locations or hardware context. So this seems so easy. Why hasn't somebody done this before? uh, because that doesn't work quite perfectly. So here's instructions per cycle. This machine issues for instructions per cycle at the maximum and so we have some numbers over here greater than 10. Hm, that's a problem, all right? And this is the log of the samples, and why does that happen?
    So the problem is, the way we're taking the samples in that loop, is not atomic, and you read the -- you start the counter, you read some other ones, and that looks pretty good. If those samples are taken -- take the same amount of time, then you've got good fidelity data. But occasionally, when you're taking the sample, something else happens, like you get a cache miss or you get paged out, and so that time at which you're taking the sample changes, and then you can't compare the two samples, you don't have a correct sample. So the way to detect that is actually not too amazingly hard. What you can do is you can read out the cycles at the beginning of each of two intervals, and at the end, and if the two times agree, cycles per cycle agree, then that's a good sample, because it took the same amount of time to take that sample, and the other great thing about this technique is if it, over time, it changes how long it takes to take a sample, because someone else starts interfering with you or you have shared hardware, this can adapt to that change, because it's not saying this is how long it should take. It's saying just, compare the ones next to me, did I take the same amount of time? If I did, I can do that. So we're using the clock as the ground truth. We compute this ratio of cycles per cycle and then we can simply get rid of bad samples. So now if we go back to our problematic trace and we look at that yellow bar that I put back on there, is cycles per cycle. So you can see that cycles per cycle corresponded to a lot of the noise out on the side, so when we do the filtering and we make it just consider the ones that are cycles per cycle is 1% of 1, then we get rid of all the bad data that we were seeing that is clearly wrong because you can't issue more than four at once, but -- and we also get rid of some that were too short. Now, do we know this is perfect yet? Do we know it has perfect fidelity? No, of course not, there can be other things happening. But this is much closer to what looks like ground truth.
    So and here's an example of how powerful this approach is. If you, instead of using -- if you use interrupt-driven sampling right now and you take the IPC of individual methods in Lucerne, you get what you would expect if you were sampling what has a high sample to noise ratio, you get a flat, it's all the same. And even if you go up to the maximum of 100Hz, it's all the same. But here we can identify meth No. 4 as being a real problem. And overheads are important. If you're reading it from the same core, you're always going to have a lot of overhead. If you read from another core and you try to read at 30 cycles, you can, but you get a factor of 2 overhead, but if you slow it down to reading about every 1,000 cycles, you can basically make this not interfere with that thread on the other core. So now that we have the right mechanism to watch this, let's configure our shim thread to look at thread IDs, time stamps, and program counters, things we can always read out of memory. So those are awesome.
    But we need to also look at request IDs, OK? So the software you're using has to say a little bit more about what it's doing. In this case it just needs to give you a request ID, so when we do a sample, we know this thread is working on a certain request, because the same thread will work on multiple requests. So that's all we need to add to our program to get this to work.
    All right and so now we can take that cumulative distribution, and now what I've done is spread it, write it down a little differently, and I have request groups from the slowest to the faster and the latency on the Y axis and now I just have client latency versus average queuing time. So you can see, hey, all right, long requests have, if we concentrate over there, long requests have some queuing time and they also have some work time. So now what we're going to do is just take the 99th percentile, and look at those in more detail and we're going to use the program counter to tell us what -- where these -- what these requests were doing when they -- when they incurred the long latency. So this is the tail. These are the longest requests, and now you've been waiting for 10 minutes and we're finally at this part. All right. So what we see is in many of the requests, there is a lot of queuing at the worker and there's some blue, there's some network trash and so what does that mean? That means my network controller, like maybe it's not getting scheduled properly or maybe it just has too much work, so it's causing some queuing, so yet work imperfections and you see a little bit of orange and that reveals OS imperfections. This thread is ready to run, it's sitting around on the core somewhere but somehow it's not running, but then you see lots and lots of green. Some of the very longest requests are mostly work. And no matter what you do about queuing and making your system more perfect and getting rid of noise, you won't get rid of work in long requests by doing that. So first we're going to talk about a technique that concentrates on noise and then we're going to talk about techniques that don't have noise in them. Like how you reduce the tail and do that efficiently with work.
    So the insight here is long requests reveal themselves, so we can do something specially about them, and this is regardless cause. But if the cause is noise, all right, we can replicate and reissue. The basic system architecture I showed you already has replication in it, so if one server's slowly, probably another one's not, unless you're overloaded so you do need to take into account load. So you can use the CDF to tell you both the cost and the potential for reissuing a request. So let's say you have a fixed issue time, which was the state of the art until a few weeks -- few months ago. So what you would do is say I have a budget to reissue 10%. So that's not a perfect solution, because if the reason that you experience slow latencies is because you're overloaded, adding 10% is -- can -- is not efficient. You're using more resources, and it's also not handling the things that actually do more work. It's just making more work for the system and it can cause some more queuing.
    So maybe you want to move it over to 5% reissued in order to mitigate those extra costs, but you still have some budget at which you're reissuing and that has a cost. All right. So recently some -- my former colleagues had a really nice paper at SPA, about probabilistic reissue, because you have different reasons for tail latency. Let's move this line over, so we can reissue sooner, but let's reissue probabilistically, and not always do the reissue based on a budget, and then, by adding this randomness, they actually have a beautiful proof that shows you one reissue time with randomness is equivalent to multiple reissue times based on different features, and so this gives some beautiful results that are copied straight out of the paper, because they didn't get me the slides in time, so --
    [laughter]
    so on the far left, we have 99th percentile on the Y axis of all three of these graphs. The far left is their single random, random reissue, which is the bottom red line, and then the state of the art prior to that, which is this blue line, which is the single one time you always reissue at that delay, so if your budget is 1%, for example, you reissue -- you'd figure out the D from the cumulative distribution function and you'd always reissue everyone. So in their approach, your reissue budget is 1%, but you get to do it sooner and you get to do it randomly and you can see it has a beautiful curve. Now latency versus reissue rate on the second graph shows you the problem. When the reason is overcommitted, you can -- when you start reissuing, even at a very small budget of 6% or 5%, then you're adding too much load to the system and you're just making things worse.
    So you really need to keep track of load. And then this is their best reissue result for -- and these are Lucene results, because I promised you I was going to show you lots of stuff about Lucene, and so here they definitely reduced the tail by doing this reissue, but they have a cost. All right, so that's a good idea, and people should use that for the responsiveness, but if it's work, it's in the tail, that doesn't speed up the tail. So what speeds up things? Parallelism. We have lots of parallelism on these machines and so I'm going to show you why through-put optimizations didn't work for tail latency but we can use parallelism on a single machine to improve tail latency and then I'm going to talk about another thing that's on every machine today, which is dynamic voltage scaling, to take -- to identify the tail requests and improve those.
    And then a solution that's on your phone, but not yet in servers, but could be, is asymmetric multi-core, where you combine a big core that can execute responses faster, and little cores and why do you want little cores? Well, you're going to slow down the ril requests, the shorter requests, and so you're going to sacrifice average in order to improve the tail and that gets you efficiency and capacity and I'll show you that and then we'll be done with the talk.
    All right. So work parallelism. Parallelism has historically been used in these systems for through-put. We hand it multiple requests, it does each one completely independently and that works great.
    So the idea for using parallelism for tail latency is it -- is you could have every request be parallel, all right? So this is a fixed level of parallelism, every request comes in and you parallelize it. Why isn't that a good idea? Well, in the past we didn't use it for through-put, because parallelism has overhead. For the short requests, it's useless, and so those see overhead so that hurts through-put in systems that have uniform distributions of request time. All right. And but that's not the tail. So if we judiciously use parallelism after some delay, only on things that are going to be in the tail, then we do pay a little bit of overhead, but we ramp down the tail latency and we shorten queuing and once you shorten those tails, you actually reduce queuing of the overall system. So in our system, in steady state, you see this little diagram, you have a bunch of little short requests running and then you have some longer requests that has revealed itself and you've incrementally added parallelism to it. All right, so how do we do that? Well, every D -- this just shows the animation. I'm not ready for this. All right.
    So if we do a thread that is at fixed d, OK? So you just wait a little bit and then you add a fixed amount, regardless of load, or the number of requests in the system, then you get the following graph. So here's the sequential time. So if I say every request is going to be four-way parallelism at delay 0, then I get a good graph. At very low latencies I improve the tail, but I can't handle enough load because I've added overhead to the system and I've -- and that overhead decreases the maximum amount of requests per second. So if I wait a little while and then add my parallelism, I do better, and if I only wait 20 milliseconds, I'm almost out there at sequential time at high loads, but if I wait longer, then I'm more likely to apply the parallelism only to the tail, and if I wait even longer, I can push my requests per second out even further. So -- but we'd like to have the best of all those worlds, that green line, so can we get to that green line?
    Yes!  So what we do is we take those cumulative distribution functions and then we profile the parallelism and the parallelism efficiency on the longer requests, and so that gives you a target amount of parallelism, so that gives you -- in our systems, we saw -- and this was implemented in Bing, we saw about 4 was the right number. Sometimes 3 would be the right number of threads to add to work. And most of these -- many of these systems are very parallel. So then what we do is offline, we exhaustively explore a schedule that uses load and the amount of parallelism to ask, to minimize response time and maximize -- and minimize the response time of the tail, as well as maximize through-put and so we solve an offline implementation problem and. And based on the number of requests in the system, that tells us how much parallelism to add, and that's our -- our request is the amount of load. So let's say we have two jobs come into our system at the same time. Because there's no load on the system, we parallelize both of them and so that shortens their execution time and so that reduces the probability of queuing in the future. And then we add this green run. This green run didn't really need parallelism but we give it to anyway because we're not doing anything else and we'll know it will complete quickly in this case.
    All right, so now the blue request comes in, slightly later and now we're at 3, and it says, OK, parallelism of 1. But we don't have resources for it, so even though we start it, we might be slowing down the green thread a little bit if they do some sharing of the resources, or this thread just sits in a wait queue so we can run just as soon as the other threads stop running and so then we had the purple thread come in and it has to wait until it gets executed all. But then the green thread leaves the system and we go back to 3, and the purple thread is at parallelism 1 and so it gets to start executing and as soon as the -- this thread leaves, then it would execute -- then it could have more parallelism and so that gives us this beautiful graph that hugs the bottom of the line I showed you before, and let you both increase the -- and that's 99 percentile latency, increase the requests per second and shorten the tail latency and so you can use this in two ways: You could just say I'm going to use this approach and buy fewer servers and that saves you more money or with the existing servers that you already have, you can reduce the tail latency and the energy that this system consumes.
    All right. So that's our judicious parallelism, and now we're going to try to see how much of the rest of the talk we can get through, so now, instead of doing parallelism, maybe that's too much change to the system, maybe that -- your requests don't parallelize very well, what you can also do is use the hardware to help you. So you can use dynamic voltage scaling to focus and speed up the tail.
    So currently, all requests run just as fast as the server will let you run. But if what if instead, we run the short request at a lower frequency, and so they consume less energy? And we only run the long request at the highest frequency? And so we control their tail, and we lower the total cost of ownership because we lower the energy that the system uses.
    And then you -- and that's available in servers today, so if you built an asymmetric multi-core, this can work even better, because if you have microarchitectural techniques at your disposal, you can do much better in terms of the efficiency than DVFS does. It's a good server but it has a certain range, the micro-architecture techniques let you use a much bigger range of performance, energy, power, tradeoff. So you've seen the parallelism insights, so this has a very similar flavor to it and it's called slow to fast. So what we're going to do first is when a request arrives, we're going to execute it on either at a lower voltage and frequency, or we're going to execute it on a slow processer. And then the challenge is, of course, how to to pick D, when to migrate, and what if, in your power domain, the fastest speed is not available, or in your asymmetric multi-core domain, the fastest core is not available?
    So the insight is to use the big core just enough to get your -- get your tail target latency and that determines when to migrate, but the other idea is, when you have competition for the asymmetric multi-core, then you want to migrate oldest first, so unlike classical scheduling for interrupts when something's run for a long time and you say, oh, you're a bad job, we're going to schedule the -- the operating system is going to start interfering you are and giving you lower priority, we're always going to give the longest executing time the highest priority and so if we become crowded on the little cores, the oldest request gets to migrate first to the biggest core, so in steady state we've created a system where the youngest are on the slowest, if you have a medium core, then the middle aged are on the medium core and then the oldest are on the fast er core, the opposite of real life.
    All right, so then dynamically, you have a controller design where you take that target input, and you take the cumulative distribution function and you take a notion of load and you feed that in, and you come back with the tail latency you expect, and you use basic controller design, we just used off the shelf tool to design our controller. And then we modeled the space of -- this gives you a linear controller, so we modeled it piece-wise for systems that had more than one -- for more than one speed. So the state of the art when we started this work was a system called Pegasus from Google and Stanford, and what they had done is notice, well, at low load, we can usually make the tail latency. So at low load we'll slow all the cores down and so all the cores went slower and as loading increased, they increased the speed of all the cores together so our purpose was to do a per-core improvement and we also explored a bunch of different configurations but for the purposes of this talk we're going to just talk about two of them and in this framework, this framework you can adjust so you can optimize tail latency only and you can see, I don't care how much energy I use, I'm going to rein in tail latency so that introduces a zero threshold on our system. And then the EETL is the ideas I just spoke about where you give that target latency and you try to use that big core, that fastest speed, only as much as you need it. And this does a much more energy-efficient solution, so here are the results of those three systems.
    So on the left-hand graph again we have 99 percentile latency and requests per second on the bottom of both graphs and on the right is the energy that this system is using and this is a DVFS on a Broadwell server machine. And so you can't see Pegasus because it's under tail latency in terms of the 99th percentile latency, but it's showing you that both of them can meet the tail latency target that you sent for a wide range of requests per second, until you just get to the exponential part of the curve where you have too much load and no -- nothing you can do in this system.
    But that you can lower that even further by using it judiciously just for the longer requests with the reddish line on the bottom. All right. But now go over on the requests per second and normalized energy, you pay for that in energy. You can't -- like, if you have a system that's not meeting its tail latency goals, and this is the only way to do it, say your goal was 150, then you could consume more energy and you could get the system to meet your goal if you had a software-hardware tradeoff here essentially. But Pegasus gives you some energy benefits and it's meeting our target, which was 200 in this case, but energy-efficient tail latency which is this dynamically doing it per-core gives you another approximately 10% improvement in energy, which makes -- can make a big difference in the long run.
    Now let's look at -- since we don't have a server class asymmetric multi-core, which is AMP, what we did for our methodology was use emulation, and if you're interested in the details, I'd be happy to talk about it afterwards. And so in this configuration, I'm showing the results you just saw with DVFS but I've haded two systems and those two systems are in green and those are the DVFS with EE -- sorry, the asymmetric multi-core just optimizing tail latency or trying to do tail latency and energy efficiency.
    So if we look at just tail latency on the DVFS versus AMP, we see that you can -- that asymmetric multry core pushes you out in the requests per second, which is great, because you can handle same capacity, and these machines -- these two systems are configured so they burn the same -- they have the same amount of hardware and energy and power budget so that they're comparable systems, and then if you look at the requests per second and their normalized energy, you can see, well, under low load, they burn the same amount of energy, but as the load gets higher and higher, the benefits of the underlying architecture get more exposed and you can see that on the normalized energy, so the more load that you have, the more energy you save.
    And then if we look at just the green lines now, we see that we can really push out the requests per second with -- and do it energy efficiently, because we're staying down here at this best energy level for the whole time. And so that concludes the technical portion of my talk. And so what I've talked about today is to optimize anything, you must understand it in more detail than you do now, and we needed new tools in order to diagnose the tail, because of the in situ situation where the tail is often a rare event and caused by things that you can't capture, and so we needed new mechanisms to do that.
    And then by separating out the causes of the tail, which were noise, queuing, and work, we need different techniques, and I didn't talk about anything today to do with queuing, but the same mechanism that helps us profile, we've actually shown how you can help diagnose queuing and get rid of queuing in a bunch of different ways and then for noise I showed you results of how to do replication so it uses less resources and is a more efficient mechanism work of others, and then I also showed you some work of my team on how you could use judiciously apply your resources to these long requests when there's work in them and get a lot of benefit from that. So what you should take away from today is that cumulative distribution function is a powerful tool for this offline optimization, and so that you can use much more aggressive techniques when you characterize your system this way. And that tail efficiency is not equal to average or through-put, so some of the assumptions that we have about optimizing through-put for these systems need to be tempered with sacrificing a little bit of the average in order to get the tail, and there also happen to be some queuing benefits in there that aren't in your standard network queuing class that you need to become aware of, and that one of the things we're seeing in the hardware is more and more heterogeneity, a lot of focus of the architecture community has been building about specialized speed-up of things for certain workloads. What I've talked about today is software that can exploit the differences in the workload to match the software to the hardware, and but still use more general-purpose processers, which hopefully there will be more techniques like this so that we don't all need 100 different specialized components on our chip, which is too hard to manufacture, or to reconfigure without FPGA every time we have a new workload, both of which have huge challenges for the engineering of that.
    And I had a great time, and I thank you for your attention, and I'll take a few questions. I think I have two minutes for questions.
    [applause]
    
    In the center?
     AUDIENCE:  So when you have a tail latency, probabilistically, there will still be some tail.
    >> That's right.
    >> so -- let me repeat, even when you do some of these things, there will still be a few long requests, even though that 1% will have a shorter, but there will still be some 1 percenters.
    >>
     AUDIENCE:  So how do you decide this is where we draw the line or --
    >> So what do you do about those -- there's still some things in that 1% and so what do you do about them? Well, you can iterate on this process. Well, think that one of the things that people have done to good effect is have enough redundancy in their system, or not give perfectly precise answers, so you don't wait for some of those longest requests. So that's one thing. But they're still there gobbing up your system, so you have to have dynamic techniques detecting them and doing something. Right? But the fewer of them, the better-engineered your systems. These systems are highly engineered, so a lot of people spend time trying to get more and more stuff out of there, so hopefully they will reduce it, but you'll always have occasionally. In the blue shirt?
    >>
     AUDIENCE:  
    >> I can't hear you. Could you speak louder?
    >> What was
     AUDIENCE:  What was the size of that ... was it millions of documents, thousands of queries?
    >> Oh, so we used all of Wikipedia English. I meant to mention that on the -- when I described the workload. We used all English Wikipedia documents, which were like 4 gigabytes of data  -- or sorry, yes, filling up our -- and a big machine filling up our memory and we used thousands of queries from test sets and then we did validation on -- we learned on 10,000 and then had a 2,000 queries that were our test ones, and so when I showed you the 200 queries at the end, those were the 200 in the tail of one run. When you do a different run, you get a different 200, and only a few -- like only about 10%, which are the ones with the green, all appear in the ones with work always appear in the tail. That's a good question. Sorry, I didn't mention that sooner. And so, and then we match -- we showed that the instructions per cycle of Lucene was pretty much matching the reported instructions per cycle for Google and also for -- that we observed directly for Bing, so that it was a good match for their index serve when you don't hit in a cache. One more question?
     AUDIENCE:  Are most of these optimization class system level or can you put these in app-level?
    >> So you can implement -- you can implement the scheduler parts of the parallelism and also of the running on a faster core, you can actually control that at an application layer if you have control over the entire machine for this service. But if you -- but if you have multiple services running on the machine, then you have to have some cooperation from the operating system. Good question. Oh, sorry, I didn't repeat it. Do you need to do -- can you do this at the application level or do you need to do this at the operating system level? And so you can do both, but if you have the machine to yourself, you can do it all in the application layer, and the Lucene results were all done at the application layer.
    >> So I think I've run out of time. I really thank you for your attention. Enjoy the rest of the conference.
    [applause]
    
