      "Tuning Elasticsearch for English-Language Precision."
      By: Erin McKean.
      >> Hey, folks.  I'm going to go ahead and get started because this conference has been amazingly on time so far, and I don't want to be the person who screws it up.  So, hi.  So I always like to show this slide because it's not actually true, but I feel like it makes you justified in showing up in this room.
      And so also, I had a blue lanyard on but for the purposes of this talk, please feel free to take pictures for -- I don't know.  Later ridicule purposes.  But, hi, I'm Erin and as my day job, I'm a developer advocate for IBM with the San Francisco city team.  And for the rest of the time, I also founded, and I run a site called Wordnik.com which is a large and very friendly English dictionary with an open API.  And I also run the semicolon appreciation society, and I have lots of these stickers.
      [Applause]
      So, please, hit me up later.  I would be happy to give you one.  And I am, like, oh, my god why am I here?  Because this conference is pretty intimidating, and I am a community taught developer.  I like to say community taught instead of self-taught because it's the generosity of everybody in the communities that helps me learn things.  But I like to say if you hear me getting something wrong, please let me know if possible after the talk.
      [Laughter]
      And if you won't do it for me, please do it for my future audiences.  So I'm always happy to learn where I can do something better, do something clearer, and if -- especially if it's wrong.  So thank you very much.
      So I'm here, mostly, because I love words, and I like to talk about words.  And I've worked on dictionaries for 25 years and collecting words even longer.  And collecting words is pointless unless you share them because no one likes a hoarder and also for 25 years, I've been working on how best to share words.  I've put words into all sorts of boxes.  I've put them into literal file cabinets, I have put them into literal index card, like, library card catalog drawers, printed books, I've put words in HTML, XML, relational databases like MySQL and Mongo.  But my favorite place to put words so far has been from Elasticsearch.  This slide is from Joseph, which is called four colors, four words, if you ever get a chance to go see it, it's awesome.
      But why Elasticsearch?  So Elasticsearch is a distributed restful search and analytics engine, that's what they put on their website.  It's open source.  It's maintained by Elastic.co.  Is anybody here from Elastic?  Hey, thank you, guys.
      And, yeah, you're going to be the guy that tells me when I'm wrong, and that will be awesome.  So it's free to use and to point anywhere.  There's hosted services from Elastic, AWS, even IBM has one.  I think the number one reason to use Elasticsearch is because Elasticsearch doesn't presume to know your data better than you know your data.  So if you are a control freak with delusions of omnipotentence, Elasticsearch is for you.  So, yeah.
      And so when you're putting words into any kind of place, you have to think what do I mean by word?  Let's define our terms.  When I say the word, you're probably thinking something like this.  It's a nice, simple word, a prototypical word and now I have satisfied the requirement of putting every cat in a presentation.  But there are a lot of words that aren't cat-like words.  There are words that strain native speaker intuition about what words are.  And here's examples.  These are also words.  I don't care if you think these aren't words.  Come fight me later.  These are words.  And you can see some things are, well, maybe you say, oh, that's a phrase or that's just hyphenated or that's a prefix or that's an acronym or abbreviation and that's thing at the bottom, that's what people are going to fight me on.  But from my point of view as a dictionary editor, these are things that people look up.  Which makes them words.  And it might be fair to call these things lexical items, essentially these convey meanings that are not 100% transparent from their parts.  So not to get too humpy Dumpty about all of this, but if someone wants to look something up and Wordnik has data that shows how that word is being used or who uses it or where it's used or when it's used, we want to show the data that we have.  We don't want to punish people's curiosity by being all snooty and sniffy about what a word is.  If you want to know, we want to tell you, and we definitely follow apostle's law in that we're very liberal in what we accept.
      So when you think about.  Okay.  What do people really lookup?  You have to think about what do people want?  Users have different kinds of searches with different goals.  Sometimes they want to know what a word means.  Sometimes they want to know how to use a word, so they want to see an example.  Sometimes they want confirmation of spelling or pronunciation.  Sometimes they want to win an argument in a bar.  But all of these things, all of these different kinds of answers are accessed the same way by typing something word-like into a search box.
      So ideally, you would get an exact match of what's being up and what's being returned.  But if people knew everything about the words they were looking up, why would they look it up?  And these questions are often the questions that drive people to lexical resources or let's all be honest, Google, in the first place.
      And so the number of people who just automatically capitalize everything they put in a search box, even with auto correct turned off is not trivial.  I like to call the people cryptoGermans.  They have a very German thought process.  So.  Okay.  Maybe you do have an exact match.  Maybe cat equals cat.  Maybe they don't realize Dumpster is a trademark, so they don't capitalize it.  Maybe they are interested in the burning questions to whether copy editor is hyphenated or not.  Maybe they are avid readers of the New Yorker, and they want to know --
      [Laughter]
      And so maybe they haven't -- maybe they didn't grow up in a culture where they were taught dictionary practices, so they don't think that they need to look up the simplest form of a verb and instead, they look up whatever form of the verb they happen to encounter.
      So these are all different ways that people interact with the search box.
      So what do you get out of that box when you're using Elasticsearch?  So let's talk a little quickly about how Elasticsearch works.  This is not actually a diagram about how Elasticsearch works.  I just liked it.  But for the purposes of this talk, I don't really care about how Elasticsearch stores the data.  Smarter people than me work on this.  They do a great job.  I don't care about sharding or management, that guy cares about it.  They're also talking about Elasticsearch 5.5, I know Elasticsearch 6 is out, being not the first, but the newest tribe but the last to lay the oldest side.  And I'm also only talking about English, so let's get all of those caveats out of the way.
      And so Elasticsearch is basically an inverted index.  If you've ever used a book index, then you've probably used an inverted index.  So I love this index.  I'm such a nerd that I have favorite indexes.  You can say embassies if you want, but this is English.  This is Page 631.  There is no Page 633, so he has indexed the typo in the index.  Yeah.  That's the kind of stuff that dude does.
      So in a concordance, that's another kind of inverted index, it's a use of words used in a particular work, it's the background images of the concordance of words.
      So it's all of the unique words in a document, in which the document appears.  So the inverted of index of Elasticsearch is not anything enormously different than the kind of indexes that we've been making nondigitally for centuries.
      So this is where we get into trouble again because we have to make sure that our idea of what a unique word is and Elasticsearch's idea of what a unique word is are the same.  But luckily, Elasticsearch is very flexible.  And the way that you and Elasticsearch come to a joint understanding about what words are is through mapping.  And if you think about it, every kind of map is a set of decisions about what's important and what the user needs.  And an Elasticsearch map is the same way.
      So the slides with no pictures are actually important ones.  Sorry.
      So you can set up a mapping for your document, so your mapping is applied to a type and your index, but I'm just going to hand wave those away for a minute because we're going to be mostly talking about documents.  So we can say here, you can see here that we have a mapping that says.  Okay.  We've got a property of word, and it's of type text, and it has a field of key word, which is a typed key word.  And if those key words are longer than 256 characters, we're just going to truncate them.  Sorry.  You went on too long.  We're done.
      So if you shove that document there on the lower left, which is word Ennui, what's going to happen to that document?  Well, the field word of typed text is going to get analyzed.  But the field key word of typed key word is going to have an exact value.
      So what do we get?  When we shove that data in, Elasticsearch -- and we haven't said anything about how we want things analyzed.  Elasticsearch just shrugs and says I think you're going to want the standard analyzer.  You know, standard.  Off the shelf.
      And so we shove that document in, and then we make a query, and we say, hey, we want to match everywhere where the word e-nnui is, and so we get that in our first result, but then we're also going to get E prime.  That doesn't look anything like e-nnui.  What's going on?  How many people now want to know what e-nnui is?  It's the femed of boredom as you go across social media feeds.  So it's a play on -- yeah, it's not a great word.  But for our purposes, it does all right.
      So let's check the analysis.  So Elasticsearch has this great feature where you can say, hey, show me your thought process behind what you just showed me.  Or how you analyzing the text that I've just put it in.  I'm just Elasticsearch all over the place, I think of it as a library that helps me do stuff.  But it has split that string in our word field into two tokens.  And one token is letter E and one is the ennui and the hyphen, who knows where the hyphen went and that -- oh, shoot.  That is why we got to see e-nnui and E prime because the token E matched in both cases.
      So if when you shove that document in, which is called index time, no analyzer specified it.  It looks for a analyzer in the settings of your index called default.  If it doesn't find one of those, then it goes to Elasticsearch's default, which is standard analyzer, and that's what you get this kind of output from.
      So what's an analyzer?  An analyzer is made up of three parts.  Zero or more character filters, exactly one.  There could be only one Tokenizer and zero more token filters.  And tokennization splits a string into tokens, which is roughly analogous to words, but not exactly.  And the token filters help you normalize the forms of those tokens.  So basically, given a token, what do we think the most natural, most useful form of that token should be?  So it's a character filter.  A character filter lets you add or remove characters, just like what it says on the box.  And there's some built in Elasticsearch that will let you strip out HTML, you can do pattern matching, so you can convert the ampersand, you can convert characters that you don't want to use using something else using regular expressions.  All character filters happen before the string is split up into tokens.  And you can have as many character filters as you want, and they are applied in order as you set them in your settings.
      And tokennizers, they split your text up into tokens.  And these tokens may or may not conform to what your expectation of a word should be, as we saw with e-nnui.  It also tells you -- terms and records the order, the position of each token and theoff sets of the original set of characters that the original token represents.  And, again, you can only have one Tokenizer, no matter how nicely you ask.
      And then you have a token filter, which happens after the token.  Once things are split up, then you can shuffle them individually, but you can't actually change their positions or the character offsets.
      So when you've got a standard analyzer, you get the standard Tokenizer.  You also get the standard token filter, which does absolutely nothing.  It's a placeholder in case they need it in the future.  You get a lowercase token filter so once your string is split up into parts, it makes it all lowercase.  And then you get a stopped table filter.  So stop words are very common in your language.  In English, they tend to be words like the or and or but.  And you might not want to include them in your token string for mathematical reasons that become important later.
      So that's what goes into the standard analyzer.
      The standard Tokenizer follows the unicode text, which the longest this token can be used is 255.  And so now we know a little bit more about what was going on with the standard analyzer.
      So because it uses the union eye code text segmentation, it's going to break that hyphen.  And it really doesn't do very much else.  When you've got the standard.
      But because analyzers are made up of a bunch of highly configurable parts, you can tweet your analysis to get the results you need, which is really fun.  Because as we can see, here are some more because examples where the standard Tokenizer might have a different opinion about how to make things tokens than you do.  So here are a bunch of things that people may or may not look up.  They're all real, I promise you.  An apostrophe went missing.  This should have an initial apostrophe.  But this is how the standard analyzer will break these up.
      So in the title of the talk, I talked a little bit about precision, but honestly, mostly I'm going to be talking about from recall from here out, so if you want precision, especially when you're dealing with dictionary text, one cool thing that you can do is remember that your key word field is never going to be analyzed.  It's always going to be exact.
      So if you have a lot of control over your data, like, for example, you have about a million dictionary entries, and you know what the possible variant forms of all of those dictionary entries are, it's probably better for you to add all those variants to each dictionary entry as a preprocessing step and do it ahead of time, and then do exact matches.  So first, it will look for the head word, the bold word at the beginning of the entry if you're looking at a print book.  And then it can also check a bunch of other fields that are all of typed key word and get an exact match.  That would give you something way more precise if you're a user because what you're looking for is a very precise piece of text, dictionary entry that is associated with one or more forms of a word.
      So if all you cared about precision, we're done here, you can go to another talk.  But if you want to learn more about recall, we can stay here.
      So when you're thinking about recall, I find it helpful to think about having cascading options.  So, yeah.  Let's first look for all the data that we have that matches the capital D Dumpster, and if we don't have a lot of that or if we want to show a variety of forms of how this word is used, let's go and see if we can find lower case Dumpster or plural Dumpsters or even Dumpster-diving.  Let's give all the possible pluralizations and hyphenations.  Let's get I on you without the dots, and let's get marks for people who are too lazy to type the marks, which to be Frank, is everybody at this point, so let's not punish people for being curious.  Let's try and show them a good variety and an array of data.
      And remember the cryptoGermans.  People are just going to randomly capitalize stuff.
      And the way to get around this or to have this work great in Elasticsearch is multifields.  So as we saw earlier in our first mapping, we had the word field.  Of typed text.  And we didn't specify the analyzer in that mapping.  But we can say that same text, that same string that we have in our document, let's analyze it in a whole bunch of different ways.  Let's change-up our white space analyzer.  Let's use something called shingles so that we can see if we can match phrases better, and let's lower case things and use stemming where we will try and reduce.  And when you add multifields, you can catch as many matches for your data and query as possible.  So we'll see how this works.
      So if you're going to write a dictionary analyzer, and I'm sure you're all wanting to do that because it's fun.  You might use a white space Tokenizer, which is a built in one, which is a little less breaker than the standard Tokenizer.  Breaks at fewer points.  And then you could add a bunch of character filters, like, let's strip out HTML.  Let's change all of our quotes to straight quotes.  Let's take out some punctuation.  Let's get rid of single quotes when they're surrounding a word but not when they're in contractions.  Let's take out punctuation at the end of the line so that it doesn't show up in the last token of the string, and then get rid of random new lines.
      So with the white space Tokenizer, you'll see it doesn't break at the hyphen, so copy editors stay cool at the hyphen.  It doesn't break up things like control Z.  And because it doesn't have anything to do with HTML, it won't strip out your HTML text.  HTML tags are not the province of the Tokenizer.
      But they are what goes into the HTML strip.  Character filter, which is built into Elasticsearch.  And you can use escape tags to say, oh, you know what?  I want to keep the blank tag everywhere I see it.  Let's not strip that out.
      You can use a mapping filter as a character filter and make your own custom mappings.  So here we're going to change every possible quotation mark and apostrophe variant to just two so that we can make life easier for ourselves down the road.  So instead of having to figure out how we're going to strip all of those extraneous, single quotes, we collapse it down to one, and then we only have to replace that one.
      And this is how we do it.
      So this is a pattern replace filter.  Otherwise known as a regular expression filter, so I am legally obligated to give you the regular expressions warning.  And, in fact, I need to give you a double warning because it took me an embarrassingly amount of time to know that the regular expressions in Elasticsearch are Java regular expressions and not pro regular expressions, and I've always broken pro regular expressions, you know, they were good enough for Jesus, they're good enough for me.  And so anyway, that gave me a couple weird bugs.  But, yes, please understand if you're using regular expressions, please do so responsibly and not under the influence of any mind-altering substances and remember that in Elasticsearch, they're Java regular expressions.  And there's a really nice page that will show you the differences between Perl.  This is going to strip out the punctuation that's left after we strip out all the other punctuation -- this is going to strip out periods at the end of the line.  Because remember, we don't want to strip out periods in the middle of tokens, because then we're going to mess up things like IOU and MD and other abbreviations that use periods.  And then we'll also just take out new lines.  And then this is a kind of grab bag filter that's going to get rid of all the other punctuation that we don't think is a salient feature of the words that we want to look up.  So if this picture is in a string of A to Z characters, it very rarely has a word-like meaning.  Unlike the hyphen.
      So that's a lot of character filters.  But they all go into our dictionary analyzer, and so once we have all those character filters, which, remember, apply in order, then we'll go to tokennizing, and then we'll get our output.  So there are lots of other analyzers that are useful and the other is the stemmer analyzer, and that will use the standard because we want to get rid of the tokens in this case.  It will strip out the HTML and the quotes, and then we're going to have a bunch of token filters that happen after the string split into tokens.  So we're going to lower case everything and lowercasing is often a prerequisite for using a stemmer filter in Elasticsearch.  Make it lower case first.  The English possessive stemmer, bear minimum to take rid of the verb.  So it gets rid offings anded.
      If you want to learn more about stemmers, there's so much more to know about stemmers.  There are lots of arguments online about them.  They're very entertaining.
      E-mail me or tweet me afterwards and maybe I'll make another talk about stemmers because it could be, like, a did a.
      And then ASCII folding just takes characters that she that are not in the basic ASCII set like the marks we saw earlier and smooths them down into their basic ASCII representation if they have one.
      So when we run the stemmer analyzer overa string, like, I hate skipping but I love jumping, the tokens we get out are all lowercase, I hate and then skip turns into skip, but I lower case again but I love jumping turns into jump.  And this will help us make more matches down the road.
      Shingle filters are interesting.  They create a moving won't across your text that you set to a minimum and maximum size, and it outputs tokens that fall into that window.  So, for example, I want to catch a wave and the ice cream gives you these sets of moving tokens that I want.  I want to because we have a minimal shingle size of two, so the smallest number of tokens that can go into this are two.  And the biggest is three.  And we have output unigrams to false.  A unigram is a way of saying single words.  We don't need to bother to output them here.
      So now it's query time.  So we have analyzed a bunch of different fields and a bunch of different ways.  And now we're going to run a multimatch query of the type most fields across our original word field, word shingles, our lower case stemmed field and the white space field, which is the dictionary analyzer.  So what I've tried to do with this query is the use case that I care about is when a user looks up a word and wants to see a bunch of examples that may include the exact word they search foreor might also include close variants of the word that they search for.  Because if you're trying to figure out how to use the word copy editor, you might want to know how to use it in a sentence but not necessarily care whether it's hyphenated or not.
      And this is especially true forms of the verb.  Most people want to use an inflected form of the verb more than they want to use the base form.
      And in this query, your search term will be analyzed with the same analyzer as you use for each field.  So it will be analyzed multiple times in each kind of analyzer, if that makes sense.
      So here we're looking for just making it clear, we're looking for ice cream, the phrase ice cream or -- and so our first match -- hey, that's pretty good.  It matches ice cream.  It also matches capital ice cream, which is good because, you know, we don't really make that much of a distinction in English between shouty case and nonshouty case.  And then also, it will get you the more stilted hyphenated ice cream there at the end.
      So -- but how did this actually happen?  Well, what happens is it's trying to match the tokens that are output by analyzing the search term with the tokens that are output by analyzing the text.  And if there's a match, you get points.  And the more points, the higher it ranks.  And that is a very hand wavy description of a lot of math that goes on under the covers, but I will get to the math in a minute.
      So you can see that you get more points for a closer match.  Without the hyphen, matches more often than with the hyphen.
      Now, if you're, like, okay.  One of these fields I care about a lot more than another field.  You can boost that field.  So let's say that you decided that for every example in your examples database, you think it's a good example for one term.  Say, the term ice cream.  And you're going to save that in a key word field that's not analyzed, in addition to the text of the example sentence.
      So you can say, hey, if someone looks up ice cream without a hyphen, and ice cream without a hyphen is in the term field, that's an exact match, and that's worth may more to me than the other matches that I might get by jumping three more hoops of analysis.
      So you can boost that field so that it weighs more in the final calculations.
      And so then you get stuff like this.  And in this, we're looking for ice cream with a hyphen.  So now we get the thing that no human beings has ever said.  This is obviously some kind of alien transmission with a hyphen first.  And then you get the all caps ice cream.  And then you get the lowercase ice cream.
      So these results are somewhat less explainy; right?  I'm not sure why all caps came before lowercase.  To my native speaker intuition and linguist intuition, it would be, like, lowercase would be closer.
      So this is where you get into the math, and I'm going to hand wave about the math.  But Elasticsearch doesn't hand wave about the math because you can use the explain query parameter, and it will walk you through the math in its glorious, tedious detail.  So my suspicion is that the term frequency inside the text that was originally analyzed is somehow subtly different for these different sentences, but I haven't dug into it very much because usually, it doesn't matter all that much.  But for your analysis, it might be really important, so I want you to know that this exists.  And also, you can do all sorts of fiddley stuff with these numbers.  You can turn them off, you can mess with them, you could add multipliers, there's a lot of cool stuff you can do if, for instance, the math parts of Elasticsearch are way more to your liking than the language parts of Elasticsearch.
      So when you're tuning Elasticsearch, when you're trying to figure out the right match of analyzer for your data and analyzer for your queries, and how many different kinds of ways you might want to analyze your data so that it gives your user the best chance of finding it -- because there's no point in saving data that no one's ever going to find or use.  What that really involves is first, you have to know what your users are looking for and what they care about.  If you don't understand what they want, it's really hard to give it to them.  And also, how are they going to look for it?  Are they going to all caps type no matter what you do?  Are they going to add extraneous search terms because they think it's going to make things more precise when if you've got it set up in a different way, it will actually make things harder to find by adding noise into your search.
      Then you have to figure out how to change what you've got to what they want.  Maybe you should preprocess the data in the fields or maybe you should collapse the fields in your data if you have a first name and last name field but everybody who uses your phone number looks up a first name and last name, you might want to collapse those into a field.  And then you have to make a willingness to make tradeoffs.  So I could tweak the punctuation splitter.  We saw some examples like control Z and the name of the language that begins with a glottal stop that's represented by an exclamation point and really hard to say.  There are very few words in the English language that actually do have a plus sign in the middle of them.  And there are many more cases in the data where people are just putting plus signs in for decoration.  And it might be better for my data to take out all those plus signs and, you know, just kind of fiddle -- just say, oh, the people who are looking up control Z, they're just going to get a lot of examples where they may or may not be a plus sign, and it's not going to give them an exact match.  So you have to have a linguist to make tradeoffs because no analyzer is going to be perfect for your use case.  And then you really have to enjoy fiddling around with this.  If you have sat through this, and you're, like, oh, my goodness, this would lead me to self harm, then perhaps Elasticsearch is not the tool for you.  You have to enjoy spending time with a cabana console and playing around with different analyzers and, you know, going down the dark and twisted path of regular expressions and really enjoy it.
      I really enjoy it.  It's a lot of fun for me, and I think that once you have the basics of analyzers under your belt that you will also have fun trying to mind read your users, figure out what they want, check their old search logs.  You don't even have to mind read in some cases.  And tune your data so that it fulfills their needs.
      So this is kind of been a little bit of a basic introduction, especially for the guy from Elastic.  Sorry to keep calling you out.  But this is too good not to do.  But there are plenty more resources about Elasticsearch, and you should definitely seek them out.  Here is a photographable slide of these resources.  The Elastic online docs are amazing.  They are very good.  They are highly detailed, and they are clear and easy to read.  Elasticsearch, the definitely guide is a bit out of date, but it's still very useful, especially tug the underlying concepts and, you know, even though I said I didn't care about sharding and all of that stuff, the chapters on that are really good too.  There's a book out from Manning called relevant search, which goes into far more depth about tuning your data and creating analyzers and all sorts of cool things you could do with the math for the underlying TFI DF frequencies.  If you really don't think you're going to be able to sleep tonight without understanding unicode text segmentation, there's a link for that.  A link to explain the differences between Perl and Java-related expressions.  And if you want to know why linguists get so vocal about search engines, you should read the papers about the linguist search engine and understand how much modern search engines just drive linguists crazy because they take out all the stuff we care about.  You can't really search for hyphens.  It just shoves all the text down to, like, the lowest common denominator, and you can't look for linguistic variation at all.
      But thank goodness we have Twitter.  Twitter doesn't do anything.  We can do all sorts of stuff with Twitter.
      So if you want to know about how linguists think about search engines, there's a lot of great documentation on that.  It was a project out of Michigan.  Very interesting.  And if you have questions or better yet if you have answers, please find me any time, and let's talk about Elasticsearch.  Love talking about Elasticsearch.  Please also find me to get your semicolon appreciation sticker.  I have a lot of them to give away.  You can appreciate the semicolon from whatever distance you deem appropriate.  If you appreciate semicolons when they are far away from you and not in your code, I'm totally okay.  With that.  If you appreciate more when they're in your code but not in your pros, also A-okay.  Just appreciate the semicolon however you would like.  And also, you can -- there's my Twitter handle, the word for Wordnik API.  And I also want to thank the lovely people of flicker who make their images available through creative comments.  This whole presentation is CC by NCSA.  If you would like a copy of the slides so that you can give a different talk with all of these slides in the same order -- actually, you can change up the order because it's not ND, you know?  I'm happy to share the slides with you.  Just send me a message by whatever means you can find me.  Although, not Facebook, please.  I don't really find people on Facebook.  So thank you very much.  I really appreciate your kind attention.
      [Applause]
      I'll go back to this one just in case.  So I guess I have, like, one minute for questions, but easier to come up here.
      Captions provided by @chaselfrazier and @whitecoatcapxg.
      
