    "Just-So Stories for AI: Explaining Black Box Reproductions"
    by Sam Ritchie
    
    >> Am I live? Is it time? OK, everyone welcome to Just-So Stories for AI. I'm Sam, I work on the machine learning infrastructure team at Stripe, and what my team does is we build tech that helps other engineers build models to catch fraud. We'll go into this more, but that's all you need to know for now about me. So the outline of the talk is we're going to go into that, we're going to go into some techniques for detecting fraud. We're going to go into some of the choices you can make when you're an engineer who's attacking this problem. The typical thing you hear, there are interpretable models that you can understand and then there are these black box, like super accurate neural networks, random forest, things like this, that are just hard to get, but they get the job done. So we're going to talk about why that's not necessarily true. Some of the most complicated models often are the most explainable. I'm going to anchor that with a technique we use at Stripe to make very accurate predictions on fraud for our customers and then offer them an explanation of why this, you know, insane model did what it did. Whether it's a true explanation or not, that's up to you. There's a number of ways to interpret this. And finally we're going to talk about why this is an important problem, why this might be the most important thing you could be thinking about right now if you work in these systems. This is the door like I have to say the AI safety thing into ethics and morality and free will and all of those things. OK, so this is, you know, let's talk about you. You are, you know, you're young, you've always wanted to start a business, you love animals, you're itching to sort of affect the world in some way, but you're a little lost. You're browsing Reddit one day and you come across this photo and so you realize that like, this is real. Like people have this thing they're doing where they're grooming their dogs into geometric shapes and this just crushes you, you know, you realize you have found your life's purpose. You're going to stop this happening. So you come up with a business plan, you're going to take money, you're going to put it into ad campaigns or something, it's going to go to dogs, love and happiness is going to come out the other side. And this is my pitch for what Stripe is if you don't know. Stripe is a company that makes it so easy to start a company and take money from people, that even this idea can be a wonderful, legit business. Really that is what Stripe is, but Stripe is also a company that develops tools for people working with money, you shouldn't have to worry about it. Fraud detection is one of those components that we have. So you're humming along, things are going great. You're I guess accepting some number of donations and fraud strikes. You start seeing in your dashboard charges that don't really make sense. They all -- and this happens to -- this is the purpose for the example: Sites that take donations often see a kind of fraud called card testing where people will download from the internet, like a big list of card numbers, or just like try them, which seems insane, but if you let them get through, it will work, and they just start running like one-dollar donation on donation sites. They're taking advantage of your love for these puppies to test out cards and the goal is to get cards that pass the dollar donations so they can steal them and get some money. If you hit 1% or so you're going to start having issues with visa, or I guess card issuers, but through Stripe, so you pick this component off the shelf. This is the thing I work on. This is a component called radar. So radar is a component that Stripe gives you to stop fraud so I build infrastructure that kind of sits behind this thing. So the main weapon you have is these rules, so you're able to write rules that will, you know, effectively either block, allow, or put into a review queue, different charges. So a rule is a model. A rule is the first model we're going to look at today. And we're going to develop this concept. A rule is a model with a built-in explanation. So it's a model in that you put in information about the charge, the things you know, they get compared to these rules. In this case, if the card is a prepaid card and it's over a $1,000 charge and it's not from the US, go ahead and block it so the input is those three features they're called an the output is a decision to block or not and on the bottom you have stats about what would have happened if you deployed this to block your charges. Again, the question then is how do you come up with these things? Accuracy is not enough. Typically fraud rates are low. You're not going to get -- you know, what this picture shows, we have fraudulent charges in the Grayed gray area on the left, legitimate charges in the right and this circle in the middle is like just some rule, let's say that in this case is getting some fraud and getting some not-fraud. So when you come up in these rules you're not in your head thinking about accuracy because if you have 1% of your charges as fraud, you get 99% accuracy out of the box. Again, this is if you decided to do no rules. What you're really trying to do is optimize these two metrics that I'm going to introduce now, because we need them later. One of them -- the idea is that you want your rule to be specific enough that it only lives on the left side of this, so your rule circle you want to move left. You want to catch when your rule, you know, does its thing, only fraud, right? 
    The problem is that fraud doesn't have like a really obvious -- there's no algorithm for detecting fraud. You're just kind of looking at patterns and guessing. So you might say, you know, dollar charges from this place. The more specific you get, the more charges you're going to get, but the less overall fraud you're going to catch so you kind of play this game with yourself where you're coming up with a bunch of rules, each of which lives in this left side. Overall you want to cover it as much as possible, but you want to try not to block good charges. So that first good idea is called precision, how precise is your decision and recall is how much of your overall fraud you catch with the rule. So anyway ...
    Why is this not a great solution? Like, the reason why is that fraud evolves, right? So you come up with a rule, it's a great rule, you really like, you read on the internet, basically you did your SEO homework for fraud and you deploy the rule and then people just start pasting in. Mean while you're catching the stuff on the right that we cared about. So fraud a adapting all the time in your little example of your business now you're spending all the time in your dashboard being suspicious of everything. Like, this is not a good situation. So how can we make this better? There's only answer to this question: The answer is to sprinkle machine learning on the problem. So thank you, XKCD.
    The answer is basically to bring in more powerful models that can work on the data behind the scenes. So again, this is kind of the full product of radar, you have rules and then you have us doing what we can to lessen that load and catch fraud. So what's ideal here? How do we want to use machine learning? Ideally, we would catch all fraud, right and ideally for every charge we block, we would offer up an explanation for why we did that and as we saw, a rule has a built-in explanation, where, well, why did you block the charge? Well, because you told me to. This is less obvious as we move on to all of the different models for machine learning that you can possibly grab. So this is a Heatmap, basically the darker something is, the more it's able to kick the butt of other models. The higher up the more accurate you are. The lower, often there's this correlation between models lower on the list are more interpretable, models higher up are more accurate. So you have this tension here that people love to talk about. When I made this slide I was surprised to see there was a passive-aggressive model. Four from the bottom. I don't know what this is. Come find me afterward and tell me if you do.
    So you know, to anchor this, let's talk about the decision tree right in the middle. So again, a model is a function, a little bit of code, a model is function of features of things you know about the world, to some prediction, right? Ideally this is what you'd want to be able to ask for each charge that comes in. How likely is it that this charge looks like something we've seen before? How do you implement this function? You can't just sort of write it. There's no obvious way to do this. So a machine learning model, this function, the process of training a machine learning model is the process of using a bunch of data you have to basically implement that function for you. So you take your training algorithm, you take a dataset, you try to soak up as much information and meaning out of this as possible, and the result of the training algorithm is a model which again is a function that hopefully will, in this case, when you pass it in these red charges, so as these things come through the pipeline, you want a high number, a high probability of fraud out for those charges and a low for the other ones, so you want a model that matches the patterns that were in your data before. Again, they might change, but the better your model is, the more accurate. The better it's soaking up all this meaning, the more you're going to soak up these edge cases that rules just can't catch.
    OK, decision trees. A decision tree is kind of the second model we're looking at after rules. We'll talk about how this is actually a collection of rules, but what a decision tree is like a 20 questions game actually. Like you made this thing, your features come at the top, you follow the questions, and what's at the end, what's a leaf of the tree is the how many of those are fraudulent, right? And what you're looking for here is you want every path down this tree to be as specific or as precise as possible, so you want it to be that there's a very high confidence or a very low confidence. Basically you want to be very sure, once you've walked all these branches and overall you want every charge to sort of find a slot in the tree. So in this case, what's nice about this choice of model -- say we were to pick this and we were able to implement this behind the scenes we have this thing that every path is also a rule, right? So if we block fraud for a user we can present to them this explanation of why we did this. We could say if you had taken this rule and if you had deployed it at this time, it would also have caught this fraud. So right now if the amount in USD is above 20, this is the leftmost branch in the tree. The only difference is the rule just straight up blocks or doesn't block. There are these probabilities that the edge of the leaves, all you need to do to turn a decision tree into a set of rules is pick a threshold. You just sort of behind the scenes choose something and you deploy it that way. So a decision tree. What's good about decision trees? Decision trees are interpretable. What does this mean? It means that you can simulate the algorithm yourself, so you've got your function, you kind of have some trust of that works, that's your you know, your precision, your recall, but you can then look inside the black box and you can run the algorithm through for every charge that comes in so every decision has this built-in explanation and for pa product, this is important. For a radar this is important, because you have to have trust with the user, right? We'll look at an example of what happens as we get more accurate and less explainy, but if you just start blocking charges that are really subtle, you might be right, but people aren't going to know why you're doing it and they're going to be suspicious that you're blocking good charges and stopping their business. So this is kind of the tradeoff. This is why explanations matter in the fraud context.
    So not a black box. OK, so why are decision trees? What's not good about this? Shallow trees are just not very accurate. Like, things are complicated in life. Sorry. Two decisions are often not enough to explain why something is fraud. So if you want to be accurate with these things, you have to grow them really, really deep. You have to have a lot of branches and the problem them is like a really deep -- decision tree is not very explainable, right? If you show someone a rule that's 50 or 100 or 1,000 predicates long, it's insane. It's never going to work. They conceivably could have written it but there's no way to hold this in your head. The other problem is the deeper a tree fits the more it overfits to the data it saw. In a crazy case every single row of your training data can be a path in the tree and the problem in that is your new fraud probably won't look exactly like the old fraud so you've frozen this moment in time. It's probably not going to apply to anything in the future, which is the whole point of this. The final problem is that we know a lot more about users behind the scenes than we are really like OK releasing, so there's an 80% chance that Stripe has seen a card if it shows up even if you've just launched your business, so we can't really expose certain pieces of data like oh, this is a really unlikely purchase for this user to have made at this time of night from this kind of business, here's what they usually do. So we're little shackled if we only use decision trees, right?
    OK, so we gotta go higher up. We've got to go to a higher floor. Decision tree is right there in the middle. We'll talk next about random forests, so random forests are like second from the top. This is what we actually use. This is a generalization of the decision tree idea and I'm going to go through how it works and it's kind of wild, just to give you an idea of what it means when people say that a random forest is a black box, that it's hard to understand why it's doing what it's doing. OK, so the random forest: The idea here is that you -- the intuition here is you have your data, you build a decision tree, OK, that's fine. If you had more data, if you just had many, many datasets, you could build and you build a ton of decision trees, maybe that would be a more accurate way to assess what's going on, right? If Stripe had 200 times the data, maybe we'd be able to grab 200 times the data out of T there's a thing called bootstrapping, I still don't have it in my head why this isn't an insane idea, it won't work. Each time you do this roughly you'll duplicate some stuff, you'll miss some stuff, you'll end up with roughly like two-thirds of the amount of data you had going into every time you do this bootstrap sampling but you can pull as many samples as you want. So the idea is you just keep doing that, you just randomly sample your data and you just keep training your decision trees as much as you want and you just keep going and going and going, and beyond that, each tree you only send some random subset of the features that you knew about so you don't give it the opportunity to fork on every possible thing. You just pick it's typically the square root of the random number of features, you just randomly send off a batch. Final thing, when you have a prediction, how does it work? This is kind of the why the simulatability question. You take your features, you ask every tree what it thinks and then when you're done you just average all the answers together and this turns out to be really, really accurate.
    Sort of strangely, I would say. It's very flexible, it will pick up on patterns in the data that are, you know, very, very subtle and that a decision tree would overfit on and you can basically just keep cranking up the number of trees and keep making them better. So this is good. The con is like you can't simulate this. It's a black box, right? I always wonder if I would get a chuckle on this. It's like the only place that I could break this out. So why is it a black box? Like it's a black box because it's not simulatable. That's typically what people mean, I would say. Why is it not simulatable? One model that we have out now at Stripe is a couple hundred trees, each has that's 12,000 splits, 2.4 million possible explanations. So this is not OK, right? This is like asking someone why they pushed you down the stairs or something and they just give you a printout of the entire state of their brain at that time.
    [laughter]
    
    That's the explanation. That's it. That's why they did what they did. Totally not OK as a justification. And this is what would happen naively if we don't have a way of explaining these black boxes. I actually hate this. This is really what we do if we don't have an explanation: You just say you know what, there's lot of contributing factors of the risk level. There's a lot of stuff, it's a complicated world out there, just relax, just trust the model.
    
    [laughter]
    
    It's kind of patronizing and it doesn't really work when you also have these rules that are so inefficient and not good, but it's just such a beautiful story, I just want to believe it. So what we want is something like this. This is a little bit better, all right? You want something that like this is an explanation that says this card, here's the last four numbers, has been linked it he to a large number of attempted payments across the Stripe network over the last hour. Oh, my God, that kind of sounds like fraud. I don't know if it is. But if that's what the model thought, I'll go with it. Another one here, it's been used for an unusually large IP addresses. So this is an explanation generated off of a random forest model that kind of explains, it has some truth to it. It is true. Whether it was kind of the reason the forest did what it did, I'll talk about how we came up with it and just keep this in your mind and see whether you agree that this makes sense.
    So how do you do this? The technique we used to generate these is a technique called post hoc explanations. The idea is that you give up any idea that you could simulate the model. You just treat it as the black box and you train another little model that bolts onto it and tries to explain why it's doing what it's doing, right?
    And so like the intuition behind why this is easier is I don't know what to bring up like conspiracy theories but it's really hard to make real predictions, but once something happens, like, everybody's got a story for why the thing happened, right? Or I guess another way to get this across is this -- I guess my mom will say to me, like, oh, what are the chances of these people running into each other? After the fact the chance is 100% and you can come up with any story to explain why it happened, right? So we're explaining what the model is doing, but we have this lower-effort task in front of us, right?
    OK. So we started with this paper, it's called a model explanation system. It's fantastic, the concepts it puts out are great. I was totally bold over. I have no idea what's going on deep in this paper so we sort of were inspired by this paper and members of my team extracted from this paper a method of explaining black box models which we'll go over now, so the intuition here that came from this paper is what is an explanation? It's like we talked about before, like for a black box model. An explanation is a rule that, had you deployed it, would also have caught the same charge and possibly would have agreed with the model on a lot of other charges, right? So its precision is high. Every explanation's precision is high and the advantage is that we can generate a ton of possible explanations and when a charge comes through and we know what the model said, then we can do a conspiracy theory thing and turn around and pick the one that we like the most. Picking out a time is really hard, once you know what happened, it's easy. OK so let's build up to how we do this. The intuition here is that as we talked about every path through these trees is a rule, right so there's like millions of possible explanations here, so we do what any good statistician would do, we start sampling these possible explanations. Any of them might work, right so we use the tree as a sort of a source of possible explanations. Every split in the tree we'll call a predicate, so the name here is the name of the feature that the split operates on. The operation is like greater than, less than, equals, doesn't equal. Super-simple, and then the constant is what the value is so you can check what is the category, you know, is the country in this bucket, example. So for example here unique IP address in the last 24 hours, that's how many times has this card been used from the same IP address in the last 24 hours. So an explanation like we said is a list of these things, a list that are all true, so you add them all together, and they're sorted by how precise they are. Precision again is like four explanations if the explanation applies to a charge, how many of those calls did the, you know, the big, the black box model, also make? If it's 100% precision that means that this explanation may have applied to the charges, but every one of them agreed with the model, right? If you have low precision, that means it does not agree at all, it's not a good explanation, so the structure of an explanation model as we said is a list of explanations sorted by their precision, so every explanation, what we want out of this system is we want to come up with stuff, make up explanations, each one of which has high precision, so it applies to some subset of the charges very well and we can keep generating explanations until we get coverage over everything, until we get high recall, the idea being that for every fraudulent charge or every positive example there's some explanation we can pull.
    OK, this is the most dense slide I have, and like, this kind of sucks, just bear with me. This is the algorithm to do it, I gotta do it for this talk, but it's less like don't think you're supposed to memorize this, because obviously nobody will. The goal is just to show how weird and arbitrary this is, and how unrelated this is to the idea of simulating a model that we had before, yet the result is actually pretty good and kind of makes sense. So the idea is first you just generate a ton of possible predicates, so you just have your set of examples, you start moving through the tree and just I canning things out of your millions of possible ones, right? Step two is you weight them all by the precision they have, so you weight them by how much they agree with the model. Given like some lower level of recall. So the idea is if you've got something that perfectly predicts the model this is not good. Finally you pick about a thousand of those weighted by their precision, so you sample those, you set those aside, so you have a thousand of these things, and you go ahead and you take -- you get the NXN like possible two predicate explanations out of this. This is, by the way, why you can't try everything because once you get these longer explanations things go to hell quickly. You go make your million, 2-level explanations, you go back to step three and you just keep going at this. As you go as deep as you can expect people to understand. We don't, like, you know, target that per merchant, like everybody is, you know, there's no one who's like specially targeted to handle like a 20-level-deep explanation. 6 tends to be the limit of people's patience.
    So you are end up with 6,000 possible explanation, right and you go through and you sort all those by their precision. You pick one and then the final step is you go back into your set of calls that the model made and you and everything that your explanation explained, you flip back from fraud to not fraud, so you remove it from your dataset and then you go back and you do it again and you do this like 80 times. So there's this insane process of just trying out a bunch of stuff that is not tied to like the reality of whether something is fraud or not. You are H. you're just trying out these potentially plausible scenarios and trying to find the most plausible thing that fits your model's explanation and what you end up with is a list of explanations, any one of which could apply to a new charge that comes in.
    OK, so internally in our systems what does this let us do? What this lets us do is whenever a charge comes in, we're able to give a prediction for all the features that we know about the charge, we're able to give a threshold and then we can supply this explanation, which is the list of these predicates, right? So why was the prediction 48%? Well, the real answer is all the connections between the neurons and who knows what the hell is happening. But a plausible explanation that might also explain the same behavior is that, you know, the card was used on more than 10IP addresses in the last day, the billing country did not match the card country, and it was a Mastercard. Obviously!
    [laughter]
    
    And then we dilute it a little more. Like you could just show this, but we just take the top thing, which is the most precision and we go and we render it. We just have a like, you know, template for each one of these, so this strips out even more information and says, this is an unusually large number of IP addresses that this card was used from.
    OK. So this always makes me uncomfortable to think about, because in what sense is this like an explanation of what the model is doing? Like, there's no mention of the truth here. We're not talking about -- you could just as easily -- let's put this way. You could just as easily train a model on the false positives of the real model and you could offer someone -- this is not a product anyone would take at Stripe, but you could also give someone your plausible reason about why the model might be wrong, you know? So this explanation for why you might want to doubt the machine learning tech. It does not care what the truth is it's just trying to justify I guess what's going on behind the scenes.
    So I think of these as just-so stories. This is the story of how the elephant got his trunk, it had nothing to do with evolution. The elephant got in a battle with the alligator or crocodile that stretched his trunk. We're going to talk about a few other ways in black boxes are explainable, but it's not tied to what's actually going on inside the model, whatever that means.
    You can explain anything well, even if there are systematic errors underneath that you might be missing and so a final comment on this is this is a totally inappropriate technique by itself. A user should only trust this. If they have some reason to believe that inside Stripe or whatever organization somebody else is working hard to make sure that the model is actually being accurate and fair, right? So let's explore that.
    So the observation here, what we're going to go into and what the model we just talked about reveals is that black box models are explainable in certain ways. They have much richer interpretations than a simple model like a decision tree, so overall the question of why did the model do what it did, it's not really answerable globally because it's meant to absorb information from huge amounts of training examples, but you could learn a lot. You could explain individual decisions very, very well. There's a technique called LIME which it's got a great Python library it stands for locally interpretable model explanations. It's based on the idea that what a black box model is carving up the boundaries between fraud and not fraud in a very, very linear way. What that means is that for any individual example, you're pretty likely to be close to some decision boundary, right? So the idea here is, you know, what would have to change about the individual charge that would make it go from fraud to not fraud? Right, so if you have this black box with the rich internal representation, you can start asking it all these questions, and you can get out a pretty nice answer and this has a beautiful -- like image data is really nice for this, right? Because there's no way even our old explanation system would work where the features are individual pixels. All right, so this is an example of the LIME paper. This is a Husky that was classified as a wolf. Do you kind of get why it did what it did? Right? It kind of makes sense. So an explanation for an image is the pixels that if they were different, would most affect the outcome of the decision, right? So for this, oh, man, you know, the explanation is actually the snow around the face.
    [laughter]
    
    Totally not obvious. Again, you had this machinery for coming up for explanations, it turns out that in your training data you had a lot of wolves sitting in the snow and a lot of Huskies on couches or whatever, so this one example, say you work this for a few months, this like crushes your life. But it just tells you everything that you need to do to go back the model better. So this cannot happen for a simple model. Another example here, using the same software, by the way, this is a prediction of probabilities of whether this email was from like -- I think these are from use net mailing lists. So it gets it right. It's like wow, we're talking about Darwin fish, these are some atheists right here I'll tell you that. Why did it do it? 15% of the word was from the word posting. Host contributed nor 15 percent, right? Not helpful. You've overfitted heavily on this aspect of your training set and only a black box model is sort of able to be explained in this way. We gotta go, and this is not really tied to my thread but it's just so awesome. There's this technique called AI rationalization. The idea is you go back to the first sort of thing. You disconnect any meaning in the model from what's happening and you try to get, say a model that's playing like a game like Frogger, you try to get it to explain what it's thinking at that point in the game. An example of the output. This is an AI that can play Frogger, it's gotten to the logs. And it says amongst the logs, dang this is hard, I need frogging to avoid death from the edges. It's beautiful. There's a great paper that talks a lot about this. It discusses this case of how black box models are much more available to be examined. But a strange thing comes up when you read these papers. You get these comments about, like humans being black box models that have post hoc explanations only, right? So human decisions might admit post hoc interoperability despite the black box nature of human brains. Like, this is what we've been talking about, but the human mention is a little startling. Peter Norvig, who wrote the book on AI, has a similar question. He says you can ask a human what they're doing but you don't really have any insight into their decisionmaking process. They do what they do and then they just make up something. They make up some BS. As you dig deeper in this rabbit hole this is an example some of you may have heard of: This is supported by the research. Roger Sperry in '68 has a bunch of studies on split-brain patients, this is a case where you have someone with grand mal seizures, the only way to stop the seizures is to cut the corpus callosum. The experiment you can do then is you block someone's visual field, right? Your left and right eyes are connected to different hemispheres, you flash the word egg in front of someone's left eye. Their right eye can't see it, you ask them what they saw and they say I didn't see anything, right? Nothing happened and they say reach behind the screen with your left hand and pick out the thing you didn't see. That doesn't make any sense but let's do it, most of the time they pick the egg when they said the word egg and you say what are you holding and they say I don't know. I just picked up an object and you show it to them and you say why did you do that and every time the person has some BS explanation. They say, well, I had eggs for breakfast this morning. This seems to be no way to understand that and your right hemisphere is doing the thing, it has memory, its conscious and you're just sort of watching it, the part of your brain that's responsible for language is watching what's happening and trying to take ownership of the decisions. Very weird.
    So the sense you get reading about models and realizing that you are a model
    [laughter]
    
    This movie's 20 years old. I can't explain this joke if you don't get it. You just have to see it. It's so crazy at the end. So...
    [laughter]
    
    You know, this is this is an important like concluding thing. This book Homo Deus is the idea that we're on the same playing field, that the assumption in these comments is we're models, these machine learning models are models, we're on the same playing field and we're rapidly becoming outdated right? Everything is information, everything is algorithms, everything is data. The whole point is to make better decisions, why should it matter whether we're making them or not. We did this with driving, right? In a few years humans are not going to be driving. You're not going to put an ape behind the wheel when you can have a an algorithm. So why try to explain your models? Often what people -- like this line of thinking is often accompanied by this thought that like this is a harmful restriction on these models, right? Like there's a metaphor from a book by Max Tekmark, imagine yourself living amongst four-year-olds and everything you do you have to find a way to explain to these little four-year-olds. At some point it's going to become totally tedious. And if you could just do what you need to do without having to explain it, your shackles would be broken. You see comparisons to things like medicine. It would be immoral to withhold antibiotics, so I think the crux of the answer to this sort of critique of explanations, of needing explanations is the question of goals, right? Like for most things today, we don't know how to encode everything we care about in the goals of a model. So this is the -- another quote here, in the rush to gain acceptance for machine learning and to emulate human intelligence, we should be careful not to reproduce pathological behavior at scale. So explanations are important because they help us clarify what we care about. They're objections that we have that if we don't encode them in the goals of a model, the model is not going to care, it doesn't know anything about meaning, right? So explanations are almost the tether we have on maintaining some relevance, rather than just letting things spin off into meaninglessness. So explanations are the crux of a lot of approaches to AI safety research. DARPA's got a program. Berkeley is the center for human compatible AI. People are honing in on this, and a big thing that I've been thinking about lately is there's -- this is a paper about it, but the EU's data protection regulation has put into law this idea that you have a right for an explanation from an algorithm that makes decisions about your life. So an explanation is a civil right from this point of view. Now, I hope that in this talk we've talked about a few kinds of explanations. You'll see that this is -- it's not obvious, this is a good idea, but it's not obvious that this is the solution or that this is going to do what people think. Norvig's real point in this interview is you know, say you apply for a loan and you get turned down whether it's by a human or a machine, the explanation is not tied to reality. You can't tell from the explanation, right? So what he's actually recommending in this, which is the concluding point I have, is that explanations are important. Again as this way to potentially clarify our ethics and what we care about, and as wonderful as these beautiful technologies that can just predict and, you know, generate efficiency for us are, explanations let us see outcomes, judged by interrogating these black box models, whether or not these things are tracking the things we care about in hour societies. We can then go bake them back into our models and almost formalize our ethics in a way that we've never really had to. So in conclusion we talked about a lot: We talked about some models. I hope I've convinced you that black box models are in fact very explainable. Don't trust anybody who tells you otherwise. This is a really interesting accessible area, it's a great window into, you know, questions of AI safety, it's something you can be doing now and I would almost say that if you're working with models, like, you should be building these into your products. So be suspicious, and, yeah, come talk to me if you want to know more about this. And thank you so much everybody.
    [applause]
