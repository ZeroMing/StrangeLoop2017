    "Observability for Emerging Infra: What Got You Here Won't Get You There"
     By Charity Majors
     All right, I just got off the plane from Dublin. I left at least half my brain on the other side of the pond. 
     
    
    ... ... (apologies for technical difficulties at beginning of talk for the captioning)
    >> At the end, I was like, oh, yup, yup, he's doing this better than I ever could, that is cool, and since I have been very much in the camp of, you know, yeah, increasingly the visibilities is a specialized skillset, understanding how that's even happening in your software is really hard and it's getting harder. So he said monitoring systems have not changed in decades. Like fundamentally. There are in this room still running angry fruit salad of doom, right? Thank you. It's terrifying, we're still running that shit. So obviously, I'm receptive to that message. I hate monitoring, but I love debugging, like I love strace more than life itself. I love firefighting, but deny it if I'm sober. I think that it kind of took me by surprise that like observability became contested. We do have to spend some time on terms before we dive into t but I feel like the neat, the core of this talk is just like complexity is exploding, in a nonlinear way but the tools that we have for understanding our systems are very much a product of the LAMP stack era. Assumptions like there will always be a host to gather metrics by. Every single person out there charges by host. It's crazy.
    Or the idea that all the things in slash sprock are all the things you should use for debugs your systems. It's moved from systems to code, right? If you're containering, you found that out in a hurry. Our tools are fundamentally designed to answer known unknowns, and they do a good job of that.
    In Greg's talk he defined it like the act of observing blah, blah, blah, blah, and this is outdated. observability is a term that's taken from control theory, or so Wikipedia tells me, I don't actually know. The way I've always heard it and the way that I think it's relevant for us is how much can you understand about how your software works by looking at its outputs, by looking at the things that you have, like the little bits that you've planted, whether it's log lines or whether it's events, like can you craft a story, like can you have all of your dashboards go like this at once? OK, that's useful, guess, only if you couple it with enough intuition, and like behind the scenes knowledge of how your system already works. It's not helpful if you're trying to learn how your system already works. Sorry, rant, I'm only five minutes in!
    Which is why we drink.
    [laughter]
    
    I think observability is a super set. Monitoring is -- it's operability. Like when people say monitoring is not for ops, people have been very indignant about this for years, actually it is, monitoring is for ops. Ops is not a bad word. Developers should do ops. But monitoring is fundamentally about checking on your systems, making sure that they're still OK, you know, with the defined parameters of OK-ness. This is an operational task, instrumentation is, you know, how you develop a service, which is also not a bad word, because awesome engineers develop software. I find it so childish, like ops is everything. No it's not. Monitoring is for ops. It's bias towards outages. Failures, because that's what we care about when we're operating software and it's heavily toward known unknowns. Once something is known, you can monitor for it. When you see people talking about this massive explosion of metrics once they start trying to monitor everything, well, that's because you can't. You can monitor for your known failures.
    Observability is about monitoring, it's about instrumentation, it's about answering questions about your system when it's a bad -- when it's an outage, when it's a failure and when it's not, right? And when you're just like, it encompasses the kinds of things that business people need to know to do their jobs well.
    It encompasses debugging, it just encompasses like how you answer questions and if you're wondering how you can tell if a system is observable or not, can you ask it the questions that you need without jumping in and running command-line tools which we all know and love and we're all supposed to be working our way out of using. It's fine. It's totally fine!
    And the reason that we care about this is because complexity ask rising. Now, sometimes people start talking about complexity and I just start grinding my teeth. What do you mean, you know? And I hate it when they use this word, they just throw it out there to cover the other words, that they aren't being very precise in their thinking, and they're like, oh, well, it's complexity. It bugs me. But it's also a real thing. Now, this laptop here is plenty fucking complex, right, but it's not really what we're talking about when we're talking about complex systems and a distributed system is technically any two nodes that are talking across a network. Distributed like the computer science of distributed systems is at heart, it's a way of thinking about, reasoning about complexity, right? It's about moving that complexity around to different parts of the -- of your architecture, where you can deal with it better or perhaps worse.
    Systems complexity is -- well, all right, I drew you a graph that represents the state of the art in computer science.
    [laughter]
    
    So now you know it's true.
    [laughter]
    
    how to measure complexity of systems could totally be a talk in and of itself. I went down a giant rabbit hole yesterday while I was finishing my slides. Software complexity is measured by size, distribution, users, number of types of components, storage formats, there's hosted volume, how much information is in the source, that's so cool. Infrastructure complexity becomes exposure to the external network, storage types, oh, my God, remember back when there was the database? You had to choose Postgres or mySQL? I miss those days. Yes, you should choose predictable software when you can, but when you're building something new so much of your competitive advantage comes from rolling the dice on the newer, greener technologies, you know, and I find it really condescending when people are like, well, you should just always use the boring stuff. It's like, no, you don't know the process. That's another rabbit hole. Anyway. So I'm going to using terms LAMP stack and microservices even though I know they're not really accurate, but just two umbrella terms. Like the decade that we're embarking on now, there's a big difference in complexity between the LAMP stack and parse a couple of years ago. When we were in upload, cloud code, you know, requests could loop back in multiple times. And that's why we built honeycomb, honestly if you rewind the clock, we had built the system that was fundamentally undebuggable by around the time we got it acquired. So the best engineers in the world, it was not undebuggable, but it would take hours or days, weeks to track down these bugs. But as complex and interesting as that was, like, Facebook's better or worse, I started to say worse. Better. Let's go with better. It's more like the electrical power grid, right? I feel like if you want to model systems correctly in your head for like the next generation, think about, would this work for the electrical grid?
    Right? Because there are local problems you know nothing about but you have to treat them anyway, there are systemic problems that you will not know about if you're only looking at local problems.
    It gets really fun.
    Let's go through some example problems. LAMP stack. These will be very, very familiar to all of you. Someone complains, photos are loading slowly for some people. Why? Well, OK, app capacity is exceeded. Errors or latency are high, database is saturated? You know, what all of these have in common is you can pretty quickly tell where they're coming from. If it's a bug in your code or whatever, you want to attach a debugger and dive in and do a deep dive there and that's it. And this is a very easy system to monitor.
    Monitor these things. Done. Characteristics of the system. Known unknowns, blah, blah, blah. It's friendly to intuition. You get a page, you don't even need to look at the dashboards half the time, but of course you have them. Many, many, many dashboards. I fucking hate dashboards.
    [laughter]
    
    And the health of the system more or less accurately represents the experience of the individual users who are using it.
    Cool!  Best practices. Monitor it a lot. I don't know. Like it's not too hard. Write a run book for things that exceed the ability of like automation.
    >> So theory are some examples that we had at parser Instagram. Any micro-services that happen to be running on c2.4 and PIOPS storage has about a one in 20 chance if it went to disc. And the hashing fan-out model makes certain segments hit that more often. This is a bitch to track down.
    [laughter]
    
    Or like, you know, the Canadian users who are using the French language pack on this version of iPad, like caching doesn't work. Bitch to track down. Or like you know, this newest SDK gives you a feature flag so you can run updated queries sequentially, not even a bug. Like, what am I supposed to watch for?
    You can't!   These aren't problems, they're symptoms. And I had many more slides of these, believe me. It was like PTSD, the trauma just keeps coming.
    [laughter]
    
    But like any of these users write in and they're like, you know, parse is down, they'd be so angry at me and I'd be like parse is not down, motherfuckers, look at my dashboards and they're all green.
    [laughter]
    
    Every single one of them would be green. I'd get super-angry. But like, I argue with them and I'm just losing credibility, right, it doesn't make the performance any better for them, they can't log in any more than they could before I told them they were up. And this was like the -- this was the meat of the problem. When we got into Facebook and we put our stuff on the SCUBA, and they had pushed all of these different tools for us, none of them made a difference. We were still reacting to the symptoms like they were problems and when we got into SCUBA, it was life-changing for me. It is a janky-ass piece of shit software that clearly was not designed to help them track databases. And it's amazing, ask any Facebook engineer and they missed that. It changed my life. I was a better engineer with it. That's why we built honeycomb and honestly like my grand plan with building honeycomb with, so I'm leaving Facebook, someone is giving me p millions of dollars, never mind, I've built this thing when we fail and open source it, I'll never have to worry about it. But we just keep not failing, so maybe someday.
    
    [laughter]
    
    But like, you become a better engineer when your window goes from hours and days to like seconds and minutes repeatedly and what I have in common -- well, we have a lot of things in common but there's a lot of known unknown things. There's a lot of this will happen one in a million times, almost never. Well, except if you're at Facebook it happens once every few minutes, right? And then it happens 1 out of hundreds of times, and tracking down those four or five rare things that happen: Nontrivial!  And these are the problems that you all have to look forward to you. You know? Like the categories of problems that Google has had and Facebook has had for a long time are the problems that more and more people are having and some of it is self-inflicted. Let me be clear. Containers, ahem!  And some of it is just thrashing around and trying to figure out how to deal with this freight train of how everything is coming everyone's way. We've been doing microservices since like 2007, Jesus Christ. Anyway, sorry, angry old man. Unknown unknowns are most of the problems. Like, you never even look at it because it works. Right? But you have this long infinitely long thin tail, things that almost never happen, except that once that they do.
    And usually you don't get good bug reports, you get symptoms that are described to you by users and about 40% of the time it's because they didn't turn on their wifi, which is extremely demoralizing to the engineers that you're asking to go track down this problem. The hardest problem is often just figuring out which component it lives in so that you can trace it or debug it or whatever, because again, everything's going like this at once in your graphs, right?
    And it's really about the health of the system is no longer as important. Honestly if your system is was 75% down and all of your users were having a good experience, would you care? I wouldn't. I would care if it was 99.9% up and I was getting a lot of complaints.
    Right? So I don't really feel like up or down is all that meaningful when it comes to system health. What is important is being able to find and identify every single request and slice and dice by every single characteristic that they might share and the reason I think it's important to distinguish monitoring from observability. I don't want to like lose them, right? But like the monitoring principle that says that you should not have to go look at your graphs every day to know if something is wrong. It should tell you. Super-agree with that. Do I want to get paged about 21,000 fucking things? No, I don't want to get paged about all of the things that users are complaining about, or else all of my engineers will quit and I will, too. And the rest of the talk we're going to get into each of these. Instrumentation I'm not going to bother with this, because I assume you're already sold on this one.
    I will just point out that basically what we all want is distributed S trace, right? We all want to be able to hop around from node to node, we've separated out into monoliths and microservices and this means that no component can actually debug the whole story for us and the Holy Grail that we're all tracing is something that it can. Distributed tracing is cool for this. Like they take a very depth-first approach. Honeycomb is a breadth first. Honeycomb is about what do I even trace, like, seriously what?
    And events, not metrics. This is super-controversial, burr I don't care. There are a couple of reasons why this is true. Cardinality is one. Raise your hand if you've ever gotten cardinality. No. I feel like there are entire generations of engineers who have internalized metrics being internalizations of data. It's not. Metrics can't be traced. They don't have context, like events do. So like you all know what a metric looks like. This is a slide I didn't really get finished. So a metric is it a dot of data, right? It's a number and in order, and then you get all these numbers and it's like how do I find the numbers? Well, you append tags and you can have some fixed number of tags. Hundreds? I don't know what Datadog does now. The issue is it is fixed. The right amplification is huge. You don't want to append a lot of text. Think of inbuild ID. Infinitely -- so a metric has all these tags, an event is just it's a log line without necessarily flushing out to disc, right? A metric is structured set of data. And I I understand why we got started with metrics is because it's hardware is obscenely expensive, right? Like it's Google back in 2000, well, I got this really big website, how can I tell how it's doing? Well, logs, killing my system with logs. How about metrics? There's literally no more data than you can have with a metric. Like, hardware is not that expensive anymore. And also you can sample. Sampling is amazing and there are some people who have this weird allergy to sampling, which understand if you're sampling your billing system, I do not recommend that you sample your billing system. A you don't want your engineers acting like it's not sampled, because then they're going to make bad decisions. B, you really want every request that enters your system to generate many events, lots and lots, like every service that it goes too, every database that it goes to, like, shit. At Facebook it was hundreds of events, like you know like you're not going to build an observability or analytics system that is hundreds of times that of your actual system, therefore, sampling and you can do it dynamically so you're not attaching equal weight to every thing, right? Every five LX, we kept all of them, right, and we let the rate limiter like, slice off the top of the mountain. Read queries, we kept some sample of them and delete we kept all of them because people very rarely deleted their apps and we wanted to know when it happened. Sampling is awesome.
    Cardinality, literally all of the fields that I most want to know information about, all of them tend to have very high cardinality. If you have 10 million users, this is how SCUBA changed our life on Day 1. It was the ability to break down, first by that application ID, and then every combination of everything else. Boom, like that's it. Like suddenly we went from just trawling between like logs and metrics and all this shit and going back and forth to just going, oh, I see, yeah, I see that parse is down, I see that every time you're hitting the login end point it's taking 61 takes because it's doing a 5X table scan and it looks like you are doing that query there. Life-changing. Even the ones that didn't turn on your wifi, you can be just like you're not hitting your edge. Because you can break it down by that. Shopping cart ID, raw queries, memes. It's cardinality, man, it will save you. Heh-heh-heh.
    We did have to write our own storage engine to do this, though. Nothing was out there could do it. I've spent my career telling people not to write -- it's a storage engine.
    High cardinality is not a nice to have, it is a must have and especially for platforms. Platforms are kind of the bellwether. It's not just the queries that your software engineers over there wrote, it's queries that God knows who, over on the other side of the planet wrote and you can't throw things by them ever, by definition, it's a platform. And part of this is just because it's what the market is demanding is that richer feature set with more user-defined controls and with user defined controls comes great chaos.
    And these tiny, these -- like, humans just inject chaos, you know that. I'm not even going to bother with that one. And structure your fucking data, it's almost 2018, why does anybody have any unstructured logs? Another thing about this is that -- no, never mind. That's in a later slide. We'll skip it. Events tell stories. Like, metrics are never going away and I love metrics, I do. I just -- I make big statements because it's what I do. But like Facebook still has a very large metric system, ODS. Amazon does doesn't, they managed to combine them with something even better, I think. Anyway, metrics are great, but they don't really tell stories and they don't come with the rich context, you know? Any event that you submit has all of the details and it has all of the context you can pack in there about who it was for and when. They're much more explanatory, they're much more powerful. This is how our brains work, our brains tell stories.
    Had structure your data, yes, efficiency that's boring, whatever. Um, what about my dashboards? Well, what about your dashboards?
    Heh? Every place that I've ever worked has gotten dashboards to find your dashboards with your dashboards. Ah!  Dashboards rot, it's awful. The reason I hate dashboards somewhat facetiously is every dashboard is an artifact of the last problem. OK, cool, so the next time something happens, you just start jumping to answers. You're just like answer, if this one? Is it that one? Every single one of them. You're not actually doing computer science, you're not really debugging, you're not answering questions or hypotheses about how the world works you're just like aha, I got it. I hate that. I hate that feeling of your eyes just like darting around like this, well, maybe it's this, maybe it's that, oh, I can't get into the dashboard unless I write. It's like eyeball exercises, but what it's not is debugging.
    And you get like this dashboard blindness when you've been looking at them for a while where if you did not remember to graph a thing, you could be jumping around on dashboards for two hours before you remember, oh, wait, did we never set graphs up for that, or huh, maybe that thing stopped collecting data, you know, let's go look at it and the way that you build up a richer mental model of your incredibly complex system when you're asking questions and iterating on them, dashboards are like, they're like, they just short circuit that entire process.
    I think that engineers, I don't think it makes for good engineering, I've seen engineers become better by doing exploratory, iterative interrogations of the systems.
    Dashboards must die once I am queen.
    [laughter]
    
    Also, you really want raw requests. You know? You're looking -- you're looking for that needle in a haystack u' looking to find every single needle in that haystack, every single needle should be findable. You should be able to tell if a report is accurate or not. Aggregation, once you have -- time series aggregates, like we all know how they work, right, you all have an interval. Maybe a second or whatever, everything that happens in that interval gets smushed. You just through it away. You never go back. You can never ask the same arbitrary, open-ended questions that you could with the raw data. You need more detail and even more context, not less. And by this I mean write-time aggregation, of course. Read-time aggregation is terrific, you can get the same benefits without the costs, but nobody seems to be doing that except us. This is not a sales pitch, this is a cry for help!
    Black swans are the norm and you can't hunt your needles if you can't find the outliers. I'm not just talking percentiles, I'm talking actual max and min. For example, I think the only reason that DBAs exist is that the tooling is so shitty. It's just a complicated piece of software, like, everybody knows how to do one thing, look for series in this change log. But then they go oh, shit, this query hasn't changed in two years and the reason why is because of what they're looking at is they're looking at the reads, because the reads can't yield. Never going to show up there. And what you're probably looking for is what's holding the lock? Just please tell me what is the lock time telling me? Every user has this problem where you know a bot starts running, like in GitHub a bot starts all these crazy things or an iOS app goes to the app store and rises to the top. All these things happen predictably and everybody is suffering. At parse this was the story of our lives and we get into SCUBA and we're like oh, 93.7% of the time is going to that user, boom, throttled. It's transformative. Ooh, it makes me feel so excited. Or like you know, you know what I'm talking about. Rich esent they're so cool. Databases are just, I firmly believe if you have that tool you can hold database accountable for their own fucking queries. It's nice not to care.
    You need to test in production. This is like the PS that I just kind of snuck in there. Like, think about the national -- the electrical grid, right? You're not going to bring up a staging copy of that.
    [laughter]
    
    Honestly most of the time that I've spent on staging and the time I've seen other people spend on staging has been completely wasted. It drifts, gets out of synch, the data isn't the same, that long thin tail of problems that represent most of your actual problems going forward, never going to turn up there and even if you do, I'm the proud author of some capture replay software, love it, it's limited. Even if you capture and replay yesterday's traffic, you cannot predict tomorrow's, you can't predict what app is going to get launched, you can't predict any of that stuff and the place that everyone is underinvesting, hugely, is in tooling for production. And in failing fast, in rolling back, in canaries, right? I was tweeting shit up about this a couple of weeks ago. And now that may seem like a big deal to you. It should seem like a big deal to you when you remember how many interns they have.
    If it works for -- yeah, the way they do it is they deploy it to the internal Facebook for everyone, the 1% canary, and 5%, 10%, whatever, and then they deploy it to the live grid and everything. It's no good and we all have limited engineering time. People are so scared of blocking production that they're not accepting the facilitator fact that we do test the production, we can't not. By the way I'm not saying don't test before production, these are not mutually exclusive, do that, please, but then also invest that engineering time into feature flags, into shipping things safely and making it your education so that every engineer knows how to get to a good state, making it so the multiple versions can live alongside each other, data migrations. So this kind of leads into my last point, which is that I really believe that like the future belongs to the generalist software engineers, you know? They're in the driver's seat. I feel like the first wave of devops, were like devops tend to write code and the second wave is all right, software engineers, time to mind your own shit, you know? And no one is saying that these special cases are going away. Ops is always going to be needed. It's just increasingly it's on the other side of an API, right? And software engineers, no one is saying you have to do all your own backups, what we are saying is you can't craft good scalable well designed services without understanding you how they work. That's not so controversial, now, is it? I feel like every role that is not a generalist software engineer is we're forced amplifiers, right? We're amazing at helping you run your own stuff. And there's a lot of understanding how the life cycle of your code after you hit ship. It's 2017, we wouldn't hire someone who couldn't write a line of code. We should not be hiring software engineers who will not accept responsibility for their services. And I get why so long it was so awful, it was like, oh, I don't want to torture myself like the ops people do. Like, I don't want to torture myself, either, but as a group we're not known as our work-life balance. Own it. I will. It doesn't have to be awful. It really doesn't have to be awful.
    Instrumentation is part of building software, and watching it run in production is your fucking job. If you don't watch it when it's normal, you won't know what abnormal looks like.
    And I think that -- like we're trying to build something that makes it easy for just to poke around, like, look. And more people need to accept this as insanity. It's insanity that so often you hit deploy and you never look at it unless it's wrong.
    Your code is not shipped until it's in the wild. And your code is not tested until you've watched it running with real data, talking to real services, handling real users, real traffic patterns.
    Gregory's last two slides are there. The way to think about observability is to think about distributed systems. The research shows that focusing on these problems for decades, we can learn things. Even though I kind of forget that the academia doesn't exist, it does.
    >> I believe that with these big complex distributable systems, the only way to run them is with like fewer paging alerts. Just latency errors, the big three and then maybe saturation. If you trust your ability to debug any problem in just a couple of clicks and if you built resiliency into all of it you should almost never be woken up in the middle of the night and this may seem like a pipe dream, in John Lennon's immortal words, or whatever, but it's not. I've lived through this repeatedly. It works. It's beautiful. So I just have like a summary of the old way, the new way. The way things are changing and that's it. Thank you.
    [applause]
