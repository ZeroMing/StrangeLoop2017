    
     "Inside Twitter's Realtime Delivery of Tweets for Enterprise Customers"
    by Lisa White
    
    ... good morning!   My name is Lisa White and I'm a software engineer at Twitter. And I work on our realtime streaming products.
    At Twitter, we process hundreds of millions of tweets per day. If you do the math out, that's thousands of tweets per second. And we process those in real time without ever missing a single tweet.
    So we built an architecture called PowerTrack, and PowerTrack processes this massive amount of data both quickly and reliably. PowerTrack allows our customers to create a customizable set of tweets so that they can get the tweets that are relevant to them. And the reason that we do this is that for most of our customers, handling the entire firehose of tweet is a lot to handle. There are a lot of people tweeting publicly at any moment and handling that in real time would be a lot. So we allow our customers the ability to filter based on the tweets that are important to them, so that they can get the percentage of tweets that they care about, so we do all of the heavy lifting so they can get maybe the 1% of firehose of tweets that are important to them. So there are a couple of different core use cases that you might have heard about. One would be some research institutions might be looking for data around public health so that they can respond accordingly. There might be different customers that are building different customer service tools. And we also have customers that are looking for signals around natural disasters so that they can respond accordingly.
    Owe oftentimes our customers will connect to our APIs will and then they'll deliver their data to their software so that they can do further analysis on the raw data that we deliver. So as a software engineer on the product I'm excited to share with you the architecture of what we built. So at a high level, the PowerTrack architecture has a couple of different things that we took into consideration with the design. The first is how we filter data at scale. So as I mentioned, we're processing thousands of tweets per second in real time, and so we have to figure out how to make this scale and how to be able to filter in real time.
    How we ensure data fidelity, so many of our customers are Enterprise customers and dropping tweets on the ground is not OK, so we have to make sure to give them all of the tweets. We have system redundancies. Because as you know, in production, it's pretty common or not pretty common, but it happens that things go wrong and you can't plan on things being perfect all the time, so we have redundancy in order to ensure that -- in the case that things go wrong, we have a backup plan.
    Because we're processing in real time, we have to figure out how to apply customer stream configuration changes, and then lastly, we have multiple data centers, so we have to figure out how to manage the state of our customer applications across these data centers.
    So at a high level, the input into the product PowerTrack that I work on, is the public firehose, so basically all public tweets that are happening in real time.
    And ultimately we deliver data to our customers. And the -- we have a couple layers of our application. The first of which is a filtering layer. So this layer allows customers to have customizable rule sets so that they can filter only on the tweets that are important to them, and then that passes the data along to the streaming layer, which then does some additional processing and also renders the data out to the customer, and so it gets written via HTTP streaming to the customer so they have a persistent connection to us, so they'll connect one time and then they'll stay connected to maybe days or maybe even a week until our next deploy, so they'll reconnect when we do a deploy or if there's some sort of network glitch but most of the time they'll stay connected so they get all the tweets as they happen in real time. So the customer can manage their rule set that dictates which tweets they get through a REST API and so this is our rules API and it allows them to be able to create rules and delete rules and then the thing gets applied to the filter in real time. So the customer doesn't have to reconnect in order to get the updated rule set, which is helpful for them, because then they don't have to worry about missing data during the time that they're disconnected, but it does -- it does provide a little bit of a challenge for us to figure out how to apply those configuration changes in real time.
    So the first time that I'd like to dive into is the filtering layer. And so as I mentioned, we're processing thousands of tweets per second, and that's just our base volume. We also have to be able to handle large events, like a big sporting event, like the Superbowl where a lot of people are tweeting a lot more than our base volume so we have to figure out how to make this scale. For those customers, milliseconds matter so we can't deliver the data extremely late. So I'm going to be talking about filtering and how we make that scale to all of our customers and keep up with the realtime volume.
    OK. So you might be wondering what I mean by a customizable rule set, so this is a really simple example of a customer rule. So we have a filtering language that our customers use, and certain operators that they're allowed to use. So this rule would filter tweets that are from LMW, which is my Twitter handle and it heart has mentions or it is a retweet. so and then there's an implicit and from the Felemdoubleu and the retweet. Tas tweets some through the firehose, tweets that match this rule would result in the data that they get. So this is a quick example of two tweets that would match this rule. So the one on the left, you can see that is from me, it's pa tweet that I tweeted and it also has a mention. It mentions another user who is chiuki, so this would match that rule. The one ot right would also match. So I retweeted this tweet from Twitter bolder that's on the right and because it's both from me and is a retweet that, retweet of the original tweet would go through any stream with this rule, too. So we did this programmatically in real time and this is an example of the syntax tree that we would programmatically walk through the rule. So this one is pretty simple so we'd start with the and then we would check whether a tweet is from elemdoubleu and check whether it has mentions or is a retweet and you might unless there are some optimizations that can be made here, forks for example, if the tweet is not from elemdoubleu, then the whole rule is going to be false. Similarly in the or, if we determine that has mentions is true, we can just call the whole or true and we don't have to keep traversing through the tree so this is called boolean short circuiting and it's helpful but as I mentioned it's not that helpful. As I mentioned we're processing thousands of tweets per second and we also have customers that can have hundreds of thousands of rules, and each of those rules is most likely a lot more complex than this. It can have up to 2048 characters and we don't have a limit to the number of clauses that can be in there, so these rules can be pretty complex in traversing through the entire tree of every single one of our customers for every single rule that comes through. So we have a mechanism that would be able to speed this up and this is where predicate indexing comes in. So the idea behind predicate indexing is that we are trying to quickly disqualify rules from being matches, and this limits the number of full evaluations which are the tree walks we just looked at that we have to do. So the way that we do this is every customer rule is indexed based on the match type of the clauses it contains, so as an example, if someone were looking for the term Strangeloop, then we would index strange and then we could also index loop and so then those two terms would get added into the index and then any tweet that matches those then move on to be reduced from further. At its core, what predicate indexing is trying to do is it's trying to weed out rules that wouldn't match anyway. So essentially we're trying to narrow down our rule set so that we don't have to do a full evaluation on every single rule.
    OK, so to put this in other words, the rule evaluation is broken down into a two-step process so when a tweet comes through the firehose, the first step is to check the in.and check whether the tweet matches any of the indices, and then if it does, then that narrows it down to a smaller set of rules for which we've put through full evaluation.
    So as an example, let's say that a customer has 100,000 rules, then we add each of these rules to the index, and this might be -- this is most likely more than 100,000 because we have to have every rule in the index, but most rule is going to have more than one thing indexed, because we have to make sure that logically if the rule is going to evaluate to true, the index also evals to true, so we'll have more terms in the index and then when a tweet comes through, we'll evaluate the tweet against the index and that will give us a smaller set of rules for which we have to do a full evaluation.
    So you might be thinking, well, this seems to be overcomplicating it and you're adding in an extra step but because checking the index is so much faster than doing the full evaluation, this actually saves a ton of time, and saves a lot of CPU usage and speeds things up and we definitely see that in code when we pick a poor index and we end up doing a lot more full evaluations than we would see system issues.
    So the index is way faster than the rule evaluation as I mentioned, so then we can cut down on the time we're spending in CPU.
    OK. So now you may be wondering what we actually index and how we choose what we index. Well, essentially we're trying to pick whatever is least likely to occur, and the reason that we're trying to choose whatever is least likely to occur is because we're trying to narrow down the number of full evaluations that we have to do. So if we -- so if we were to pick something that's highly likely, then we wouldn't be narrowing anything down and we would end up doing a lot more full evaluations. So in this case, we have a couple of options or what we could index on. So we could index -- for an and you only have to index one side and that is because either one of those being false results in the and being falls, so you only need one of them to guarantee that the index being true results in it being a potential match for the full evaluation. So in this case we have a choice from indexing from elemdoubleu or clause. But for the or clause you have to index any one of those in there, so for it to actually follow logically if the index is true or if the rule is true, the index is also true, then you need to index everything that's in the or.
    So in this case, we can -- we could index has mentions and is retweet. The problem with indexing those two terms is that then when we evaluate a tweet, any tweet that either has a mentions or is a retweet, would match the index, which is a lot of tweets that either have mentions or are retweets, so we would end up doing a lot of full evaluations. Alternatively, we can index from elemdoubleu and although I tweet a decent amount, I don't tweet nearly as much as has mentions and is retweet, so by indexing on from elemdoubleu only, we would be able to cut down the amount of full evaluations we have to do by a huge amount because we'd only have to do full evaluations for the tweets that match from elemdoubleu.
    So in essence, doing this predicate indexing allows us to cut down on the number of full evaluationles that we have to do, which are the full tree walks, and that aallows us to save a lot of time. So this shows you if you've been following along: So this helps a lot and it saves a lot CPU, however, it's not quite enough. We are still trying to handle all of this data in real time. So we had to figure out other ways to scale our filtering, as well. So the next piece that I'm going to talk about is more related to the architecture. So at a high level again this is our architecture. The input is the public firehose of tweets.
    That is then moved into a filtering layer, where the filtering layer filters for our customers and then the streaming layer renders that and outputs that veria HTT streaming to the customer. So each of the filter apps or the filtering layer is built out into different stacks, where each stack filters for a set of different customers, so this in example you can see that stack one is filtering for customers 1, 2, 3, stock 2 is filtering for 4 and 5 and etc. So we can simply scale the stacks with our customer base.
    And we've used this in the past when we've just grown in our number of customers and had to add more stacks, we were able to do that pretty easily.
    So in addition to that, each of these stacks is processing a full firehose, because they're filtering for a different customer set and each one wants their full set of tweets that matches. But within each stack, we have a set of apps, as well. So in stack 1 in this example, let's say we have three apps, so each of those nodes would be reading in one-third of the firehose of tweets and then each of those would all be filtering for all of the customers, 1, 2, and 3, and then writing the data down to the streaming layer.
    So by sharing the load, we're allowing ourselves to be able to handle big spikes in tweets and things like that, by sharing it across three nodes.
    And the way that we handle this technically is by using a messaging queue. In Twitter we use a Kafka-like messaging queue. Has anyone used Kafka before? So it's a decent amount of people here. So essentially each of these stacks would have the same subscriber ID as you'd call it in Kafka, and by using the same identifier, each of the nodes within each stack would be able to share the full copy of the firehose.
    And so stacks 1, 2, and 3 would independently keep track of where they're at in the queue, so that each one gets its own copy. Whereas the nodes within stack 1 all share the same identifier, so that they can keep their place together and all share the same firehose effectively.
    And so one thing that this allows us to do is as tweet volume changes, we can scale our stacks up and down. Similarly, as we improve filtering in different ways and see different kinds of improvements with our filtering algorithm, then we might need less nodes per stack, so we can have that ability to be able to adjust it.
    And so we have a dashboard that we use to be able to do the stack management, and to be able to control the number of nodes per stack and also how many stacks we have and which customers are assigned, etc.
    So in addition to filtering being very important, data fidelity is also really important to our customers. So as I mentioned earlier, many of our customers are Enterprise customers and data fidelity is extremely important to them so making sure that we deliver all of the data that matches their rule set.
    So the way that we do this again is along the same lines of using a messaging queue, and in Twitter, that's Evempus, so as I mentioned between the firehose layer and the filtering layer, we have a messaging queue, so the other advantage is it allows data to back up. So if there's a big load spike, then the data would back up into the messaging queue and the filtering layer would be able to keep processing and then it would catch up as the load starts to level out and so it is possible that we might get a little bit behind in the filtering layer, but it's more important that we deliver all of the data than it be like immediate all the time, although we do do consistent load tests to ensure that we're able to keep up with certain levels of volume, but in the case of extreme events, sometimes it is possible that we might get a little bit behind.
    >> So we also have a messaging queue between the filtering layer and the streaming layer and you'll notice here that for each of the three customers you see in the diagram connected, each one of those has their own messaging queue and so what this allows you to do is the filtering layer can publish down to the messaging queue for each of the customers, but the different customers won't affect each other.
    So if customer -- if the customer in the middle was backing up, it wouldn't affect customer 3 being able to get their data on time. And it also -- the other thing that this allows is if there were to be a network blip or we were to do a deploy, they can request backfill. And so what this means is we can take the point arrow we can go back a minute or five minutes or whatever it is that they specify and just change the pointer in the queue so that they can catch up and read the data that they missed rile they were disconnected and so using this technology and having a queue percuss mer we're getting them out of the box functionality that provides the customer a lot of safeguards around making sure that they have data fidelity.
    OK, so we also use system redundancy to deal with failures. So obviously it would be great if things always went perfect in production, but sometimes things go wrong. So we have redundancy in order to deal with those failures.
    OK, so I'm going to talk a little bit about the filtering layer. So as we talked about, the filtering layer is broken up into three different stacks, which allows us to separate out the customers from each other, and also to be able to scale as our customers scale. Each of those stacks has multiple applications, which allows us to be able to increase number of tweets or change things based on tweet volume and then on top of that, we have all of this basically replicated. So we have side A and side B, where you can see that for customers 1 through 8, every single one of those customers is both in side A and side B so we're filtering every single customer twice. And for us we chose this approach of having two different sides, because operationally it makes it really easy to reason about.
    So we know that we can lose a single node on Side A and none of the customers will be impacted because Side B is not impacted. And this is really helpful for things like deploys, because we can deploy Side A as long as Side B is healthy because it does take a bit of time for our applications to come back up after they go down and the reason is we have to do all the loading and preprocessing in order to optimize the rule evaluation. So for us filling each customer only once could be problematic if a node could go down because then they could get delayed data, but by having two sides, we make sure that the customer is being filtered for in real time at least once and then if they had a problem, then the customer would not be impacted so our goal here is to limit any kind of customer impact and in addition if we had any kind of operational issue, let's say on side A and it's 2 a.m. in the middle of the night, we don't have to worry about putting in a change really quickly and messing with things and making a bigger problem in production, because we know that we can kind of take a deep breath and just kind of move on with like a thought-out way of approaching it because the customer impact is not there. They're not being impacted because Side B is healthy. So it also helps with peace of mind in being able to make sure we're taking the right approach.
    OK, so configuration application, this is how we apply customer configuration in real time. So as we talked about earlier, in addition to the flow of filtering customer data, they also have a REST API where they're able to update their rules in real time. And this is essentially a different product but it's used for PowerTrack in order to be able to update their rules.
    And the way that we handle this on the back end is we have two different mySQL tables, one for rules and one for rule updates and just to walk through a simple example of how this would work, let's say they added a rule that was from elemdoubleu and then they decided my tweets were actually pretty boring, so they deleted it. So you can see that the update table is just keeping a log of what's being updated while a rules table is actually kind of the -- it's the system of record where we have what the state of their rules is at that time. Then they might decide that they're interested in learning about hiking, and then that they want to get tweets from Jack.
    So the advantage of us having the data stored in these two different tables is that each filtering application can then subscribe to the updates table, so that once it has the rule set loaded it, only has to get updates. And this is helpful because if a customer has 250,000 rules, then if they make a change, we don't want to reload all of the rules from the rule table. All we want to do is see what changed.
    So by the customer subscribing to the updates table, they're able to get changes in real time and then they can simply just add that rule or remove it without having to reload all the rules. And as I mentioned previously, reloading the rules is a pretty big task because we have to do all that reindexing of the rules and the optimizations, so reloading the entire rule set is not a super-simple task and it's much easier to simply apply the updates. But we still keep the rules table completely up to date, because that way when a filtering application is restarted, it can simply just reload all of the rules from the existing state, also for the customers to be able to manage their rules and be able to get all their rules to see what they are, so that they can make changes to them, make sure that they align with what they have in their system and things like that.
    So the rules table is the system of record and the updates table allows the filter applications to be able to apply those configuration updates in real time and then the customer could use this REST API where they're creating and deleting rules, where the customer doesn't even have to disconnect so they're able to continue getting data while all this is happening in the background.
    So the way that we handle this behind the scenes is that the filtering applications pull for these updates on a certain interval. So we've explored ways of having the rule updates to be pushed to the filter apps, but in the past, this hasn't been quite as reliable, and we've ended up implementing pulling anyway, so we kind of start with pulling just so that it can make sure that the data matches what it's supposed to match, but we've talked about in the future, having the updates get pushed to the filtering application, so it can get applied a little more quickly.
    OK, so in addition to applying rule update configurations in realtime, we also can apply stream configurations in realtime. So we have a dashboard where we can basically see the configuration of the various sides and stacks, and it also allows us to look at monitoring statistics, like what the lag looks like when looking at the subscribing to event bus, whether it's behind for a certain stack, whether it's -- how long things are taking to process, so how much time it's spending on the rule evaluation per stack to see if there's a certain problematic customer. And then based on what we see in that dashboard, and you know, based on certain observe ability or alerts or things like that, we can use this dashboard to diagnose and mitigate any kind of customer problems.
    So one thing that you might have noticed looking at this diagram earlier is that customers 1-8 are both in side A and side B, but the configuration of customers 1 through 8 is completely different. So stack 1 on side A is completely different than stack 1 on side B and this is intentional and basically to reduce our customers being able to impact each other. So each of these customers were filtering on rules that they uploaded and so we try to put all the mechanisms we can in place in order to make them not affect each other, but sometimes there might be things that we miss. So if customer 1 were able to upload a set of rules that ended up to be problematic or let's say some tweets started coming through the firehose that all of a sudden matched a ton of their rules, just because of some special event that was going on, then it is possible that stack 1 could get a little bit behind in trying to process that data.
    So on side A, that would impact customers 2 and 3. But on side B, customers 2 and 3 are not on the same stack as customer 1. So customers 1 and 2 would still get their data in real time and not be behind because they're getting that data from side B and so then we would be able to go into our dashboard once we realized that customer 1 is problematic and decide to take any kind of mitigation. So if we -- if we have the option of moving a customer to their own stack, and this is essentially called quarantining them. We have quarantine stacks where they end up on their own infrastructure, so if they have complex rules based on a certain event that's happening, having their own infrastructure gives them a chance to catch up while we figure out if we need to reach out to this customer whether they are rules need to be changed if it's a problem with their rule or if we need to make a code change to optimize for this case that was missed.
    And then it would also make the other customers on stack 1 on side A, which would be 2 and 3, no longer be impacted and so essentially these stacks would pick up in real time which streams are assigned to them and then we could make those changes in the dashboard accordingly.
    And this is -- so this is -- this is helpful for us, so the filter apps in the same way that they look for rule updates, they also pull an outside application that's kind of managing the state of where each of the -- what each of these workers should be working on, and then they get back the list of customers that they should be responsible for. So it is possible that we might move things around, we might decide to at additional nodes to stacks if things start increasing and that's also something that gets applied in real time and is kind of unknown to the filter apps they're simply just working. So for example in side A, stack 1, you can see, we have three filtering applications there. And if we were to decide to add another one, let's say tweet volume increased and we needed more capacity to be able to handle all of the volume, then we could add another one there, so then that one would have four filtering applications and so instead of each one getting a third of the firehose, each one would get a fourth of the firehose and that's something that we would just get kind of out of the box, just because we're using the messaging queue, so each one of those is using the identifier for stack 1 and so just by adding another node, that would just all of a sudden start getting its portion of the data.
    In the same way, if a node were to go down, then the ones remaining would kind of pick up the slack and it would not be affecting the customers getting their data on time.
    OK. So state management refers to how we manage the state of our customers across our different data centers.
    So back to the system redundancy, we talked about how we have two different sides and how that allows redundancy, so if something goes down on one side it's not affecting the other.
    So you can duplicate that on the other end because we have momentiment data centers. It is a little bit tricky in terms of managing the state across the different data centers, so one particular example of where this becomes tricky is customer connections. So when a customer connects with us, they connect with a certain identifier for their stream, and they will have purchased a certain amount of connections, so let's say that a customer purchased three connections. That means that they get three redundant connections and that could be because they are bringing the data into their development environment or it could be because they want redundancy into their system, just for extra backup to make sure that no tweets are missed if they were to get disconnected or things like that.
    And the customer has no knowledge of which data center they're connected to, so that's all done by routers on our side, so the customer connects, and then they could be connected to two different data centers, so we have to be able to communicate across those two data centers in order to basically make sure that we're not cutting them off of connections they should be having, but that we're also not giving them a crazy amount of connections that would hurt our system. So the way that we chose to do this is by using Zookeeper and the reason that we chose Zookeeper is because when a customer connects, that then requests a lease from Zookeeper for that stream ID, and then Zookeeper just returns back whether a lease is available or not, and so this took a lot of the overhead of writing the code away from us, and one of the advantages of this was that the leases are ephemeral, so unlike if we were to put a record into mySQL or something like that, if a node were to just go down, then the lease would disappear from Zookeeper and we'd no longer be charging the customer for that connection, whereas if we were to insert a row into a database then we'd have to put in some kind of a cleanup process and make sure that the customer is still connected. So using Zookeeper and just because by its nature the leases are ephemeral that kind of takes a lot of that extra work of figuring out how to make the node's heartbeat and things like that away and gets rid of a lot of error and we were able to use that out of the box technology. So in addition we're using a cluster that's shared across data centers, so that provides the advantage of it working across data center.
    OK, so for the PowerTrack architecture, what we talked about today was how we filter at scale, so how we basically use predicate indexing and how we have multiple applications per stack to be able to filter data at scale, how we're filtering thousands of tweets per second for our customers' hundreds of thousands of rules. How we insure data fidelity, so basically how we use event bus, our messaging queue to be able to back data up to into the messaging queue and ensure that data does not get lost along the way to our customers, how we have redundancy of the two different sides to insure that we're OK in the case of any kind of failures.
    How we apply configuration updates in real time, such as real updates or where a stream is being filtering from. And then lastly, how we manage the state of our customers across different data centers.
    So that's the end of my talk, and so thanks for listening, and I'm happy to answer any questions if people have them.
    [applause]
    .... Yeah?
     AUDIENCE:  So I think I heard your question was if the two different sides write to the same stream or different streams? Is that correct?
    >> So the two sides write to the same stream, and what that allows is the streaming layer essentially can then read directly from that stream. So you might be wondering, well, doesn't that end up with duplicate data for the customer? One thing that I didn't mention, so thanks for pointing this out, is that one of the things that the streaming layer does is dedupes, so it kind of keeps a rolling window of tweets that it's looked at in the last period of time and then does deduplication based off of the tweet ID. So that we make sure that they don't get duplicates, but it makes it easier from an operational perspective that it's only reading from one stream rather than have to coordinate multiple. Yeah?
    >>
     AUDIENCE:  I have a question with respect to -- does the system ... does the system guarantee that the order of the tweets is maintained?
    >> Sure, so the question was, do we have any guarantees around fidelity or order. And so around fidelity, we -- in the case that we were to miss any tweets, that would be considered like a system outage that we would work with support to post and communicate to customers. So I'm not sure about like formal SLAs, but we do consider it a production issue if there were to be a case where data were to be missing and that's why we have so many things in place like the redundancy and multiple data centers and we have the messages in the queue in order to ensure Fidelility. Another piece is that the filtering application doesn't acknowledge that it read and processed the event until it processes out to the rendering layer, so it makes sure that the data has actually been written out and the same thing with the rendering layer, it doesn't acknowledge that it read the data until it writes that out to the customer stream, so this makes sure that we don't lose data if a node were to go down, even, because then another node would pick it up, because it's never actually been acknowledged. And then the next part of your question, can you remind me what that was?
    >>
     AUDIENCE:  Does the order of the --
    >> Yeah, so the order is not guaranteed. So it's basically producing the tweets in real time as quickly as we can get them out. Yeah, one more question?
    >>
     AUDIENCE:  I'm actually curious about how you handle production issues. So let's say there was a bug or something happens, how do you, because it sounds like there are so many steps along the way, like maybe if you're looking at reconfiguration or trying to figure out what tweets -- I mean it seems like a lot of steps to go into even resolving a production or taking the steps to resolve whether something occurred. It's really difficult because you almost have to rewind back in time across potential -- I don't know, I'm just curious like let's say someone missed all the tweets from Donald Trump one day, which would really suck, what would they do to make sure that all the tweets were actually brought back to them.
    >> OK, so the question was, how do we deal with production issues, it seems like, because the system is pretty complicated that it would be difficult to kind of nail down where the issue was, and I think because we have it broken down into multiple different application, it's pretty easy for us to see where we've seen issues, so you know, we'll have error counters and have that in your monitoring dashboard, so we'll know if we ever weren't able to publish an event, and so I mean for the most part, we get notified of issues in our system before a customer would even notice or a lot of the time the customer doesn't even ever get affected because it only happens on one side and not the other side and so it's not actually even customer impacting, so for the most part, based on what the issue is, we're able to kind of narrow it down just because we have things broken down into discrete applications that we monitor separately and all have different jobs that they're responsible for. We also have a QE team that is constantly streaming data and they're making sure that we're not missing data from the firehose so this is really helpful for us so we can make sure this is running in our staging environment happily before we ever push that to production, so that definitely helps with just kind of peace of mind and making sure that things are fine and we're not pushing those kinds of bugs to production that would result in data loss. So I think that we're out of time, but I'm happy to stick around and I'll be around for the rest of the conference, so if anyone has any additional questions, I'm happy to talk about it. So thank you.
    [applause]
