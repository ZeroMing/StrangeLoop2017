    "Scaling Validation and Quality of Streaming Data Products at Twitter"
    by Kelly Kaoudis
     Hello, so I'm Kelly, and I'm a software engineer at Twitter. Today I'll be talking about what I learned  ... (too fast to transcribe) 
    Eventually Twitter saw our value as a source and bought us, but some of the things unique to the culture and operational mindset lived on, especially in how we think about and test our systems. We don't just rely on engineers running tests and keeping our structure but we also have a team of testing an ops folks. Q exists in its current form today because we're one giant distributed system bridging Twitter and our data customers, I like knowing how things work but also how they came to be the way they are. Testing end to end started when GNIP we just had a couple of apps then. As we grew from GNP into offering a wider variety of products, we offered services when we needed they will and soon no one person in the organization had everything in their head. We're not Netflix, but it still happens. We didn't have the consistency between products that we wanted and in some cases we couldn't figure out why our system produced outputs. Our quality just wasn't where we wanted it to be. Today I'm going to give you a motivating example of one of our products that QE tests, talk about our approach to testing the whole system holistically, cover some of the challenges we faced in trying to come up with testing strategies that are tailored appropriately for the work that data products does and the systems we have, and I'll cover some of the tools we've developed in the process of trying to write and enable feature chains despite they challenges. To start with I'll define what QE actually does. So it writes tests to determine how our systems actually work and end, then we convert them into guardrails that confirm system uses to keep working. In order not to disrupt the experience in production, we have our feature teams deploy everything to an environment with just fake clients that we control and then we run a suite of operational tests before code goes to production and that's our staging environment. We also have a separate experimentation environment, and that's not our staging environment. This is basically like a mirror of production. Anything deployed to staging is done so with the expectation that it will end up in product unless there's something awful. We handle real Twitter data in real staging environment just like we do in prod, we have a set of operating constraints that are relatively unique for part of Twitter. Our clients just kind of sit there and consume data, they're fairly passive. They're people making financial decisions and writing academic papers, so we just want to make sure they get all the tweets and all the tweets have the right parts. We also run some of our fake clients against production data products so we can compare what we're seeing in those fake clients in staging and production during the period before it deploy, but we have the staging environment in particular just to rattle its system from end to see and see what falls out. The quality engineering team handles the deploy. Because of the soak period and the comparisons we do, rattling the system end to end and putting things into production are pretty tightly coupled for us. We use the data for our fake clients to determine the steady state of the system. So does anybody remember the DDoS outage from never of last year? It was fairly big for us and it took a pretty big chunk out of our expected down time for the whole year. We operate in a mainly streaming micro-service-based environment. The QE team is in general most interested in trying to find causes for your classic distributed systems like lag or I consistency of output of the whole system and looking for results between interactions between parts of the system that are supposed to be cooperating. So the Dyn outage we definitely couldn't predict but there's potentially going to be something wrong so we have this staging mirroring product set up with a soak period because it allows you to see what will happen in prod before interaction bugs in particular get there, so tiny cuts, if you have enough of in many still compound as a pretty big injury. Our customers so the kind of testing we do for data products is at the application layer and primarily from a customer's point of view.
    End to end testing takes place in both a gray area between services and outside the system boundary in our fake clients, so it makes sense it isn't usually the first thing in a lot of people's minds. Future teams tend to rely on it in the background unless we find an issue. But the number of bugs we have have made it worthwhile. End to end testing is not a replacement for your garden variety feature testing. In feature testing your engineer will muck out the downstream tendencies just to route out the code they've written. But this is live in end to end system testing on the other hand nothing is mocked, all dependencies are up and at 'em, you know, databases streaming connections, the works. While bugs caught by other forms of testing could definitely have customer impact also, our specific target with this kind of testing is false interactions between disparate parts of the distem and fidelity between different parts of the same data. Finally Twitter has between 1500 and 2,000 engineers, so 20 sore so of them can work on CI and work on our. QE is not that. Our work is tailored just for data products. The Twitter observability team is produced through efforts of the QE team. An example of a product we validate on a continuous basis is PowerTrack. I'd like to briefly describe PowerTrack so you can more easily visualize what I'm talking about. The firehose is what we call the stream of all realtime Twitter data. There's so much of it per second that that consume the whole thing as a customer, you have to make 20 nonredundant connections to us to get all the tweets. PowerTrack filters out the tweets you might not be interested and serves you with just the good stuff. When we're getting ready to launch our most recent version of PowerTrack, we had two problems to solve: First we needed to be faithful to our other sources of Twitter data, especially the old version of PowerTrack and the firehose. We needed to make sure the new version wasn't lagging compared to the old version, especially if we were going to ask customers to switch soon and if we saw a tweet in the old version, we also saw it in the new version with all same components. With this update to PowerTrack, we wanted ...: So this isn't like, you know, your everyday push to production, this was kind of a big change for us. Now, planning for an average level of tweets per second isn't that hard, especially when we have decent steady state metrics, but planning for meeting our high water mark on the other hand can be a little bit more difficult. So that's our second problem. Most systems, should be able to handle a certain number of tweets per second comfortably as input without any human intervention and about double that number should be handled without paging even if our observability setup starts e-mailing us and says something is ugly. We'd rather be proactive than reactive, especially when it comes to making our customers happy, so what could we do to meet those needs and certify a PowerTrack release? So a book that's been sitting on my shelf for a while and kind of getting dusty but I've picked up is Polya's how to solve it. Taking the time to think critically and analytically about your problems is pretty generally applicable. His main idea is it is just easier to not try to tackle the whole problem head on. There are four steps to this approach, and I find sometimes the best thing to do is just to go back to first principles. I find it helpful to structure my thinking about how to go about finding solutions in this way. The first step is understanding. If an engineering manager comes to us and say, hey, I'd love to have a test for this and for this and for this, we need to ask some clarifying questions, we need to look at our existing metrics about the state of the system, we need to go talk to people who are working on the product and figure out what they think. If we can't restate the problem in our own words or we don't know what we're being asked to find or show it's really difficult to write tests. So we've gathered some information but regardless of how well we understand the problem, we should still check back with whoever gave it to us to make sure our expectations align with theirs. The second step, once we've got a problem, is to figure out an approach once we know what we're looking for. We could start with a list of potential approaches, and try the best one. We could reduce our problem to just the essentials or maybe there's some small chunk that needs to be solved first before the rest can be solved, like you know, if we're testing something maybe we need to get access to a database, maybe we, you know, whatever, just somethings small that you can pull off first. Thirdly we'll try that out and finally we'll figure out if it worked to our satisfaction. For the new version of PowerTrack, making the new rules API, which allows folks to add custom rules be more consistently and be more reliable was a subproblem of making all of meeting our SLAs, so the rules API is pretty simple, it allows you to upload rules in JSON format up to a size limit. When you've successfully added a rule, if a rule already is actively filtering a screen, you should get back a 200, a message he go telling you so. The fourth step in Polya's approach is to trying to figure out and fill out any holes in the exclusion. Maybe we need to take a step back and find another small piece of the problem to solve before we can get to what we originally thought would be the first thing and this is when we decided that. So one of the things that we were working on with this PowerTrack update was the ability to focus on unicode instead of words and phrases in the various languages that Twitter supports, plus hashtags. We also wanted customers to be able to focus on emoji points in particular. One of these, the soccer ball emoji was not acting as expected. We had system tests that added a couple simple rules, and checked to see that what we got out of the system made sensings but we wanted everything to at least work for the data release, comparing data from PowerTrack streams with the same simple rules was a limited version of the problem of making any PowerTrack stream consistent with any other PowerTrack stream with the same filtering rules.
    We held to the teams working on the PowerTrack rate tests that compared soccer ball emoji tweets from various data centers to make sure we were seeing the right tweets when we expected them. Coming out much this deep dive in the PowerTrack product and the systems that make it up I started thinking about the way that I was approaching testing data products as a system and how it had been really helpful to just concentrate on aspects of PowerTrack for a while. As at this point I was fairly new to Twitter. I thought that the best thing I could try to do on my own, aside from whatever the day to day was, was to learn as much of our entire architecture in detail, which right now I think is maybe 60, 65 microservices, as possible, so QE started experimenting with embedding with feature teams more often. We wanted to be visible every day and on people's minds so we could get a better sense of what issues they had that we would take off their plates. Another benefit that we hoped for was the ability to pinpoint problems in areas that made more sense, so we weren't just beating on services that were already sturdy that were kind of the obvious places to look and ignoring things that might have been more weak. So these are all good things but we didn't needs to know absolutely everything about the system in order to just support it. Exploring and experimenting and gathering information are all important for building intuition about the system, both in terms of traditional QA and in terms of red teaming but it was impossible for one person to have deep knowledge of the architecture at the same time. You lose context if you aren't in immediate contact with a service or subsystem for a bit. The things that we were able to write without enough context, we were also leaving a piece of the system out of our model of data products and that was the other engineers, so we had a pretty good set of experiments going, but trying to be end to end testing as a service was bottlenecking the other engineers at getting features into production. QE writing and maintaining all the system tests and system testing infrastructure controlled well for feature teams' possible blind spots with regard to their own code and things that they might think were obvious, but needing to understand enough of our system in order to file tickets for bug reports that would be accepted and acted upon was a drain on the amount of time we had available to write the right tests. Sometimes with our limited system knowledge we would think we found a bug, but it was really expected behavior of the system that just wasn't documented or that kind of customer input would never be presented at the API because there were five other systems between it and the edge. Sometimes we didn't have enough facts in our quiver about the expected behavior of the system and all its individual components to talk to the right person or team first. Our relationship with feature teams needed to be more collaborative. The way we were approaching system testing simply wasn't optimal for the organization itself. Now, Conway's law is that the systems that are created by any organization are reflective of how the people in that organization communicate with each other. A system has to be considered in the context of the people who work in it every day and that's what this guy is saying, too, it's really if you're going to think about testing something, you have to think about the people who wrote it, as well and if haven't looked at this paper, totally do, it's all about process and formal verification, and some of the examples are like refrigerators, but it's really cool, so if you liked the talk before mine and you like this, you'll probably like that. So when another team made time for us, like with their work with PowerTrack before our latest big update was to be released we had success in drilling down into that team's portion of the system and its dependencies and helping them with their interaction testing but we were still having difficulty keeping up across the org so it was time to reconsider again, so Sarah mei is kind of the goddess of feature testing. Her blog posts is something that I like to spend time with from time to time. So he I'm going to paraphrase a bit: Good tests should tell people how to use a system and what they can expect from it even in faulty states. If we just and it's difficult to prevent future editions from moving away from the functionality that our customers are paying for without tests proving the entire system works and test proving the boundaries of the specification. Good tests do not confirm information that we already know. Falling into the confirmation trap again doesn't help us challenge the beliefs of our organization if something is wrong with the system to motivate change. A good end to end test should do two things: Fail with the existing state of the system if it's behaving badly and then once the system is fixed, continue to ask as a should always pass guardrail until we reevaluate that test and decide if it's needed or not. Otherwise, we don't really need to add that test to suite.
    We were kind of in a holding pattern trying to keep up with everything everyone else was doing, writing all the system tests and doing manual testing, a hack week sort of snuck up on us. Part of our problem our tools for the tests we were trying to write weren't the greatest. They really written for anyone to use, so here was a nice subproblem, really, taking some of the load of end to end testing off of everybody by iterating more complex tools into something simpler and easier to apply that other people might be able to use. A couple of my teammates worked on writing a new cleaner version of some of our testing tools that hack week, so the idea was, with easier to use tools we could get feed back an whether we were actually finding bugs sooner and we could even take the time to teach teams that already had domain expertise what to look for in test results when their systems weren't interacting correctly with one another. If we could teach feature teams what to look for, we could write simple code then teach them to write systems tests themselves and fit those together no an end to end suite. Then we'd really only need to step in ourselves to provide advice to write and interpret tests in more complicated cases when we had time to dig in and explore. But we were running into another problem. Now our own expectations about the quality engineering team and where we fit as people in the org were different from those of everyone else. We eventually went over our managers, but now we had to harden and scale our new tools, gradually phase out the old tools and get the rest of data products on poor. We did a lot of peer programming writing tests using our library and explain why we thought it would be easier for everyone involved if we didn't try to duplicate the expertise and understanding of about 35 people with four. So part of the initial problem was -- with no product manager it's easy to fall into the habit of building tools just for ourselves. But that's not our job. I in particular wanted to see if engineers following our process and waiting for us to catch up with their testing were happy and there were some grumbles. People were fairly patient with us because they knew we had a big job but we needed to be smarter about our approach. When GNIP was much smaller doing everything ourselves and building things with just our team in mind worked pretty well, but with 40-odd engineers add things every day there is a lot more overhead. So when we deploy, what eventually goes to prod is ...: One direct benefit is they're at least somewhat more likely to pay attention when those tests fail. So issues have started to get fixed faster. A team whose service causes a test failure was generally not able to get their code into production until they've fixed that code or fixed the test that failed since we treat tests like a blocking deploy step. If folks want their new features prod, fixing has to happen first. However shall this is still not a perfect system.
    So it's time to take a step back again and look for gaps in our coverage: Why we have the right tests? Were we too focused on a couple of products to the exclusion of others? What were we missing? Taking it back to the early days again, when we were GNIP we had full control over other systems, it was easy enough to inject enough fake tweets to hit our high water mark and measure the results both on the intermediate system and on our fake clients. As part of Twitter, though, our apps are connected with our in-house pub system and we monitor with tools maintained by the observability team. We don't control our environment anyway and had been missing a way to load up the whole system at the same time for a while but we'd been hesitant to develop for a while. So up there on the left, I' got Ellen's tweet from the Oscars in 2014 and it was retweeted a bajillion times and favorited and whatever but that event isn't really like the other two at all. So we had two things on our calendar, New Year's Eve and the data products engineers, especially the streaming team, wanted to know if we could help them plan capacity for high-load events like New Year's, and so I paired up with somebody else to explore what tools we might already have for stress testing. So Twitter does have a backend tool but it runs in prod, which would mean serving our people we're serving as 24/7 fake tweets. Fake tweets served by that tool are stopped before they get to the UI but as data products we have different priorities. So data customers have somewhat different expectations of our API than people tweeting from the Twitter UIs for example, one pretty significant drawback of both our stress testing in the old GNIP environment and this other tool is they just play for data. They're unintentionally imposing further constraints on the model of the actual system and based on the test results can reinforce part of the system that might not need to bear the same kind of load. So in the middle of this hurricane and on the right is a football game. Recruiting a known stressful events such as the Oscars, where Ellen took this picture, might be a good start for generating a fake data set. Requests per second load testing can definitely inform our picture of the client experience, but it's less of a primary concern since we're streaming. What we were interested in is more along the lines of loading ourselves up with input in terms of tweets per second as a relief node of Twitter. So instead of taking the fake data approach, we decided to leverage what we already had, and in progress migration to our very own pub subcluster and the ability to subscribe to all tweets just as if we were the initial app at the top of data products streaming architecture. So we worked with the streaming team, as well as the pub sub team. We weren't using fake data, but we had direct access to all events occurring in real time, so we built a lightweight tool that allows anybody who wants to pick a topic to subscribe to, a topic to inject to, and a desired number of events per second to inject. We made two design decisions with our load tool. Like our original replayed load generation tool, the new tool has a number of workers that you can use based on how much you want your load to be multiplied, but two, unlike our old tool, each worker can be set to duplicate each event a set of random number of times. So in this way we can bring production load to our staging environment without getting trapped by patterns in a fake dataset so since we can randomize and pick the number of events that we want to see every single time, the size and the type of events are produced differently with each test.
    So you know, with just three things, really, you've got the topic that you're reading from, the topic that you're injecting to, and then what you want to see. And then they can run in our staging environment, as long as they've notified anybody who might be affected, and enough worker nodes are free.
    So this also ties back into our testing building blocks ideas. We run big load test on the entire system after all the system tests pass during our predeploy run, but individual engineers can impose their own tests for any other kind of system testing that they would like. Now we had something approaching a fairly tool chest with our architecture. So system measurements and requests from leadership and whatnot are great tools, but without soliciting feed back from people using our tools and fixing bugs based on the information we provide them there's no way to know if our method is working for anyone else. So we started on this track of reinventing our tools and just trying to leverage all possible resources we had, because it was taking us too long to test in particular the new PowerTrack version and we were so focused on keeping up that we weren't collaborating well with the rest of everybody and we weren't, you know, providing as good tests or helpful results as we possibly could have. If nobody's getting good information from the tests you're writing or what you're doing, if you're testing, even if that's your job description, you shouldn't be writing those tests. We were catching some bugs, but we weren't working at pace with the rest of the org, really and we weren't preventing all the faults we could have. So it's important to take time to consider your solution every so often, change up your approach if you need to and iterate to stamp out any edge cases you might not have considered. It's also important to consider what kind of organizational culture that we want to work in. Do we want to move as fast as possible or do we want to provide stability for our customers or do we want to balance these needs? We've shift today a less high-touch model of testing and deploy for QE as we've evolved as a team and the apps we test have grown and changed, but we still have to balance the needs and desires of engineers with the contractual obligations we have with people who pay for us to build things. Giving engineers the opportunity to write their own tests when they want to, given advance notice has started to allow us to put less effort into every system test written, but the agreement we have contractually with some of our customers, you know, you will get this many tweets at this time if they exist, should still be not ignored. Like we still have to deliver a certain level of quality and pay attention to detail. But maintaining a quality product shouldn't be an excuse for not improving human process as the system and the organization grow and change. We do want to make sure that any new approach we try will actually meet the needs of our customers, but we also want to make sure we're creating precedent for a place where people get to work on new things and see them running faster in production and where people can have an on-call experience where they're not getting paged repeatedly every night at 3 a.m., because no one likes that. If you do, talk to he me, please. Especially with testing it's important to experience what could work best for your own org as well as your org's current stage of growth.
    Not every type or style of testing or of deploy works for every situation. We've had a quality engineering team since we were GNIP because it's continued to work for who we are and the way our systems are, but this is just one way of doing stuff. There are many possible ways that you could catch distributed interaction problems and I haven't thought of them all, but they're out there, I'm sure. We just want to make sure that we're doing our best to give people a reason to continue to work with us, both as engineers and as customers. If the org changes again, system testing and data products at Twitter is an evolving process and I can't wait to see where it goes next.
    [applause]
