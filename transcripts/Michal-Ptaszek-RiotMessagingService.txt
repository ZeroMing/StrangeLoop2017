      "Riot Messaging Service - Message Players Like a Pro."
      By: Michal Ptaszek.
      >> Hello, hello.  Good morning.  Good morning, Strange Loop.  Thank you, all, for coming today.  It's Saturday.  It's before noon, so really appreciate the audience.
      My name is Michal Ptaszek, and I'm a software architect at Riot Games.  I'm here today to talk about Riot Messaging Service.
      So before we kick off, just for my observation, how many of you ever heard or played League of Legends?  Holy crap.  Okay.
      [Laughter]
      That's amazing.  Cool.  So Riot Games is a developer and publisher of League of Legends and more about the game for those who never heard of it in a second, but as a company, our main focus is really on the player experience.  We use it as a raiser for making any decisions, regardless if they are product or technology-oriented.
      It help us tremendously that we are all gamers here.  Like, we really play games and enjoy them, and we can empathize with other players.
      Our mission is that we aspire to be the most player-focused game company in the world.
      So what is League of Legends?  League of Legends, we launched a game in 2009 and since then, we've grown the player base to some pretty big numbers.  And more about the numbers in a second.
      League of Legends is a team-based game in a genre multiplayer online battle arena where two teams compete to control in a map objectives.  Each player of these teams controls one champion.  And typically teams consist of five players.
      The entire game is placed in a modern fantasy world.
      To give you some more context around the scale and types of problems we were trying to solve when architecting messaging service, here's some numbers we released in September 2016 regarding popularity of League of Legends.  So we are super lucky to have over 100 million players enjoying the game monthly.  And out of which over 27 million players play league daily.
      If you check our systems at any given time of a day, we can see at least 7.5 million players connected to our servers.  So all of these numbers heavily influence decisions we've made when building our amass.
      So now let's jump to quick overview of what we're going to cover today.  I would like to start with a very high level design of Riot Messaging Service, then I would like to follow up with the implementation.  How we got there and what were our technicality decisions behind R MS, as well as what were the key lessons we learned when created it and also what our plans for the future service are.
      Let's start.  So before we jump directly into design of RMS, let's kick off with describing why we even wanted to build RMS in the first place.  In the old way when we started building League, the old way of handling player to server communication for the out of game events, persistent TCP connections using protocol, that's the binary protocol introduced by Adobe.  Overall it was for media a great fit for game traffic, and it turned out that the protocol, really, was having really big issues with security.  The way we build the platform as a monolith prevented us from growing it fast deploying and transitioning into the microservice world with very fast iteration loops.
      We also suffered from pretty significant scalability issues.  A single instance, a single server holding player connections could only handle around 30,000 layers.  Given the scale we were operating at, building out the platform for, you know, hundreds of millions of players was a huge challenge.
      And as we were building more and more features for League and also other products, we realized we cannot replicate the same pattern, and we needed something better.
      And that's how we created RMS.  RMS or Riot Messaging Service is a back-end service.  Something we apply on the server side.  It provides asynchronous messages capabilities notifications that are sent using the fire and get semantics from back-end services to connected players.  So that's the way for the back-end microservices to somehow deliver either notifications or payloads to clients.  For example, League client.
      And today RMS has been adopted by over 20 different products and features at Riot and is used every day by all League players all around the world.  To give you an example of what features use RMS, its clubs, which are player organized social groups, its missions, which are challenges for the players with rewards, and honor, which is a system that encourages positive behavior among the community.
      To better illustrate how RMS works, let's use push -- mobile push notification analogy.
      In the beginning, let's imagine that we have somebody sending us an e-mail.  The e-mail hits our e-mail server, and e-mail server stores it in some sort of a database.  The interesting thing is that the e-mail server -- because everyone now uses smartphones -- figures out that.  Okay.  That recipient probably has a phone and would like to be notified that he has an incoming e-mail.
      In that case, the e-mail server sends a request to the push notification service, informing it that, hey, you've got an e-mail.
      But because your incoming e-mail might be pretty big, it might also have attachments, you don't necessarily want to receive them as push notification.  As a matter of fact, the push notification servers have very strict limits when it comes to size.  So what happens is that push notification servers actually receives only a very high level digest of what the e-mail is, which is the subject of the e-mail, sender, and maybe few first paragraphs of text.
      It sends a push notification to your phone, your phone buzzes and displays toast on your home screen.
      And then some time later, you're going to go into your e-mail app, open it up and download the original e-mail, including attachments, all the text, and all the context around the e-mail.  But instead of going to the push notification service, you're going to go to your original e-mail server.
      RMS works in a very similar way.  But instead of the mobile phones, we have desktop clients or League of Legends clients.  And instead of the push notification service, we have RMS.
      Let's see how it works with League of Legends example as clubs.
      As mentioned earlier, clubs are player-created organized social groups.  Each club can be described using several properties, for example, the club name, club roster, which is list of members with their ranks, message of the day, and so on.
      First of all, whenever you start League of Legends, you start and authenticate with our servers, you get a token claiming who you are, so you can prove with other servers that you are, in fact, you.
      As soon as authentication is done, RMS requires clients to establish a persistent TCP connection using WebSocket protocol as a transport in order to TV any incoming notifications.
      This is an example of WebSocket URI.  The aforementioned token we got from the authentication service is highlighted in red here.
      So we pass that to RMS, and RMS is essentially authenticating, validating the token and letting you in.  As one of the claims encoded inside of the token, there is a field called subject, which is -- which we interpret as player ID.
      Player ID is a standard way to identify players in our ecosystem, and is used by all internal services at Riot.  It's also unique and immutable.
      Internally, we call this player ID as PUUID.
      And as soon as the connection is established, league will go to the service to fetch latest state.  And by state, I mean list of clubs that this player is a member of and for each of those clubs, all the details about the clubs.  And then clubs are ready to go.  You have chat, you have a list of members, and you can start using them.
      What happens when there is an invite incoming to the club?
      In this case, club service will update their internal state, and then issue arrest -- a post request to RMS.  With a notification.  The notification itself is a simple JSON blob that informs RMS what to do with the message.
      There are few required fields in that blob.  One of them is called resource, which is rest-like string denoting which component of the client should handle this incoming notification.  You can think of the resource as the routing key that is used by the client to dispatch the notification to a corresponding plugin or subsystem.
      The next one is version, which is a string that allows us to guard against duplicates.  So if the sender service fires few notifications which carry the same version, it is safe to assume for the client that he can ignore it.  It also helps with notification reorganized because we use HTTP request can actually get reordered on the way to RMS.
      The next one is a list of player IDs.  A list of PUUIDS, and that's a list of all players that should receive this notification.  In the case of batch notifications, for example, when the club changes the message of the day, instead of notifying 50 players one by one, club service can actually issue one notification with 50 player IDs on this recipients list.
      And lastly, we have a short optional payload to deliver and more about it in a second.
      So as soon as RMS receives this notification, it will use apply internal routing rules and forward notification to connected client.  As a result, client will then go to the club service to fetch the entire state to figure out what actually happened, why did it receive that notification?  And then display a proper toast to the player.
      As mentioned earlier, RMS can also carry short payloads in the notifications, and they are completely oblivious to RMS.  It's just a binary data that's transported from point A to point B, and it can be anything from JSON base 64 blob XML or even plane stream.
      Okay.  Let's dive deep into RMS implementation.
      As mentioned earlier, RMS is a server side notification.  It's built by two different tiers.  Routing and edge.  Routing is responsible for notifications from external services like clubs as well as maintaining the list of all connected players to the platform.
      On the other hand, edge is responsible for handling player connections.  This WebSocket connections I was talking about.  And forward incoming messages directly to the clients.
      Both tiers are completely decoupled.  We can scale and deploy them independently.  For example, in one of our RMS clusters, we have four servers that run in the routing, and run ten servers depending on the scale and the time of the day in the edge tier.
      Their code has their own repository, and we release them as two completely separate products.
      Edge tier is a collection of independent servers that do not share anything with each other.  Each of the servers is responsible for accepting incoming player connections, authenticating the players by validating the sign tokens, holding the sessions, so holding a persistent TCP connection between player and edge and also registering them with the routing tier.  By registering, I mean issuing a request to their routing tier telling routing tier, hey, player with player ID equals X is connected to me.  Forward me any incoming messages, any incoming notifications that are addressed to that player.
      This tier is typical memory bound because it holds all the player connections in memory.  And even though we load tested pretty extensively, we were able to achieve around 10 million connections on a single server.  We decided that we didn't want to put too many eggs in one basket because losing that one node with ten million players will result in a disaster.
      So we decided to cap the number of connections to around 350,000 per box.  This tier is also really linearly scalable, and we can increase this tier capacity by simply throwing more servers at it.
      On the other hand, we have routing tier.  This tier is completely different from edge.  It's a cluster of very tightly-coupled servers sharing a global routing table mapping player IDs to the edge nodes that hold at session.  It's also responsible for monitoring health of each of the edge servers.  And also accepting authenticating publishers accepting incoming notifications and aforewarning them to the edge tier.
      This tier is CPU bound, and it's typically much, much smaller than the edge tier.
      The routing tier I mentioned a little bit earlier is a simple translation table between player IDs, that's the column on the left, and the edge nodes.  That's the column on the right.  This table is in the routing cluster and is used whenever there's any incoming notification addressed to the player.  We simply perform a lookup in the table and figure out what the edge node is and forward it.  Forward that message to the edge node.
      Let's follow the flow of the published message in the system.  First of all, we have a back-end service like clubs that are trying to communicate with the players.  It sends a request, publish request to RMS, we have a low balancer sitting in the front of the routing servers, and this low balancer forwards a notification to any of the existing routing nodes, it can select any because we have the global routing table that is shared across all the servers.  One of the routing servers will look up into the routing table, select the edge node that holds player connection, forward it to the edge server, which then delivers that to the player using players WebSocket connection.
      The entire RMS stack is built using Erlang.  Erlang is a functional programming language, which comes with a great support for building highly concurrent and distributed systems.  Certain language constructs or philosophies like the one around actor model or the concept of lightweight processes or even, like, an excellent framework, they allow us to focus primarily on the core logic of RMS instead of trying to solve scalability issues.
      The team behind RMS has already built chat and a few other services internally at riot using this technology, so building RMS was really, really easy, considering we had a lot of other libraries that we use in other projects.
      If you would like to know more about Erlang, I would highly recommend you reading book by Fred Hubbard.
      As mentioned earlier, each of the tiers routing and edge are completely separate.  And we release them independently.  Even though they are two different Erlang releases, we package them in Docker containers, which gives us great service isolation.  We can relocate the servers with ease and also it gives us independence from the underlying OS.  We don't have to make any assumptions about availability of the local libraries or installed packages.  We have the golden image.  We can drop on any server, any cloud provider, and we will be sure it's going to work.
      Whenever we build a new Docker image, we upload them to AWS container registry.  So as you can guess, RMS runs on AWS.  However, the code itself does not have any dependencies on the underlying infrastructure.  It is completely possible to run RMS on your local Mac or any other cloud provider.
      For infrastructure automation, we use Hashicorps teraform.  We store our infrastructure descriptions in a Git repository.  We version it, we have history with our infrastructure with all limitations.  And also, we can express it in a very declarative way.
      This approach allows us to reduce the risk of running into very costly human mistakes whenever someone clicks on the wrong button in the UI.  But also gives us an ability to create ephemeral environments for load testing within minutes.  We can stand up the entire RMS environment with one click of a button.
      And because RMS is a core piece of riot infrastructure as mentioned earlier used by over 20 different teams, one of our goals when building RMS was that we would like to be the first to know whenever there is any disruption of the service.  To do that, we build a battery of automated health checks for simulating publishers and clients.  They run every 30 seconds against our infrastructure, and the only measure correctness of the service but also verified the latency is not detected.  Whenever we detect any anomalies in the behavior, we have SNS queue that notified PagerDuty.
      Let me now trying to instill some points using on your own when building similar services.
      When building and testing distributed systems locally on my Mac, at least, I'm really -- I tend to ignore and forget about the fallacies of distributed computing.  Those are the set of false assumptions we typically make about replications.  As an example that the network is reliable.  We assume that with the split brain syndromes will never happen to us.  Or that the latency is zero because I tested locally using my loop bug interface.  Or we will not see request reorganized.
      To combat that, to fight this behavior in very early in the development process, we started using a tool called jepson.  It's a tool for verification of distributed systems which allowed us to inject failures and faults into our system.  We were able to simulate different types of problems, for example, splitting our routing cluster into two different hubs and then after running some additional tests and merging them back, we were able to verify that everything went back to normal, and we have consistency in our system.
      One of the problems that hit us very hard early when we forgot about the network and the infrastructure doesn't necessarily have to be super reliable was the example of having too many connections on a single edge server.
      So whenever one of the instances in AWS failed, which happens from time to time, we lost around 500,000 connections in a single second.  All of those players, because of the lodging in the client, attempted to reconnect to RMS immediately.  So we essentially DDO (s) ourself.  Which is fine.  We would like players to be able to connect to RMS at any given time.  But the problem is that the load balancer that was sitting between edge and routing tier that was used for registering new sessions was called.
      So what happened is 500,000 players were trying to reconnect in a same second and hovering in that ELB, that ELB was trying very hard to warm up and was failing.  And for five to ten minutes, we had half a million players who were completely dark.  They were not able to receive any notifications and could not understand what was going on.  Like, they were not able to see game notifications or other things.
      We essentially at the end of the day we removed the low balancer entirely from our infrastructure, replaced it with service discovery, and also reduced the number of connections we can have on a single edge server.  But that's one of the examples that actually demonstrate how important it is to think about failures when building your systems.
      Another thing that is really important and dear to my heart is visibility into our systems.  We probably collect more statistics that we realistically should.  We try to measure pretty much everything.  Things like not only messaging published by back-end service, but also distribution of different published sizes, the latency within the system, the number of currently connected users, publish errors, and so on.  It is even more important that with ephemeral containers with auto scaling groups that we use centralized logging and centralized monitoring because nodes go up and down all the time.  And if we only keep the logs and metrics locally on every box, it is more than guaranteed that they're going to be gone within the matter of hours, and we will have no history and no way to really triage and understand what really happened after the fact.
      We also learned that even though the detailed metrics and monitoring is super cool for developers and maintainers of the system, customers don't really give a shit about it.
      [Laughter]
      What they care about is RMS up or down?  Do my players can receive notifications?  And to do that, we need to take a step back, abstract things, and present them with a dashboard that only has two states.  Is it green or red?  And this dashboard is actually built using those health checks we talked about a few minutes earlier that run against life system every 30 seconds.
      Another is support for dynamic configuration.  With such a large number of customers coming in, we had to find a way to fine-tune this system effectively without having to restart the service or go to every single instance around the globe changing the configures by hand.  Our configurations live in a Git repository, and we have a job transformer that moves them from that Git repo and publishes them to the centralized server.  We call it configurous.
      RMS pulls that server for changes every 30 seconds as well and converges dynamically without any need of restart on the new state.
      That -- it's a perfect thing for us, for things like feature toggles.  Pretty much every major change we introduce into the system sits behind a feature toggle, which we can turn on and off with a single Git comment globally and see the change being applied on all the servers around the world in 30 seconds.
      Also with a very small team behind RMS, we only have four engineers, and many internal customers.  We had to find a way to avoid becoming a blocker for teams to really triage problems and adopt our service.  We essentially had to build self service capabilities.
      We invested a lot in client and publisher SDKs implementation examples in easy to use triage tools.
      As an example here, we have a realtime dashboard that allows publishers to take a sneak peek into what's actually being routed right now into the system.  The dashboard allows us to sample the traffic and route and display it to the publishers.  So whenever they see a problem, they can immediately go and see, oh, is there anything that's actually being processed?  Is there anything that is maybe parsed incorrectly?  And that way they can at least help themselves first before they come to us for physical help.
      We intentionally block teams only on creating new accounts, and that's primarily to understand our publisher use cases needs and also to establish the relationship between the RMS owners and the customers.
      Another thing I'm really passionate about is the capacity planning.  We load test the entire system to understand its limits to establish the baseline and also identify all the bottlenecks.
      For a globalized service like RMS, we also had to make sure we had no downtime upgrade capabilities, and we can do it even during the peak hours.
      During the low test, it was very important for us that we also were able to shut the entire system down and bring it up, even though we had over 10 million players trying to hammer and get back in.  That allowed us to simulate and be prepared for any sort of massive failures of our infrastructure and giving us that assurance that we can bring back the system online, even if it's peak hours, and we have so many players trying to reconnect.
      Question to you.  So this is a graph of our connections to the service.  And around the third of May, we experienced some weird behavior.  Does any one of you have any idea what it might be?
      So what happened is all of a sudden within a few hours, we essentially doubled our connection counts.  The team was, like, super excited.  We were, like, fuck, yeah, we just doubled the CCU, we established new records chilling champagne going to party.  Super cool; right?  Big numbers.  It turned out that actually that was a patch day and the client got patched, and we ended up instead of one concurrent connections, we had two concurrent connections going to RMS.
      Yeah, pretty bad.
      [Laughter]
      So we had two options here.  We could either release a hot fix to the client, which would trigger the full rebuild process retros and everything else, players playing because players would have to patch the client log out and log back in, do everything like that, or we just used capabilities of our auto scanning groups and resize the system to handle the double of the load.  It turned out not a single page was issued within those two weeks.  And because we load tested the system to probably 10X of its current capacity, we didn't have to act at all.  We just waited for two more weeks, that's the date of May 15th when the new client got released after two weeks and the connection count dropped back to the original number.
      Okay.  Cool.  So what's next?  Just to wrap up, I wanted to share a few roadmap plans for RMS.  First thing is, obviously, widening the adoption of the service.  We would like to have more teams internally to use RMS, pushing even more data through the service.  Next thing we are considering about is support for topic-based routing.  So instead of addressing individual players, we believed able to say.  Okay.  Deliver this message to all players in Germany.  Or deliver this message to all players using Mac or Windows.
      And finally, we would like to introduce persistence into the system, which would allow publishers to tell us that this message should be delivered, even though player might be offline right now, so we could persist it somewhere and then deliver it next time the player shows up.
      If you are interested about more details on RMS, we have a tech blog.  Engineering.riotgames.com.  There is a blog post on it and now I can probably take a few questions, but we can probably do it offstage as well, if that's fine.
      The question was which back-end do we use for the global routing table?  We use a build in Erlang distributed key value store.  We patched it heavily to support the concurrency in dynamic cluster topology.  But, essentially, the 95% of the code is the standard Erlang library code.
      [Question off mic]
      >> How do we maintain WebSocket connections?  Sorry.  What was the second part of the question?
      Oh, how do we rotate edge servers with persistent connections?
      So to do that, we have an internal protocol that whenever the server -- the edge server is about to shut down, we receive SNS notification from auto scaling group, and we drain the connections that are established to that specific edge server by -- before shutting down the connection we send a message using internal RMS protocol to the connected client, telling the client to, hey, I'm about to shut down.  Please open another connection to the service.  But allows us to drain I think 300,000 connections in, like, two minutes or so, so it's a pretty fast process.
      What do we use for our balancer?  It's a standard ELB from AWS.  Because it's Web sockets, and we use the classic load balancers, it's actually TCP-based, so we cannot use all the goodness coming from ALBs or HTTP aware balancers, but it's still pretty good.
      [Question off mic]
      >> Okay.  How do we keep the routing table up to date with the connections coming in and going out?
      So, essentially, each edge server is all the connections established to that particular node.  And whenever there is new question that is authenticated and established to the server, we issue a rest call to the routing tier to notify about that change.  And the same happens whenever the connection is terminated.  So edge server will inform the routing tier that the connection is down.  And it should remove that player ID from the table itself.
      Yeah, so when the edge node crashes, so the routing tier also maintains health check logic for every edge server, so it has some sort of garbage collection mechanism that allows it to eventually clean up the mess in the routing table.  We also have if the unregister connection call fails, so we, like, time out or whatever, next time routing tries to deliver the message to that ghost session, to the orphan session, it will receive 404 error from the edge server.  But it's supposed to hold that session, and it will clean itself up as well.
      [Question off mic]
      >> So the question is what's the protocol used for communication between edge and routing tiers?  That's pure rest over HTTP.  We use distributed Erlang protocol for internal cluster communication within routing tier.  Those are the normal -- that's normal Erlang cluster for communication between tiers is pure rest.  Which also gives us the possibility to potentially replace technologies used on both sides, so we don't have to user language, necessarily, on both ends.
      [Question off mic]
      >> Sorry.  I couldn't hear the question.
      >> Auto scaling on the edge tier.  How concerned are you with cost and what, if any, how do you scale the server up and down with respect to demand?
      >> Sure.  So the question is about auto scaling of the edge tier.  How concerned we are about the cost and how do we manage our policies for going?  Like, growing the cluster or shrinking it down?
      We use two metrics for that.  We use CPU utilization and also we use the connection count metric.
      I think CPU comes out of the box for AWS for connection count we have to run that essentially publishes that metric to cloud watch every minute or so telling auto scaling group what's the current number of established sessions to that server.
      We are not super concerned.  I mean, we are concerned.  We are cost aware, but we are not trying to optimize it to its limits.  We prefer to play safe on the reliability side and make sure that players have good experience instead of, like, trying to save every dollar.
      But the scaling actions happen probably two, three times within a day, so it's not that infrequent.
      >> Thank you.
      >> How did you get Erlang distribution working in Docker.
      >> How did we getter language distribution working in the Docker?
      Every single container we have has its own IP address.  We also limit the actual range of distribution ports to only, I think, five or six.  There are VM settings to specify the upper and lower range.  And we also run the containers in the host network mode, so that specifically for the edge tier is important because with the proxy default proxy mode, the proxies will simply get overloaded with, like, 10 million connections going through them.
      Okay.  I think we are running slowly out of time, but I would love to take any questions offstage.  Thank you so much.
      [Applause]
      Captions provided by @chaselfrazier and @whitecoatcapxg.
