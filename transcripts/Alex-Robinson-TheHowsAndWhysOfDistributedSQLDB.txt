    "The Hows and Whys of a Distributed SQL Database"
    by Alex Robinson
    
    >> All right, can you guys all hear me OK? Awesome, so thank you guys for coming. My name is Alex and this is my first Strangeloop so I'm super excited being here to talk to you. I am a startup engineer at a place called Cockroach labs and I know the name is kind of silly, Cockroach Labs, the database is called CockroachDB. The idea is like cockroaches, the databases replicate itself and are hard to kill. I'm not here today to talk to you specifically about Cockroach, though, I'm interested in this real new class of database that's been coming around the last five to ten years, these distributed SQL database. I'm interested why they're being built now as opposed to 20 years ago and what makes them different from old databases or from previous database and so this is what we're going to take a look at today, you know, why a distributed SQL database, why would anyone care about them, why would you devote years of your life to building them or millions of dollars to building them and how do they work. Specifically how data gets spread around the system, how it gets replicated in a systematic way and so let's start by looking at the history. Any time someone introduces a new system, you really need to ask why. House this system going to be better than the hundreds of systems that have come before it in the history of computing and whys it being built now? Around so we'll begin with the very first database and these were being built in the early 1960s, at companies like IBM and General Electric. These databases were pretty primitive because they were the first ones of their kind. They have didn't know a whole lot about what they were doing yet. The interfaces that they exposed to programmers were very tough to use, so rather than having a table of records like we're used to today or a key value interface like we're used to today, it was almost like they were interested in programming these series of records that your queries would have to iterate one by one and traverse from like an organization record to iterate over all of the employee records that are working at that organization. And the queries that you wrote were really tightly coupled to the storage format of the data on disk and so if you ever wanted to change that to optimize for different access patterns you'd have to pull all the data out, load it all back in and then rewrite all of your queries as a result. This was a huge pain, they were really hard to use or required a lot of effort to use. And so this the staked the change in the early '70s, there was a mathematician by the name of Ted Codd at IBM who saw all of the engineers having around him and thought that there had to be a better way and being an mathematician he was all about relevance so he created what's now known of the relational model and this wanted to get rid of all of this tight coupling between queries and how data is stored on disk. And the queries here are very independent from the storage on disk, so the database software is free to optimize things underneath the covers without the programmer needing to worry about it. It can change how things are stored without anyone ever having to know. So these systems are much more developer friendly and as a note of historical context, they were always designed to just run on one machine. In the '70s, most organizations didn't even have one computer let alone multiple computers so these systems were always built for a single machine. The first was Oracle, and it continued to grow from there. More vendors started putting out SQL database, there were acquisitions, there was lots of competition, it was great but in this time there was also a new kind of database that got introduced that didn't really take off. You know, it was attempted but didn't really get the necessary traction. And these were object-oriented database. The motivation for these systems is that object-oriented programming languages were growing in popular. C++, I believe Java, and there is a bit of an impetus mismatch when you're trying to choose between the objects and the rows of comes that SQL database store so they argued that these were easier to use because you could effectively take an object in memory and dump it into a database.
    But at the time, there wasn't enough reason to switch. In order to switch, you had to give up things like complex queries that SQL database allowed, and all these different object-oriented databases had pretty disparate APIs, there was no standard as there was with SQL. So these systems never quite took off. The first real big change in how database worked or how they were used came in the early 2000s during the web boom. So suddenly there were more and more companies that needed to support more and more users, they had to build services to support millions and millions of users and that had to be up 24/7 and they couldn't scale their database to support this. You can only scale a database as big as you can buy a single machine so these tig companies like Google, eBay and Amazon, all having to do things like sharding their database and what this meant is they'd get ten machines to put their data on and they'd break their table up into chunks so the first chunk would go on the first machine, etc.
    And for queries to work they had to put this middleware layer. This limited what programmers could do, because they couldn't really do things across shards and any time the data grew to be too big to fit on the current number of shards, it was a huge operational headache to try and reshard, to move all of the data on to different machines so you could have more of them to account for the growth. So this was worth it for the really valuable services that these companies were building back then, you know, like Google famously sharded a mySQL set up. This might not be worth it for smaller services like Google calendar due to all the effort involved. And the same companies, Google, Facebook, Amazon all started building noSQL database and they had the primary goal of being scalable. They wanted scalability at almost any cost. So these noSQL systems had to give up a lot of things in order to get the scalability that they were after. They gave up the operational control which is why they are called noSQL. They usually didn't offer the same sorts of consistency of data that programmers were used to with SQL systems and so, you know, whereas Ted Cod in the '70s, took applications and brought it into the database. NoSQL did the reverse. They pushed it back out into the application saying we can't handle this anymore, you're going to have to deal with this, you're going to have to work around with the database's insufficient capabilities.
    And so now we're in the 2010s, and there's this new class of systems coming around, sometimes called newSQL, distributed SQL, some people call them elastic SQL, whatever you want to call them and why are these being built? I think it should be fairly clear now based on the history, but without distributed SQL database you have to make a choice when you're starting a new product. Are you going to limit your future scalability or are you going to pick a noSQL database and work around the lack of a solid foundation to build on? And there have been a lot of really successful services built on top of in SQL, right? So they're clearly not useless. So it's a lot harder. So Google built Spanner put in this choice quote of why they thought it was worth it to build into this system and to have most of their servers switch over to use it and I think it's worthwhile to have developers occasionally deal with performance problems than to always have to deal with lack of transactions, it safes Google money in the long run to do this because it saves their developers time and energy.
    And that's why we wanted to make that tradeoff. So there's a whole class of these systems appearing that are distributed databases providing full SQL semantics where you can go and talk to any of the machines but they'll do all the work under the covers to work as a distributed system so we're attempting to combine the best of both worlds and to build this functionality into a previous database you'd basically have to rip out the guts and put in entirely new guts and at that point it's almost like building a new system anyways. So the two primary systems I'm going to be referencing a lot today are Google Spanner, which was published back in 2012 and another paper published earlier this year and then CockroachDB because I work on it on a daily basis and all the information is out there on the internet due to it being open source. So that's why these systems are being built, but there's still the question of how. So what makes them different? Why couldn't they have been built 20 years ago and how are they getting this different set of tradeoffs? So we're going to specifically focus on how that data is distributed, how remote copies of the same data are kept in synch in a way that keeps it consistent and how transactions work. And I'm going to specifically compare all the different types of database, so how SQL database, noSQL and newSQL or distributed SQL database do things. There are a ton of database out there that work in different ways, I can't cover every exception to every rule. I'll do my best to speak in generalizations that are as true as possible and point out differences if I know them. But there are too many systems for me to know about them let alone point out the differences and I'm going to be focusing on Spanner and cockroach, but there are some slightly different designs out there for these newSQL systems, as well. So let's dive into data distribution.
    All the data distribution means is just this question of how we're distributing data amongst the cluster and for SQL database there's not a whole lot to talk about because they're designed to run on a single machine, right, so all your data is on a single server and you might have backups somewhere, you might have secondary replicas, but all the data is on one machine or all the data is copied onto a couple machines. There is this option as I mentioned to manually shard data across multiple instance of a database but that's not the database software doing anything. That's still just the database storing ought of its data on one machine and the human who's managing the system spreading the data across multiple machines manually.
    NoSQL and newSQL systems have to, though, because they're running with this core assumption that all your data is not going to fit on one machine, so you can't do things the SQL way so you're going to have to break up your data in some way and divide it up amongst all the clusters. NoSQL pioneered, newSQL is following in their footsteps.
    Once you've divided up the data in chunks, how do you find where those chunks are located within the cluster because it's no use to have your data broken up and spread out if you don't know where it is at any given period in time. There are two primary approaches to do this: Hashing and order preserving. So this hashing approach to data distribution just means that you're going to pick a hash function, maybe multiple hash functions and you run these on the keys, assuming it's a key value system. It spits out a result and you deterministically map that result onto one of the machines in the cluster, you maybe maybe just doing a modulist on one of the machines or something like this.
    All you need to do is run a function on this key and you know exactly which machines it's going to be located on owe reads and writes can find the given machine they want to talk to very quickly. The problem here that makes it prohibitive on newSQL database is you can't do hash scans. So if you wanted to scan over all of the keys from A to B, you can't do that in a system that's dividing up the data based on a hash function. So this is used in cases like Amazon's database DB but it's not used on any newSQL databases as far as I'm aware.
    Some other noSQL systems use what's known as an order preserving approach and this means that you put all all of your data, lay it out from A to Z or the beginning of the alphabet to the end of the alphabet in alphabetical order and split it up into chunks that give you nicely sized chunks in the middle. So you might have a chunk in the beginning of the alphabet from A through F, from F through Q and so on and distribute these chunks evenly across all the nodes in the cluster. This is really good from range scans of course because we're keeping the data in order. You might have to talk to multiple nodes if you're scanning across a very big range but the data is all going to be in order for you. The down side is that you no longer have this deterministic function so you have to add on some kind of range indexing structure on top of your actual data to track which range exist in the cluster and where they're given at any point in time.
    So you're going to have to divide up your data into what we call ranges. Each just holds keys and values that fall into that range of data. You also add on this range index on top as I mentioned and that's going to track which ranges exist and where they're located so any time a query comes in, say you want to read the key "mango" you asked why one does mango get in and in this case it's the middle one. So you can send a request to that machine for that key and it might seem like this is going to add a lot of overhead to all of your requests, but in pla these ranges don't move often so they can be really easy cached on all the machines so you don't have to do this extra lookup all of the time and finally you can do a range scan as I mentioned.
    The last thing you have to ask about this approach, though, is when do you split up the data? Because you can do a split pretty easily by just picking a point in the middle of an existing range and divide it in two. So you can divide up that last range and split it into two if you get a few more keys added but you have to decide when to do this. So you could wait until the data got too big to fit on a single machine or maybe you could do it sooner. In practice somewhere on the order of 100 megabytes of scale, because if you let your ranges get too big then moving them between machines is quite slow. So if you need to quickly cover them to recover data from node that crashed or to rebalance it's tough to do that with ranges that are really big, but if you make your ranges too small, then your range index gets really huge. It adds an overhead into the indexing and the caching of that index and so you don't want to have tons of them, either.
    And because this is a distributed system, you have to assume that failures are going to happen from time to time so you have to have multiple copies of your data such that any given one is on a machine that feels you'll still have a copy of them.
    You're going to spread the nodes around in your cluster and you have to have some kind of a control process that does node balancing. You don't want it to just be empty. You know, you have to proactively move data onto it so this control system could live inside the database itself, it doesn't make a huge difference in terms of how you operate it.
    But it needs to detect when there's this imbalance of data. So here nodes 1 through 3 have way more data than node 4 does. So it makes sense to move some over to 4. So node 3 can wipe that off its disk. It doesn't need to have it anymore. So you can keep doing this until the cluster gets approximately balanced and you do this for the size of the data,  the number of range or even the amount of load. So if some are way hotter than others you could split them up into two and spread them out into different machines in order to balance out the CPS system on your cluster, which could be important in practice.
    So if a machines you could be running with fewer copies of data than you want so you have to replicate these things so you get back to the desired number of replicas at all times whenever possible and then if the node never comes back we'll still be okay, and have enough copies.
    So that's how data gets distributed around these clusters. Both in noSQL systems and in these new distributed SQL systems, but once you do that, as soon as you have multiple copies of data in a distributed system, you really need to ask yourself, how are you keeping them in synch, or how are you dealing with conflicts where one copy says one thing and one copy says another thing? Because it's bound to happen. Network requests can be lost, a machine can be down and it might not be up to date and you need to have some way of deciding which copy correct at any given time.
    And so SQL database since they store all their data on one machine don't really have to deal with this unless they're running some form of primary or secondary application but that is pretty common these days. If you're running primary or secondary application, there might not be an expectation that your secondaries are up to date, because the way it works is just that you have one copy of your database that's considered your prime, where you send all of your writes and oftentimes all of the reads. And in theory it's a very simple concept, right, the primary gets a write, it writes it to its disk and it passes on to the secondary or the multiple secondaries. In practice there is a bit more complication. There are edge cases where you're going to lose data or you're going to be reading stale data and it's tough to really manage this as an operator. You have to choose between asynchronous or synchronous replication. Where the what that means is that the secondaries won't necessarily have all the data the primary has. If you have to do a failover, you could be losing recent writes. If you're doing reads on the secondary you could be missing out on recent writes.
    If you do S. synchronous replication, this is going to introduce ex that network delay into all of your write requests and it also introduces interesting questions like what if the secondary fails or the secondary gets slow, how long are you willing to wait on that secondary to respond to a request before you give up on it, you know? And either way failover is really tough to get right, because you have a lot of clients in your data center and they need to know which database to talk to at each point in time. So it's tough to fail them over without leading to a sort of split brain.
    But moving on to noSQL systems, they really do a whole ton of different things to keep track of their copies of data and most of them are what's known as eventually consistent which is kind of just a euphemism for not consistent.
    [laughter]
    
    Most of these different solutions for maintaining copies of data can fail in weird ways, whether there are partitions or not. They could be dropping writes if two writes come in for the same keys around the same time. One of them is going to clobber each other and it's nondeterministic. In some cases you might get something that was never really acknowledged to be committed. The only thing that can really be called consistent is these conflict-free replicated data types. These are some really cool algorithms that have been talked about at past Strange Loops, so I'd recommend that you look at those. We don't have enough time to really drill into the details of how all these different things work. You know, you could fill multiple really interesting talks just doing surveys of all the different mechanisms, noSQL database use to keep their data in synch, just know that typically they don't really keep data conin any consistent way.
    NewSQL database or distributed databases use what's known as a distributed consensus algorithm and these consensus protocols have been heavily researched. There's a lot of proposals proposing improvement on existing protocols and they've been proven through formal methods to be correct, to keep your data consistent and reach agreement on all the writes in the system. The most famous of these protocols is Paxos, it was written about 30 years ago almost but is hard to understand and hard to implement. So there Google wrote a paper on how many years it took them to get one of these into a reasonable reliable production-ready state. It's not easy. Raft is a new protocol that was written by a student at Stanford that has really taken off. You can go out on GitHub and get open source replications of raft, it's not necessarily easy to use but it's available if you want to use it so this has made consistent systems. The way that these protocols typically work is they have some odd number of machines in a cluster, so three or five, maybe seven and commit just happens when a majority of those have written the data to disk. There is more in the details that makes them always correct but that's the high level, and so let's take a look at Raft in particular. We share that's what we use at CockroachDB. It's a pretty hardened implementation at this point. And Raft is based around leader election. So if you you'd say you have a set of replicas that choose a leader and if you've been to past Strange Loops, you have these rights being appended to a sequential log and then that log just gets replicated from the leader to the followers, and the commands are considered committed once a majority of the nodes in the system have written into disk. So in this example if write comes in for the key cherry, the leader will write it to its own disk in that sequential log, and replicate it to the followers. As soon as either of those followers has written it to disk, we suddenly have it written to disk on a quorum of them, right? So we have two out of three. At this point, the write is considered committed even if all the machines were suddenly to be turned off at this point, that write would have been committed. So you do have to wait this one network hop before you can acknowledge writes, which is a bit different from those SQL database that just have everything in place. But notice here that you didn't have to wait on that third replica to write it to disk. All of these algorithms require typically is a majority to be up to date. So if one node is running slow for some reason or say it gets shut off for a few minutes or it's on the other side of the country, you don't have to wait for it. Which allows for some interesting configurations where you can deploy two replicas on the east coast and one on the West Coast and those two that are close together can work easily and don't have to wait for the one that's far away.
    And so if we have this same example where a write comes in for the key cherry, and the leader dies before it's am to replicate it. By the definitions we've been given that write has not been committed because it's only on one hard drive, right, so those two followers are going to elect a new leader so the we have a new leader now and it doesn't have that key, because the two followers didn't know about it so if a read comes in for that same key, accord be to Raft, according to the consensus protocol, that key was never written. So the leader is going to go to its followers and say hey, do you have this key? No, no, I don't. The first replica then will then have to be asked to remove it because it was never truly committed and on a leader changeover, any uncommitted proposals just get dropped and so at this point the key is just returned as not found.
    And that's like a surprisingly thorough overview of Raft. I mean there are more details of course. But it is an interesting algorithm. I'd recommend reading the paper if you're interested in this kind of stuff. In practice, newSQL database have to run one of these for each range of data. We'd have to have one group for each of those ranges, so three consensus protocols running in a cluster. And in practice we'd have way more than three ranges, right so if you have terabytes of data in your system you could have hundreds of thousand of these ranges.
    So there are a lost engineering complications that come into play in these systems, it's tough to scale to have this many ranges and to be able to handle member changes when you're moving them between nodes, to handle splitting them consistently, to deal with software upgrades where some are running newer versions than others.
    Assuming you have the solid foundation of research underneath you telling that you this replication is correct, ... And so finally let's take a look at transactions. So we have consistently replicated data on a per-range basis, but SQL database allow transactions to touch any rows anywhere in the system so you have to handle coordination between these ranges. Between these consensus groups, and so these groups might be on different machines all throughout the cluster.
    And when we're talking about transaction, we're talking about good old old-school ACID transactions -- atomic, consistent, isolated and durable. The two more interesting facets here, the two toughest ones to provide are being atomic and being isolated. Consistent and durability is really just about synchronizing your writes to disk properly but the hard parts are being atomic and isolated. And so atomic means that all of the writes that are sent together either commit or none of them commit. You have to get all or nothing and being isolated means that having concurrent transactions trying to operate on the same data on the system, that they can't interfere with each other so it's as if they all have their own correct consistent view of the database at all times and there are multiple isolation levels, but we're going to focus on serializability today. Because you don't get any anomalies, no incorrect data out of the database.
    So first let's look at SQL database. And, you know, a lot of SQL databases are proprietary, so I'm focusing on Postgres here. All of mutations that are applied as part of a transaction, all of those writes, they're written with the transaction ID associated with them or attached to them so if I write to the key cherry it's going to have this little marker next to it that it's written as part of transaction foo and those writes aren't considered to be visible to reads necessarily because they're still tagged with that marker and transactions have to know which transactions have committed or not.
    When you do want to go ahead and commit the transaction, you have to write it to this commit log and that's where the atomicity comes in. Because you can do a single write in an atomic way to the commit log. Then we have in one fell swoop committed every operation that happened as a part of the transaction and that one disk write will either fully succeed or fully fail.
    That's how you get atomicity to be committed all at once. You boil them down to just this one disk write.
    Isolation is provided in a couple of different ways which we'll look at individually. Read-write locks are the easiest way or the simplest way. This is how SQL databases did it for decades. It's the same as you'd think. When you want to access a row you take a read lock on it, when you want to write a row instead of read it, you have to take a read lock on it. And then you have to hold these locks until you commit. And release the locks once you've committed your transaction. So the database has to do some deadlock detection, but this is pretty simple to understand and implement.
    There are ways to reduce your isolation level by reducing locks earlier on in a transaction, but then you'll have potential anomalies appearing in the behavior a user sees. The other approach to doing isolation in SQL databases is what's known as multi-version concurrency control or MVCC for short, because that's quite a mouthful. And what this is is any time you write or over on disk, and ... any time you update you have to write a new one next to with a different time stamp and anyone that wants to read a particular row has to get the most recently written version of that row. This has a few nice properties and allows access to historical data. You can run a query as you you know, 24 hours ago and get a consistent view of what the state of your system was. This could help you track down things like bugs you can pin down when it started affected your system.
    You're not holding onto these write locks any more, you're just running it in older time stamp and not blocking anything else from happening. So MVCC doesn't help by itself. You have to have something on top of this in order to prevent things from doing things it shouldn't: So it's a little tougher to implement. Postgres implemented a version of this I think about a decade ago that it uses for certain isolation levels now.
    And so noSQL systems don't have a whole lot to talk about because they just as a rule don't really offer transactions. The best thing you're going to get is a single key transaction, so you might be able to do a read, modify, write loop on just a single key, but you won't be able to atomically commit multiple keys at the same time or write some keys on one part of the system, then do some reads and write some keys somewhere else and have that all happen as an atomic event. It's something that they sacrificed in order to have scalability.
    And so you know, this is kind of all I can really so about noSQL transactions. Finally, distributed SQL, has to support the same semantics as traditional SQL databases because you can't replace unless you have the same semantics, so we need full acid, we need serializable isolation and that takes a bit of work in a distributed system but we can learn a lot from how SQL databases does it so we can provide atomicity in a similar way. All writes can be tagged as usual and we just have to have this transaction record somewhere in a consensus group, because writes are atomic. They either do happen or they don't. So as a single node, single database can use a single disk write, we can use a single consensus group write. So it takes that flipping of the transaction status from in progress to committed to mark all of the writes as done.
    And isolation can be similarly done. It's a bit harder due to network latency involved. But we can use similar approaches. I think most newSQL systems that I am aware of use some version of MVCC. Spanner uses some form of MVCC. They have to rely a lot on their hardware clocks that are in Google these days.
    Cockroach has to -- if you do have replicas on both coasts of the US for example but let's take look at this MVCC conflict approach to transactions.
    So you'll start by, you know, beginning a transaction and putting a key to the database and you know, that key will be in some range and for convenience in cockroach at least, we put the record for that transaction tracking status in the same range as that first write so they're collocated if the transaction happens to be happening in just that one range. So notice that the key was tagged with the transaction number. And that's to indicate that it might not be committed yet. If some other transaction comes along and wants to read the key cherry it's going to have to check the transaction status for that transaction to be aware of whether or not it should be returning that key. The transaction can go off and write keys in other ranges of the system, other tables. SQL system and so on. You know, so it could write mango in another range and then when it's done it has to go into a single consensus write to flip the status of the transaction record so it's going to mark it as being committed and at this point the transaction is done. Any new transaction that comes along and reads those keys will know to just check the status of the transaction and see that it's committed. That does add some extra work to the reads, though, so after a transaction finishes it goes and proactively cleans up those transactions. And once we've cleaned up the pointers, we can get rid of the transaction record because it's not needed anymore. But again, any time you're talking about one of these distributed systems you have to ask about what happens when things fail, when things conflict, when things don't go wrong in general so let's take a look at one particular type of conflict. It's going to be a write conflict. So a transaction will start out a same way. It will write a key and put a transaction record into that transaction for the same range but then another transaction is going to come along and try to write to the same queue, and so in this case the state to detect that a conflict has happened is on the machine itself. So the machine knows that something else is written to that key recently. So there are multiple ways to try to handle this. You can make the transaction wait on the first one that's already running or you could do an abort of one of the two transactions and force them to restart. So assuming we're going with the abort method. You have to pick one to abort. This can be on a priority scheme, it could be user assigned, randomly generated. And you could make a hierarchy. In this case, transaction 2 has to check whether transaction 1 is done though. In this case it's not, though, so transaction 2 is going to decide to push or abort transaction 1. Push would mean make it start over at a higher time stamp. So transaction 2 has aborted the conflicting transaction. It can update the write to its own key where intent just means the transaction ID tagged on it and then commit. So at this point transaction 2 is done, and transaction 1 is going to have to restart. It might be able to do this restart internally to the database or it might have to send that abort error out to the client for the client to retry.
    So that's a quick overview of how these conflicts work, of how these systems work, more details can be found on the Cockroach Labs blog, other places online. Because this description had to omit things like the MVCC itself and other types of conflicts. So to summarize any time you're looking at a new system, whether it's a database or some other kind of thing you should try to understand why it's being created. How does it fit into the historical context of that system. Why is it being built now, why wasn't it built a while ago? And try to get a better sense or what tradoffs are involved in picking a system. So we've done that. We've also looked at how do database work. How do they replicate that data in a different way which is different from SQL or from noSQL. It's notably not not-known from academia, though. And then how transactions work. Which has been done by learning a lot from how SQL databases work and applying some elbow grease to make it work in a distributed setting. So thank you a ton for coming out and listening today. I'll be around for questions afterwards. I'll recommend there are a lot of papers out on these topics, you can check stuff out on the Cockroach Labs blog or ... the GitHub: But thank you for coming.
    [applause]
